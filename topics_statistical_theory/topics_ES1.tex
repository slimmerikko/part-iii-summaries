\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=3cm]{geometry}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools, amsmath, amssymb, amsthm, mdframed, bbm, graphicx, float, physics, xcolor, cleveref}

\hypersetup{
    colorlinks   = true, %Colours links instead of ugly boxes
    urlcolor     = blue, %Colour for external hyperlinks
    linkcolor    = blue, %Colour of internal links
    citecolor   = red %Colour of citations
}

% Definition of numbered environments.
% Usage: \begin{theorem} ... \end{theorem}
% Remark has no numbering.
\theoremstyle{plain}
\newtheorem{question}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% Question with box around it
\newenvironment{qbox}{\begin{mdframed}\begin{question}}{\end{question}\end{mdframed}}

% A proof with "solution" instead of "proof" and no QED symbol
\newenvironment{solution}{\begin{proof}[Solution]\renewcommand\qedsymbol{}}{\end{proof}}

% Some renewed commands
\renewcommand{\vec}{\mathbf}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\phi}{\varphi}

% Frequently used math alphabets
\newcommand{\Bb}{\mathbb}
\newcommand{\Cal}{\mathcal}
\newcommand{\Bf}{\mathbf}
\newcommand{\Rm}{\mathrm}

% Frequently used letters in the blackboard alphabet
\newcommand{\CC}{\Bb C}
\newcommand{\NN}{\Bb N}
\newcommand{\PP}{\Bb P}
\newcommand{\QQ}{\Bb Q}
\newcommand{\RR}{\Bb R}
\newcommand{\EE}{\Bb E}
\newcommand\HH{\Cal H}
\newcommand\FF{\Cal F}
\newcommand\Nn{\Cal N}
\renewcommand\SS{\Cal S}

% Usage: \ang{...} is equivalent to \langle ... \rangle, while \ang*{...} is equivalent to \left\langle ... \right\rangle
% For other delimiters: use \qty from the physics package (i.e., \qty(...))
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Frequently used commands
\newcommand{\T}{^\top} % Matrix transpose A\T
\newcommand{\C}{^\complement} % Set complement A\C
\newcommand\ceq\coloneqq % Definitions :=
\newcommand\pow{\Cal P} % Power sets
\newcommand\eps\epsilon
\newcommand\ind{\mathbbm 1} % Blackboard 1 for indicator functions
\newcommand\restr{\mathord\restriction}
\newcommand\TODO{{\color{red} TODO: }}
\newcommand\toprob{\overset{\Rm{p}}{\to}}
\newcommand\todist{\overset{\Rm{d}}{\to}}
\newcommand\toas{\overset{\Rm{a.s.}}{\to}}
\newcommand\deq{\overset{\Rm{d}}=}
\newcommand\iid{\overset{\Rm{iid}}{\sim}}

% Functions that appear frequently
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator\Exp{Exp}
\DeclareMathOperator\MSE{MSE}
\DeclareMathOperator\MISE{MISE}
\DeclareMathOperator\Bias{Bias}
\DeclareMathOperator\Beta{Beta}
\DeclareMathOperator\Gam{Gamma}
\DeclareMathOperator\Cov{Cov}
\DeclareMathOperator\diag{diag}

\title{Topics in Statistical Theory --- Example Sheet 1} % subject
\author{Lucas Riedstra}
\date{8 November 2020} % date

\begin{document}
\maketitle

\begin{question}
    Let $U_1, \dotsc, U_n \iid U(0, 1)$ and let $Y_1, \dotsc, Y_{n+1} \iid \Exp(1)$. Writing $S_j \ceq \sum_{i=1}^j Y_i$ for $j = 1, \dotsc, n+1$, show that
    \[
    U_{(j)} \deq \frac{S_j}{S_{n+1}} \sim \Beta(j, n - j + 1)
    \]
    for $j = 1, \dotsc, n$. 
\end{question}

\begin{solution}
%    First we compute the distribution function of $U_{(j)}$. If $U_{(j)} \leq x$, then at least $j$ of the $U_{i}$ must be $\leq x$. 
%
%For $k = j, j+1, \dotsc, n$, there are $\binom{n}{k}$ ways to choose $k$ of the $U_{i}$'s that must be $\leq j$. Fix $x \in [0, 1]$, then we have
%\begin{align*}
%    \PP(U_{(j)} \leq x) &= \sum_{k=j}^n \binom{n}{k} \PP(X_1 \leq x, \dotsc, X_k \leq x, X_{k+1} > x, \dotsc, X_n > x) \\
%    &= \sum_{k=j}^n \binom{n}{k} (F(x))^k \cdot (1 - F(x))^{n-k} \\
%    &=  \sum_{k=j}^n \binom{n}{k} x^k (1-x)^{n-k}.
%\end{align*}
%
%Now, the density function of $X_{(k)}$ is given by the derivative of $F$. This is a hefty calculation, see for example \href{https://math.stackexchange.com/a/1179378/696417}{this StackExchange link}, but it turns out to be
%\[
%\dv{x} F_{(j)}(x) = f_{(j)}(x) = \frac{n!}{(j-1)!(n-j)!} x^{j-1}(1-x)^{n-j},
%\]
%which is also the density function of the $\Rm{Beta}(j, n-j+1)$ distribution.

We compute the density function of $U_{(j)}$ as follows: let $x \in (0, 1)$, then we know that 
\[
f_{(j)}(x) = \dv{x} F_{(j)}(x) = \lim_{h\to 0} \frac{F_{(j)}(x+h) - F_{(j)}(x)}{h} = \lim_{h \to 0} \frac{\PP(x < U_{(j)} \leq x + h)}{h}. 
\]
The probability $\PP(x < U_{(j)} \leq x + h)$ is the probability that exactly $j-1$ of the $U_{i}$ are less than $x$, and that at least one of the $U_{i}$ is in $(x, x+ h]$. 

The probability that two or more of the $U_{i}$ lie in $(x, x+h]$ is $O(h^2)$ and therefore negligible, so we must compute the probability that exactly $j-1$ of the $U_{i}$ are smaller than $x$, one of the $U_{i}$ is in $(x, x+h]$, and the other $U_{i}$ are greater than $x+h$. This is easily seen to be
\begin{align*}
&\binom{n}{j-1} \PP(U \leq x)^{j-1} \cdot (n - j + 1) \PP(x < U \leq x + h) \cdot \PP(U > x + h)^{n-j} \\
&= \frac{n!}{(j-1)!(n-j+1)!} (n-j+1) x^{j-1} h (1 - x - h)^{n-j} \\
&= \frac{n!}{(j-1)!(n-j)!} x^{j-1} (1-x-h)^{n-j} h. 
\end{align*}
Therefore, we easily compute
\[
f_{(j)}(x) = \lim_{h \to 0} \frac{\frac{n!}{(j-1)!(n-j)!} x^{j-1} (1-x-h)^{n-j} h}{h} = \frac{n!}{(j-1)!(n-j)!} x^{j-1} (1-x)^{n-j}. 
\]
This is also the density function of a $\Beta(j, n-j+1)$ distribution. 

Finally, define $T_j = S_{n+1} - S_j$, so that $S_j$ and $T_j$ are independent. It is known that $S_j \sim \Gam(j, 1)$, $T_j \sim \gamma(n - j + 1, 1)$, and furthermore that
\[
\frac{S_j}{S_{n+1}} = \frac{S_j}{S_j + T} \deq \frac{\Gamma(j, 1)}{\Gamma(j, 1) + \Gamma(n - j + 1, 1)} \sim \Beta(j, n - j + 1). 
\]
\end{solution}

\begin{question}
    Let $X$ be a random variable with mean zero that satisfies $a \leq X \leq b$. Use convexity to show that for every $t \in \RR$, we have
    \[
    \log \EE(e^{tX}) \leq - \alpha u + \log(\beta + \alpha e^u),
    \]
    where $u \ceq t(b-a)$ and $\alpha \ceq 1 - \beta = -a/(b-a)$. Using a second-order Taylor expansion around the origin, deduce that $\log \EE(e^{tX}) \leq t^2 (b-a)^2/8$. 
\end{question}

\begin{proof}
    Let $x \in [a, b]$, then we know there exists a unique $\lambda \in [0, 1]$ such that $x = (1-\lambda) a + \lambda b$. A simple computation gives $\lambda = (x-a)/(b-a)$, $1 - \lambda = (b-x)/(b-a)$. By convexity of $t \mapsto e^{tx}$ we find
    \[
    e^{tx} \leq \frac{b-x}{b-a} e^{t a} + \frac{x-a}{b-a} e^{t b}. 
    \]
    
    From this we deduce that 
    \[
    \EE[e^{tX}] \leq \EE\qty[\frac{b-X}{b-a} e^{t a} + \frac{X-a}{b-a} e^{t b}] = \frac{b}{b - a} e ^{ta} + \frac{-a}{b - a} e^{tb} = \beta e^{ta} + \alpha e^{tb} = e^{-\alpha u + \log(\beta + \alpha e^u)}. 
    \]
    Since $\log$ is increasing, we can take the logarithm on both sides to conclude
    \[
    \log \EE[e^{tX}] \leq -\alpha u + \log(\beta + \alpha e^u). 
    \]
    
    Now, we compute the taylor polynomial of $f(u) \ceq -\alpha u + \log(\beta + \alpha e^u)$ in $u = 0$: we have
    \begin{align*}
        f(0) &= \log(\beta + \alpha) = \log(1) = 0; \\
        f'(u) &= -\alpha + \frac{\alpha e^u}{\beta + \alpha e^u}; \\
        f'(0) &= -\alpha + \frac{\alpha}{\beta + \alpha} = 0; \\
        f''(u) &= \frac{(\beta + \alpha e^u)\alpha e^u - (\alpha e^u)^2}{(\beta + \alpha e^u)^2} = \frac{\alpha e^u}{(\beta + \alpha e^u)} \qty(1 - \frac{\alpha e^u}{(\beta + \alpha e^u)})
    \end{align*}

Note that $\frac{\alpha e^u}{\beta + \alpha e^u} \in [0, 1]$ since $\alpha, \beta \geq 0$ (this holds because $a$ must be negative and $b$ must be positive due to the condition $\EE X = 0$). For $y \in [0, 1]$, the polynomial $y(1-y)$ takes values in $[0, \frac14]$. Therefore, by Taylor's theorem with remainder, we conclude
\[
\log \EE[e^{tX}] \leq \qty(\sup_{u \in \RR} \frac{f''(u)}{2} )u^2 = \frac18 u^2 = \frac{t^2(b-a)^2}{8}. 
\]
\end{proof}

\begin{question}
    Let $X_1, \dotsc, X_n$ be independent with distribution $P$ on a measurable space $(\Cal X, \Cal A)$, and let $\hat P_n$ be the empirical measure of $X_1, \dotsc, X_n$; thus $\hat P_n(A) = n^{-1} \sum_{i=1}^n \ind_{U_{i} \in A}$. Show that, for all $\eps > 0$ and $A \in \Cal A$, we have
    \[
    \PP\qty(\abs{\hat P_n(A) - P(A)} > \eps) \leq 2 e^{-2n\eps^2}. 
    \]
\end{question}

\begin{proof}
    Define a new distribution $Y = \ind_{X \notin A}$. Its distribution function is given by
    \[
    F_Y(y) = \begin{cases}
        0 &\text{$y < 0$}; \\
        P(A) &\text{$y \in [0, 1)$}; \\
        1 &y \geq 1. 
    \end{cases}
    \]
    
    The empirical distribution function of $Y_1, \dotsc, Y_n \iid Y$ is given by
    \[
    \hat F_n(y) = \frac1n \sum_{i=1}^n \ind_{Y_i \leq y},
    \]
    and thus for $y \in [0, 1)$ we have
    \[
    \hat F_n(y) = \frac1n \sum_{i=1}^n \ind_{Y_i \leq y} = \frac1n \sum_{i=1}^n \ind_{X_i \in A} = \hat P_n(A).
    \]
    
    By the DKW inequality we find
    \begin{align*}
    	\PP\qty(\abs{\hat P_n(A) - P(A)} > \eps) = \PP \qty(\sup_{y \in \RR} \abs{\hat F_n(y) - F(y)} > \eps) \leq 2e^{-2n\eps^2}.
    \end{align*}
\end{proof}

\begin{question}
	Let $X \sim \Rm{Bin}(n, p)$. Compare the Hoeffding, Bennett, and Bernstein upper bounds on $\PP(X/n \geq \frac12)$ as $p \to 0$. 
\end{question}

\begin{solution}
	Note that $X/n$ is the average of $n$ i.i.d.\ random variables $Y_i \sim \Rm{Bern}(p)$, where $Y_i \in [0, 1]$ for all $i$.  
	
	We start with Hoeffding's inequality. In this case, we have
	\[
	\PP(X/n - p \geq x) \leq \exp(-\frac{2n^2(1/2)^2}{n}) = \exp(- \frac n2),
	\]
	and this bound also holds as $p \to 0$. 
	
	We continue with Bennett's inequality. We consider the random variables $Y_i - p$, which are bounded from above by $b = 1 - p$. Now Bennett's inequality tells us, with $\sigma_p^2 = \Var(Y_i - p) = p(1-p)$ that
	\begin{align*}
		\PP(X/n \geq x) &\leq \exp(- \frac{n}{(1-p)^2} h\qty(\frac{1-p}{2p(1-p)})) = \exp\qty(- \frac{n}{(1-p)^2} h\qty(\frac{1}{2p})).
	\end{align*}
We compute
\[
h\qty(\frac1{2p}) = (1 + \frac1{2p}) \log(1 + \frac1{2p}) - \frac1{2p} = \log(1 + \frac1{2p}) + \frac1{2p} \qty(\log(1 + \frac1{2p}) - 1) \overset{p \downarrow 0}\to +\infty. 
\]
Since $\frac{n}{(1-p)^2}$ is clearly bounded by $n$, we conclude that 
\[
\PP(X/n \geq x) \to e^{-\infty} = 0. 
\]
	
%   Bennett's inequality tells us that
%	\[
%	\PP(X/n \geq \frac12) \leq \exp\qty(- n\nu h(1/(2\nu))), 
%	\]
%	where $h(u) = (1 + u) \log(1+ u)$ and $\nu = p(1-p)$. Plugging this in yields
%	\[
%	\exp(-n\nu h(x/\nu)) = \exp(-n p(1-p) \qty(1 + \frac{1}{2p(1-p)})\log(1 + \frac{1}{2p(1-p)}))
%	\]

We finish with Bernstein's inequality.  We have for $q \geq 3$ and $p \leq\frac12$ that 
\begin{align*}
	\EE[(Y_i - p)_+^q] &= p(1-p)^q = \sigma_p^2 (1-p)^{q-1} = (q! \sigma_p^2 (1-p)^{q-2}/2)\cdot (2(1-p)/q!) \\
	&\leq q!\sigma_p^2 ((1-p)/3)^{q-2}/2.
\end{align*}

Now Bernstein's inequality tells us that
\[
\PP(X/n \geq \frac12 \leq \exp\qty(- \frac{n(1/2)^2}{2(\sigma_p^2 + (1-p)/6)}) = \exp(- \frac{n}{8\sigma_p^2 + 4(1-p)/3}) \overset{p \to 0}{\to} \exp(-\frac{3n}{4}). \]

Of course, the true limit is 0 for any $n$, which is only given by Bennett's inequality. We also see that Hoeffding's inequality gives the worst result. 
\end{solution}

\begin{question}
	Derive the following alternative form of Bernstein's inequality: under the same conditions, 
	\[
	\PP\qty(\bar X \geq \sqrt{\frac{2\sigma^2 \log(1/\delta)}{n}} + \frac cn \log(1/\delta)) \leq \delta
	\]
	for every $\delta \in (0, 1]$. 
\end{question}

\begin{proof}
	Define $x^* \ceq \frac{2^{1/2}\sigma}{n^{1/2}} \log^{1/2}(\frac1\delta) + \frac cn \log(\frac 1\delta)$. Then we have
	\[
	(x^*)^2 = \frac{2\sigma^2}{n}\log(\frac1\delta) + \frac{2^{3/2}\sigma c}{n^{3/2}} \log^{3/2}\qty(\frac1\delta) + \frac{c^2}{n^2} \log^2\qty(\frac1\delta),
	\]
	and therefore
	\begin{align*}
	-\frac{n(x^*)^2}{2(\sigma^2 + cx)} &= - \frac{2\sigma^2 \log(\frac1\delta) + 2^{3/2}\sigma c \log^{3/2}(\frac1\delta)/n^{1/2} + c^2 \log^2(\frac1\delta)/n}{2\sigma^2 + 2^{3/2} \sigma c \log^{1/2}(\frac1\delta)/n^{1/2} + 2c^2\log(\frac1\delta)/n} \\
	&= -\log(\frac1\delta) \frac{2\sigma^2 + 2^{3/2} \sigma c/n^{1/2} + c^2\log(1/\delta)/n}{2\sigma^2 + 2^{3/2} \sigma c/n^{1/2} + 2c^2\log(1/\delta)/n} \\
	&\geq -\log(\frac1\delta) = \log(\delta),
	\end{align*}
	
	so by Bernstein's inequality we have
	\[
	\PP(\bar X \geq x^*) \leq \exp(\log(\delta)) = \delta, 
	\]
	which is what we wanted to prove. 
	
%	We have 
%	\[
%	\PP(\bar X \geq x) \leq \exp(- \frac{nx^2}{2(\sigma^2 + cx)}) \eqqcolon \delta. 
%	\]

	Now we just need express $x$ in terms of $\delta$: taking logarithms on both sides we obtain
	\[
	- \frac{nx^2}{2(\sigma^2 + cx)} = \log(\delta) \implies nx^2 = 2(\sigma^2 + cx) \log(1/\delta) \implies nx^2 - 2c\log(1/\delta) x - 2\sigma^2\log(1/\delta) = 0.
	\]
	
	Using the abc-formula with the fact that $x \geq 0$ yields 
	\begin{align*}
	x &= \frac{2c \log(1/\delta) + \sqrt{4c^2\log^2(1/\delta) + 8n\sigma^2\log(1/\delta)}}{2n} \\
	&= \frac cn \log(1/\delta) + \sqrt{\frac{c^2}{n^2}\log^2(1/\delta) + \frac{2\sigma^2}{n} \log(1/\delta)} \\
	&\geq \frac cn \log(1/\delta) + \sqrt{\frac{2\sigma^2}{n} \log(1/\delta)}. 
	\end{align*}

So we have ?????
\end{proof}

\begin{question}
	\begin{enumerate}[(a)]
		\item Let $X_1, \dotsc, X_n \iid F$ and let $\hat F_n$ denote their empirical distribution function. For $t_1 < \dotsb < t_k$, write down the distribution of 
		\[
		n\qty(\hat F_n(t_1), \hat F_n(t_2) - \hat F_n(t_1), \dotsc, \hat F_n(t_k) - \hat F_n(t_{k-1}), 1 - \hat F_n(t_k)). 
		\]
		\item Find the asymptotic distribution of $n^{1/2} (\hat F_n(t_1) - F(t_1), \dotsc, \hat F_n(t_k) - F(t_k))$. 
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[(a)] \item
	Write $n \hat F_n(t) = \sum_{i=1}^n \ind_{X_i \leq t} = \# \qty{i \mid X_i \leq t}$, and analogously, for $t < u$, 
	$n (\hat F_n(u) - \hat F_n(t)) = \#\qty{i \mid t < X_i \leq u}$. 
	
	Then, defining $t_0 = -\infty$ and $t_{k+1} = \infty$,  we find that
	\begin{gather*}
	\PP\qty[ n \qty(\hat F_n(t_1), \dotsc, 1 - \hat F_n(t_k)) = (a_1, \dotsc, a_{k+1})] \\ = \PP[\text{exactly $a_i$ of the $X_i$ lie in $(t_{i-1}, t_i]$ for $i = 1, \dotsc, n$}].
	\end{gather*}

In this case, we have a multinomial distribution with $n$ trials and probabilities $F(t_1), F(t_2) - F(t_1), \dotsc, F(t_k) - F(t_{k-1}), 1 - F(t_k)$. Therefore, the probability is 0 if $\sum_i a_i \neq n$ and else it is
\[
\frac{n!}{a_1! \dotsb a_{k+1}!} F(t_1)^{a_1} \dotsb (1 - F(t_k))^{a_{k+1}}. 
\]

\item By the central limit theorem, the asymptotic distribution is $N(0, \Sigma)$, where $\Sigma$ is the covariance matrix of $(\hat F_n(t_1), \dotsc, \hat F_n(t_k))$. We will compute the entries of $\Sigma$. 

Choose $t \in \RR$ arbitrarily. Then we have
\begin{align*}
	\Var(\hat F_n(t)) &= \EE[\hat F_n^2(t)] - \EE[\hat F_n(t)]^2 = \EE\qty[\qty(\frac1n\sum_i \ind_{X_i \leq t})^2] - F^2(t) \\
	&= \frac1{n^2}\EE\qty[\sum_i \ind_{X_i \leq t} + 2\sum_{i < j} \ind_{X_i \leq t} \ind_{X_j \leq t}] - F^2(t) \\
	&= \frac{F(t) + (n-1) F^2(t)}{n} - F^2(t) = \frac{F(t)(1 - F(t))}{n},
\end{align*}
so we have computed the diagonal entries $\Sigma_{ii} = \frac{F(t_i)(1 - F(t_i))}{n}$. 

Now we must compute the covariances: assume $s < t$, then  
\begin{align*}
	\Cov(\hat F_n(s), \hat F_n(t)) &= \EE[\hat F_n(s) \hat F_n(t)] - \EE[\hat F_n(s)] \EE[\hat F_n(t)] \\
	&= \frac1{n^2} \sum_{i, j} \EE[\ind_{X_i \leq s} \ind_{X_j \leq t}] - F(s)F(t) \\
	&= \frac1{n^2} \qty(n F(s) + n(n-1) F(s)F(t)) - F(s)F(t) \\
	&= \frac{F(s) + (n-1) F(s) F(t)}{n} - F(s)F(t) = \frac{F(s) - F(s)F(t)}{n}. 
\end{align*}
This gives the diagonal entries $\Sigma_{ij} = \frac{F(t_i) - F(t_i)F(t_j)}{n}$ for $i < j$. 
In the end, we find
\[
\Sigma_{ij} = \frac1n \cdot \begin{cases}
	F(t_i)(1 - F(t_i)) &\text{if $i = j$}, \\
	F(t_{\min(i, j)}) - F(t_i) F(t_j) &\text{if $i \neq j$}. 
\end{cases}
\]
\end{enumerate} 
\end{solution}

\begin{question}
	We say that a continuous process $(B_t)_{t \in [0, 1]}$ is a \emph{standard Brownian motion} on $[0, 1]$ if $B_0 = 0$ and if, for $0 \leq s_1 \leq t_1 \leq \dotsb \leq s_k \leq t_k \leq 1$, we have $(B_{t_1} - B_{s_1}, \dotsb, B_{t_k} - B_{s_k}) \sim N_k(0, \Sigma)$, where $\Sigma \ceq \diag(t_ 1- s_1, \dotsb, t_k - s_k)$. The process $(W_t)_{t \in [0, 1]}$ defined by $W_t \ceq B_t - tB_1$ is called a \emph{Brownian bridge}, or \emph{tied-down Brownian motion}, because $W_0 = W_1 = 0$. Compute the distribution of $(W_{t_1}, \dotsc, W_{t_k})$. 
\end{question}

\begin{solution}
	Note that $W_t = B_t - t B_1 = (1-t) (B_t - B_0) - t (B_1 - B_t)$. Now, since $(B_t - B_0)$ and $(B_1 - B_t)$ are independent with distributions $N(0, t)$ and $N(0, 1-t)$ distributions respectively, we find that 
	\[
	W_t \sim (1-t) N(0, t) + t N(0, 1 - t) = N\qty(0, \frac{t}{\sqrt{1 - t}}) + N\qty(0, \frac{1-t}{\sqrt t}) = N\qty(0, \frac{t}{\sqrt{1 - t}} + \frac{1-t}{\sqrt t}). 
	\]
	
	??
\end{solution}

\begin{question}
	Let $\phi$ denote the standard normal density function, which is a bounded, second-order kernel. For $r \in \NN_0$, define the $r$-th Hermite polynomial $H_r$ by $H_r(x) \ceq (-1)^r \phi^{(r)}(x) / \phi(x)$. Prove that $H_r$ is a monic polynomial of degree $r$ that is even if $r$ is even and odd if $r$ is odd. Show further that
	\[
	\int_{-\infty}^\infty H_r(u) H_s(u) \phi(u) \dd{u} = \begin{cases}
		(2\pi)^{1/2} r!, & r=s, \\ 0 , & r \neq s.
	\end{cases}
	\]
	Now fix an integer $\ell \geq 2$ and define 
	\[
	K_\ell(u) \ceq \sum_{r=0}^{\ell -1 } \frac{H_r(0) H_r(u)}{(2\pi)^{1/2} r!} e^{-u^2/2}. 
	\]
	Prove that $K_\ell$ is a bounded kernel of order $\ell$. 
\end{question}

\begin{proof}
 	We prove this by induction on $r$. For $r = 0$, we have $H_0(x) = 1$, which is indeed an even monic polynomial of degree 0. 
 	Now, suppose the claim holds for a given $r$, that is, $H_r(x) = (-1)^r \phi^{(r)}(x)/\phi(x) = p(x)$ for some monic polynomial $p$ of degree $r$, which is even if $r$ is even and odd if $r$ is odd. Then we have
 	\begin{align*}
 	\phi^{(r)}(x) &= (-1)^r p(x) \phi(x) = (-1)^r (2\pi)^{-1/2} p(x) \exp(-x^2/2) \\
 	\phi^{(r+1)}(x) &= (-1)^r 2\pi^{-1/2} \qty(p'(x) - x p(x)) \exp(-x^2/2) \\
 	H_{r+1 }(x) &= (-1)^r (-1)^{r+1} \qty(p'(x) - x p(x)) = xp(x) - p'(x). 
 	\end{align*}
 	Now it is clear that $H$ is a monic polynomial of degree $r$ since $p$ was assumed monic. Furthermore, since derivatives of even functions are odd and vice versa, it is clear that $H$ is odd if $p$ is even and vice versa. 

	Now, suppose $r < s$, then 
	\begin{align*}
		\int_{-\infty}^\infty H_r(u) H_s(u) \phi(u) \dd{u} &= (-1)^s \int_{-\infty}^\infty H_r(u) \phi^{(s)}(x) \dd{u} \overset{\text{IBP}}= \int_{-\infty}^\infty H_r^{(s)}(u) \phi(u) \dd{u} = 0,
	\end{align*}
since $H_r^{(s)} = 0$ if $r < s$. 

However, if $r = s$, then following the same line of reasoning as above and using the fact that $H_r^{(r)} = r!$, we find
\[
\int_{-\infty}^\infty H_r^2(u) \phi(u) \dd{u} = r! \int_{-\infty}^\infty \phi(u) \dd{u} = r!. 
\]

Now we consider $K_\ell$: we have
\[
\int_{-\infty}^\infty K_\ell(u) \dd{u} = \sum_{r=0}^{\ell - 1} \frac{H_r(0)}{r!} \int_{-\infty}^\infty H_r(u) \phi(u) \dd{u} =  \sum_{r=0}^{\ell - 1} \frac{(-1)^r H_r(0)}{r!} \int_{-\infty}^\infty \phi^{(r)}(u) \dd{u}.
\]
Note that every term in the above sum vanishes except for the $r = 0$ term due to the integral, and the $r = 0$ term is 1, so $K_\ell$ is indeed a kernel. 

We verify that $K_\ell$ has order $\ell$: let $j \in \qty{1, \dotsc, \ell - 1}$, then we have
\begin{align*}
	\int_{-\infty}^\infty u^j K_\ell(u) \dd{u} = \sum_{r=0}^{\ell - 1} \frac{(-1)^r H_r(0)}{r!} \int_{-\infty}^\infty u^j H_r(u) \phi(u) \dd{u}.
\end{align*}
Write $u^j = \sum_{k=0}^j c_k H_k(u)$, then the integral will vanish unless $k = r$, so we get
\[
\int_{-\infty}^\infty u^j K_\ell(u) = \sum_{r=0}^{j} (-1)^r c_r H_r(0) = \sum_{r=0}^j c_r H_r(0) = 0^j = 0,
\]
since $H_r(0) = 0$ for $r$ odd. 
%Clearly, if $j < r$, integral in the sum vanishes. If $j = r$, the integral yields $r!$. So we have
%\[
%\int_{-\infty}^\infty u^j K_\ell(u) \dd{u} = (-1)^j H_j(0) + \sum_{r=0}^{j-1} \int_{-\infty}^\infty u^j \phi^{(r)}(u) \dd{u}. 
%\]
%
%??
\end{proof}

\begin{question}
	For $\beta \in \NN$ and $L > 0$, define the \emph{Sobolev class} $\SS(\beta, L)$ to be the set of $(\beta - 1)$ times differentiable functions $f \colon \RR \to \RR$ for which $f^{(\beta - 1)}$ is absolutely continuous with $L^1$ derivative satisfying $\norm{f^{(\beta)}}_{L^2} \leq L$. Recalling the Nikolski class $\Nn(\beta, L)$ from lectures, prove that $\SS(\beta, L) \subseteq \Nn(\beta, L)$. Writing $\FF_\SS(\beta, L)$ for the densities in $\SS(\beta, L)$, deduce that a kernel density estimator $\hat f_n$ constructed from $X_1, \dotsc, X_n \iid f \in \FF_\SS(\beta, L)$ with a kernel $K$ of order $\ell \ceq \beta$ and bandwidth $h > 0$ satisfies
	\[
	\MISE(\hat f_n) \leq \frac1{nh} R(K) + \frac1{((\ell - 1)!)^2} R(f^{(\beta)}) \mu_\beta^2(K) h^{2\beta}. 
	\]
\end{question}

\begin{proof}
	Let $f \in \SS(\beta, L)$ and $t \in \RR$, then we have 
	\begin{align*}
		\int_\RR \qty[f^{(\beta - 1)}(x + t) - f^{(\beta - 1)}(x)]^2 \dd{x} &= \int_\RR \qty[\int_x^{x+t} f^{(\beta)}(y) \dd{y}]^2 \dd{x} \\
		&= \int_\RR \qty[ \int_\RR \ind_{(x, x+t)}(y) f^{(\beta)}(y) \dd{y} ]^2 \dd{x} \\
		&\overset{GM}\leq  \qty{ \int_\RR \qty( \int_\RR \ind_{(y-t, y)}(x) f^{(\beta)}(y)^2 \dd{x})^{1/2} \dd{y}}^2 
	\end{align*}
??
\end{proof}

\begin{question}
	\begin{enumerate}[(a)]
		\item 	Verify the algebraic identity
		\[
		\phi_\sigma(x-\mu)\phi_{\sigma'}(x-\mu') = \phi_{\sigma\sigma'/(\sigma^2 + \sigma'^2)^{1/2}}(x - \mu^*) \phi_{(\sigma^2 + \sigma'^2)^{1/2}}(\mu - \mu')
		\]
		where $\mu^* \ceq (\sigma'^2 \mu + \sigma^2\mu') / (\sigma^2 + \sigma'^2)$, and $\phi_\sigma$ is the $N(0, \sigma^2)$ density. 
		
		\item Let $X_1, \dotsc, X_n \iid N(0, \sigma^2)$. Taking $K$ to be the $N(0, 1)$ density, show that the MISE of the kernel density estimate $\hat f_n$ with kernel $K$ and bandwidth $h$ can be expressed exactly as
		\[
		\MISE(\hat f_n) = (2\pi)^{-1/2} \qty{\frac1{nh} + (1 - \frac1n) \frac{1}{(h^2 + \sigma^2)^{1/2}} - \frac{2^{3/2}}{(h^2 + 2\sigma^2)^{1/2}} + \frac1\sigma}. 
		\]
	\end{enumerate}

\end{question}

\begin{proof}
	\begin{enumerate}[(a)]
		\item We have
		\begin{align*}
			&\phantom{=\ }\frac{(x-\mu)^2}{\sigma^2} + \frac{(x - \mu')^2}{\sigma'^2} \\
			&= \frac{\sigma'^2(x - \mu)^2 + \sigma^2(x - \mu')^2}{\sigma^2\sigma'^2} \\
			&= \frac{(\sigma^2 + \sigma'^2)x^2 - 2 (\sigma'^2 \mu + \sigma^2 \mu')x + \sigma'^2 \mu^2 + \sigma^2\mu'^2}{\sigma^2 \sigma'^2} \\
			&=  \frac{(\sigma^2 + \sigma'^2)(x^2 - 2\mu^*x) + \sigma'^2 \mu^2 + \sigma^2\mu'^2}{\sigma^2 \sigma'^2} \\
			&= \frac{(\sigma^2 + \sigma'^2)(x^2 - 2\mu^* + \mu^{*2}) - (\sigma^2 + \sigma'^2)\mu^{*2} + \sigma'^2 \mu^2 + \sigma^2\mu'^2}{\sigma^2 \sigma'^2} \\
			&= \frac{(x - \mu^*)^2}{(\sigma\sigma'/(\sigma^2 + \sigma'^2)^{1/2})^2} + \frac{\sigma'^2 \mu + \sigma^2 \mu'^2 - (\sigma'^2\mu + \sigma^2\mu')^2/(\sigma^2 + \sigma'^2)}{\sigma^2\sigma'^2} \\
			&= \frac{(x - \mu^*)^2}{(\sigma\sigma'/(\sigma^2 + \sigma'^2)^{1/2})^2} + \frac{(\sigma^2 + \sigma'^2)(\sigma'^2\mu + \sigma^2\mu'^2) - (\sigma'^2\mu + \sigma^2\mu')^2}{\sigma^2 \sigma'^2(\sigma^2 + \sigma'^2)} \\
			&= \frac{(x - \mu^*)^2}{(\sigma\sigma'/(\sigma^2 + \sigma'^2)^{1/2})^2} + \frac{(\mu - \mu')^2}{\sigma^2 + \sigma'^2} \\
			&= \frac{(x - \mu^*)^2}{(\sigma\sigma'/(\sigma^2 + \sigma'^2)^{1/2})^2} + \frac{(\mu - \mu')^2}{((\sigma^2 + \sigma'^2)^{1/2})^2}, 
		\end{align*}
	which proves the claim. 
	\item Let $K = \phi_1$ and define $K_h(x) \ceq h^{-1} K(x/h)$ so $K_h = \phi_h$. Then recall from the lectures that
	\[
	\MISE(\hat f_n) = \frac1n \int_\RR \qty[(\phi_h^2 * \phi_\sigma)(x) - (\phi_h * \phi_\sigma)^2(x)] \dd{x} + \int_{-\infty}^\infty \qty[ (\phi_h * \phi_\sigma)(x) - \phi_\sigma]^2 \dd{x}
	\]
	\end{enumerate}
\end{proof}
\end{document}