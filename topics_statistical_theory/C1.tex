\section{Basic concepts}
\subsection{Parametric vs nonparametric models}
\begin{definition}
    A \emph{statistical model} is a family of possible data-generating mechanisms. If the parameter space $\Theta$ is finite-dimensional, we speak of a \emph{parametric model}. 
    
    A model is called \emph{well-specified} if there is a $\theta_0 \in \Theta$ for which the data was generated from the distribution with parameter $\theta_0$, and otherwise it is called \emph{misspecified}. 
\end{definition}

\begin{recap}
    Let $(Y_n)$ be a sequence of random vectors and $Y$ a random vector. 
    \begin{enumerate}
        \item We say that $(Y_n)$ \emph{converges almost surely} to $Y$, notation $Y_n \toas Y$, if $\PP(Y_n \to Y) = 1$. 
        \item We say that $(Y_n)$ \emph{converges in probability} to $Y$, notation $Y_n \toprob Y$, if for every $\eps > 0$ we have $\PP(\norm{Y_n - Y} > \eps) \to 0$.
        \item We say that  $(Y_n)$ \emph{converges in distribution} to $Y$, notation $Y_n \todist Y$, if $\PP(Y_n \leq y) \to \PP(Y \leq y)$ for all $y$ where the distribution function of $Y$ is continuous. 
        
        This is equivalent to the condition that $\EE[f(Y_n)] \to \EE[f(Y)]$ for all bounded Lipschitz functions $f$. 
    \end{enumerate}
    It is known that $Y_n \toas Y \implies Y_n \toprob Y \implies Y_n \todist Y$. 
    
    If $(Y_n)$ is a sequence of random vectors and $(a_n)$ is a positive sequence, then we write $Y_n = O_p(a_n)$ if, for all $\eps > 0$, there exists $C > 0$ such that for sufficiently large $n$ we have
    \[
    \PP\qty(\frac{\norm{Y_n}}{a_n} > C) < \eps. 
    \]
    We write $Y_n = o_n(a_n)$ if $Y_n/a_n \toprob 0$. 
\end{recap}
In a well-specified parametric model, the maximum likelihood estimator (MLE) $\hat\theta_n$ typically satisfies $\hat\theta_n - \theta_0 \in O_p(n^{-1/2})$. On the other hand, if the model is misspecified, any inference can give very misleading results. To circumvent this problem, we consider \emph{nonparametric models}, which make much weaker assumptions. Such infinite-dimensional models are much less vulnerable to model misspecification, however we will typically pay a price in terms of a slower convergence rate than in well-specified parametric models. 

\begin{example}
    Examples of nonparametric models include: 
    \begin{enumerate}
        \item Assume $X_1, \dotsc, X_n \iid F$ for some unknown distribution function $F$. 
        \item Assume $X_1, \dotsc, X_n \iid f$ for some unknown density $f$ belonging to a smoothness class.
        \item Assume $Y_i = m(x_i) + \eps_i$ ($i = 1, \dotsc, n$), where the $x_i$ are known, $m$ is unknown and belongs to some smoothness class, and the $\eps_i$ are i.i.d.\ with $\EE (\eps_i) = 0$ and $\Var(\eps_i) = \sigma^2$. 
    \end{enumerate}
\end{example}

\subsection{Estimating an arbitrary distribution function}
\begin{definition}
    Let $\Cal F$ denote the class of all distribution functions on $\RR$ and suppose $X_1, \dotsc, X_n \iid F \in \Cal F$. The \emph{empirical distribution function} $\hat{F_n}$ of $X_1, \dotsc, X_n$ is defined as
    \[
    \hat F_n(x) \ceq \frac1n \sum_{i=1}^n \ind_\qty{X_i \leq x}.
    \]
\end{definition}

\begin{recap}
    The \emph{strong law of large numbers} tells us that if $(Y_n)$ are i.i.d.\ with finite mean $\mu$, then $\bar{Y} \ceq \frac1n \sum_{i=1}^n Y_i \toas \mu$. 
\end{recap}

Note that the strong law of large numbers immediately implies that $\hat{F_n}(x)$ converges almost surely to $F(x)$ as $n \to \infty$ for all fixed $x \in \RR$. However, the following stronger result states that this convergence holds uniformly in $x$: 

\begin{theorem}[Glivenko-Cantelli] \label{thm:GC}
    Let $X_1, X_2, \dotsc \iid F$. Then we have 
    \[
    \sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)}\toas 0. 
    \]
\end{theorem}

The main idea of the proof is to ``control'' $\hat{F_n}$ in a finite number of points $x_1, \dotsc, x_k$, and then deduce what happens between those points using the fact that distributions are increasing and right-continuous. On \href{https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem#Proof}{Wikipedia}, 
	a simplified proof can be found assuming that $F$ is continuous, which still encapsulates the main idea.  For the general proof, we need the following fact about quantile functions:
	\begin{recap}
		For any distribution function $F$, its \emph{quantile function} is defined as
		\[
		F^{-1} \colon (0, 1] \to \RR \cup \qty{\infty} \colon p \mapsto \inf \qty{x \in \RR \mid F(x) \geq p}. 
		\]
		Note that since $F$ is right-continuous and non-decreasing, the infimum is well-defined and may be replaced by a minimum, and therefore we always have $F(F^{-1}(p)) \geq p$. 
		
		When necessary, we also define $F^{-1}(0) \ceq \sup\qty{x \in \RR \mid F(x) = 0}$.

	\end{recap}
	
	
\begin{proof}
    Let $\eps > 0$ and choose $k$ such that $\frac1k \leq \eps$. Now set $x_0 \ceq -\infty$ and $x_i \ceq F^{-1}(\frac ik)$. 
   Then we have 
    \[
    F(x_i-) - F(x_{i-1}) \leq \frac ik - \frac{i-1}{k} = \frac1k \leq \eps
    \]
    for all $i$. Define $X = \qty{x_1, \dotsc, x_k, x_1-, \dotsc, x_k-}$ (we abuse notation here) and 
    \[
    \Omega_{n, \eps} \ceq \qty{\max_{x \in X} \sup_{m \geq n} \abs{\hat F_m(x) - F(x)} \leq \eps}. 
    \]
    By a union bound and the strong law of large numbers we have
    \[
    \PP_F(\Omega_{n, \eps}\C) \leq \sum_{x \in X} \PP(\sup_{m \geq n} \abs{\hat F_m(x) - F(x)} > \eps) \to 0, 
    \]
    since $\hat F_m(x)$ and $\hat F_m(x-)$ are the sample averages of the random variables $\ind_{X \leq x}$ and $\ind_{X < x}$ and therefore converge almost surely to their means $F(x)$ and $F(x-)$. 
    
    Now, fixing $x \in [x_{i-1}, x_i)$ we have for any $n \in \NN$, $m \geq n$
    \begin{align*}
    \hat F_m(x) - F(x) &\leq \hat F_m(x_i-) - F(x_{i-1}) \leq \hat F_m(x_i-) - F(x_i-) + F(x_i-) - F(x_{i-1}) \\
    &\leq \max_{x \in X} \sup_{m \geq n} \abs{\hat F_m(x) - F(x)} + \eps, 
        \end{align*}
    and analogously $F(x) - \hat F_n(x) \leq \max_{x \in X}\sup_{m \geq n} \abs{\hat F_m(x) - F(x)} + \eps$.
    
    Therefore, we have 
    \begin{align*}
    	\PP_F\qty(\sup_{m \geq n} \sup_{x \in \RR} \abs{\hat F_m(x) - F(x)} > 2\eps) \leq \PP_F \qty(\max_{x \in X} \sup_{m \geq n} \abs{\hat F_m(x) - F(x)} > \eps) = \PP(\Omega_{n, \eps}\C) \to 0. 
    \end{align*}

	Noting that $\eps$ was arbitrary, we conclude
	\begin{align*}
		\PP_F\qty(\sup_{x \in \RR} \abs{\hat F_n(x) - F(x)} \to 0) &= \PP_F\qty(\forall L \in \NN \ \exists n \in \NN \ \forall m \geq n : \sup_{x \in \RR} \abs{\hat F_n(x) - F(x)} \leq \frac1L) \\
		&= \PP_F \qty(\bigcap_{L=1}^\infty \bigcup_{n = 1}^\infty  \bigcap_{m = n}^\infty  \qty{\sup_{x \in \RR} \abs{\hat F_n(x) - F(x)} \leq \frac1L}) \\
		&= \lim_{L \to\infty} \lim_{n \to\infty} \PP_F \qty(\sup_{m \geq n} \sup_{x \in \RR} \abs{\hat F_m(x) - F(x)} \leq \frac 1L) = 1. 
	\end{align*}
\end{proof}

\begin{theorem}[Dvoretzky-Kiefer-Wolfowitz] 
    Under the conditions of \cref{thm:GC}, for every $\eps > 0$ it holds that
    \[
    \PP_F \qty(\sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)} > \eps) \leq 2e^{-2n\eps^2},
    \]
    and this is a tight bound. 
\end{theorem}

We will not prover this theorem, however, we will explore a few consequences. 
One of these consequences is the following: 

\begin{corollary}[Uniform Glivenko-Cantelli theorem] Under the conditions of \cref{thm:GC}, for every $\eps > 0$, it holds that
    \[
    \sup_{F \in \Cal F} \PP_F \qty(\sup_{m \geq n} \sup_{x \in \RR} \abs{\hat{F_m}(x) - F(x)} > \eps) \to 0 \quad\text{as $n \to\infty$.}
    \]
\end{corollary}

\begin{proof}
    By a union bound, the DKW inequality, and convergence of the geometric series we have
    \begin{align*}
    \sup_{F \in \Cal F} \PP_F \qty(\sup_{m \geq n} \sup_{x \in \RR} \abs{\hat{F_m}(x) - F(x)} > \eps) &\leq \sup_{F \in \Cal F} \sum_{m = n} \PP_F \qty(\sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)} > \eps) \\
    &\leq 2 \sum_{m=n}^\infty e^{-2m\eps^2},    \end{align*}
which converges to 0 as it is the tail of a converging sum. 
\end{proof}

\begin{consequence}
For another consequence, we consider the problem of finding a confidence band for $F$. Given $\alpha \in (0, 1)$, set $\eps_n \ceq \sqrt{-\frac1{2n}\log(\alpha/2)}$. Then the DKW inequality tells us that
\[
\PP_F \qty( \sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)} > \eps_n) \leq \alpha,
\]
or equivalently, that 
\[
\PP_F\qty(\hat{F_n}(x) - \eps_n \leq F(x) \leq \hat{F_n}(x) + \eps_n \ \text{for all $x \in \RR$}) \geq 1 - \alpha. 
\]
\end{consequence}

\begin{discussion}
Let $U_1, \dotsc, U_n \iid U(0, 1)$ with empirical distribution function $\hat{G}_n$, and let $X_1, \dotsc, X_n \iid F$. Then, we have
\begin{align*}
	\hat{G_n}(F(x)) = \frac1n \sum_{i=1}^n \ind_\qty{U_i \leq F(x)} \deq \frac1n \sum_{i=1}^n \ind_\qty{X_i \leq x} = \hat{F_n}(x), 
\end{align*}
where $\deq$ means equality in distribution. It follows that
\[
\sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)} \deq \sup_{x \in \RR} \abs{\hat{G_n}(F(x)) - F(x)} \leq \sup_{t \in [0, 1]} \abs{\hat{G_n}(t) - t},
\]
with equality if $F$ is continuous. We conclude that if $F$ is continuous, the distribution of $\sup_{x \in \RR} \abs{\hat{F_n}(x) - F(x)}$ does not depend on $F$, and that continuous functions give a ``worst-case'' scenario for $\sup_{x \in \RR} \abs{\hat F_n(x) - F(x)}$. 
\end{discussion}

\begin{discussion}
	Other generalisations of \cref{thm:GC} include Uniform Laws of Large Numbers. Let $X, X_1, \dotsc, X_n$ be i.i.d.\ on a measurable space $(\Cal X, \Cal A)$, and $\Cal G$ a class of measurable functions on $\Cal X$. We say that $\Cal G$ satisfies a ULLN if 
	\[
	\sup_{g \in \Cal G} \abs{\frac1n \sum_{i=1}^n g(X_i) - \EE[g(X)]} \toas 0. 
	\]
	In \cref{thm:GC}, we showed that $\Cal  G = \qty{ \ind_\qty{\cdot \leq x} \mid x \in \RR}$ satisfies a ULLN. 
\end{discussion}



\begin{recap}
    We recall the central limit theorem: if $X_1, \dotsc, X_n$ are i.i.d.\ random variables with mean $\mu$ and variance $\sigma^2 < \infty$, then $\sqrt n \qty(\bar{X_n} - \mu) \todist N(0, \sigma^2)$. 
    
    Dividing by $\sigma$ yields
    \[
    \frac{\sqrt n \qty(\bar{X_n} - \mu)}{\sigma} \todist N(0, 1), 
    \]
    and multiplying both sides by $n$ and writing $V_i = \sum_{j=1}^i X_j$ we obtain
    \[
    \frac{V_i - \EE V_i}{\sqrt{\Var(V_i)}} \todist N(0, 1).
    \]
\end{recap}
\begin{discussion}
Another extension starts with the observation that $\sqrt n \qty(\hat{F_n}(x) - F(x)) \todist N(0, \sigma^2)$, where
\[
\sigma^2 = \Var(\ind_\qty{X \leq x}) = \EE[\ind_{X \leq x}^2] - \EE[\ind_{X \leq x}]^2 = F(x) - F(x)^2 = F(x)(1 - F(x)). 
\]
This can be strengthened by considering $\qty(\sqrt n (\hat{F_n}(x) - F(x)) : x \in \RR)$ as a stochastic process. 
\end{discussion}


\subsection{Order statistics and quantiles}
\begin{recap}
			
	If $U \sim U(0, 1)$ and $X \sim F$, then for any $x \in \RR$ we have
	\[
	\PP(F^{-1}(U) \leq x) = \PP(U \leq F(x)) = F(x) = \PP(X \leq x). 
	\]
	This can be written simply as $F^{-1}(U) \deq X$. 
\end{recap}
\begin{definition}
    Let $X_1, \dotsc, X_n \iid F \in \Cal F$. The \emph{order statistics} are the ordered samples $X_{(1)} \leq \dotsb \leq X_{(n)}$ (where the original order is preserved in case of a tie). 
\end{definition}

The order statistics of the uniform distribution can be computed explicitly: 
\begin{proposition}
    Let $U_1, \dotsc, U_n \iid U(0, 1)$,  let $Y_1, \dotsc, Y_{n+1} \iid \Exp(1)$, and write $S_j \ceq \sum_{i=1}^j Y_j$ ($j = 1, \dotsc, n+1$). Then
    \[
    U_{(j)} \deq \frac{S_j}{S_{n+1}} \sim \Rm{Beta}(j, n - j + 1) \quad\text{for $j = 1, \dotsc, n$}. 
    \]
\end{proposition}

\begin{proof}
    See example sheet 1, question 1.  
\end{proof}
\begin{definition}
    Let $X_1, \dotsc, X_n \iid F$. Then the \emph{sample quantile function} is defined as 
    \[
    \hat F_n^{-1}(p) = \inf \qty{x \in \RR \mid \hat F_n(x) \geq p}. 
    \]
\end{definition}
Note that the sample quantile function is the quantile function of the empirical distribution function. 

\begin{proposition}
    It holds that $\hat F_n^{-1}(p) = X_{(\ceil{np})}$. 
\end{proposition}

\begin{proof}
    By definition, $\hat F_n^{-1}(p)$ is the smallest value of $x$ for which $\hat F(x)$ is larger than $p$. Note that
    \[
    \hat F(x) \geq p \iff \frac1n \sum_{i=1}^n \ind_\qty{X_i \leq x} \geq p \iff \sum_{i=1}^n \ind_\qty{X_i \leq x} \geq np \iff \sum_{i=1}^n \ind_\qty{X_i \leq x} \geq \ceil{np}.
    \]
    The smallest value of $x$ for which this occurs is the smallest value of $x$ such that exactly $\ceil{np}$ of the variables $X_1, \dotsc, X_n$ satisfy $X_i \leq x$. We conclude that $\hat F_n^{-1}(p) = X_{(\ceil{np})}$. 
\end{proof}

For example, this proposition tells us that $\hat F_n^{-1}(1/2) = X_{(\ceil{n/2})}$, the median of the data. We now explore the distribution of $X_{(\ceil{np})}$. 

\begin{recap}
We recall two theorems. The first is \emph{Slutsky's theorem}:
\begin{theorem}
    Let $(Y_n)$ and $(Z_n)$ be sequences of random vectors with $Y_n \todist Y$ and $Z_n \toprob c$ for some constant $c$. If $g$ is a continuous real-valued function, then $g(Y_n, Z_n) \todist g(Y, c)$. 
\end{theorem}

The second is the \emph{delta method}:
\begin{theorem}
    Let $(Y_n)$ be a sequence of random vectors such that $\sqrt n (Y_n - \mu) \todist Z$. If $g \colon \RR^d \to \RR$ is differentiable at $\mu$, then
    \[
    \sqrt n \qty(g(Y_n) - g(\mu)) \todist g'(\mu) Z. 
    \]
\end{theorem}
\end{recap}

\begin{lemma}
    If $U_1, \dotsc, U_n \iid U(0, 1)$ and $p \in (0, 1)$, then $\sqrt n \qty(U_{\ceil{np}} - p) \todist N(0, p(1-p))$. 
\end{lemma}

\begin{proof}
    Let $Y_1, \dotsc, Y_{n+1} \iid \Exp(1)$, $V_n \ceq \sum_{i=1}^{\ceil{np}} Y_i$ and $W_n \ceq \sum_{i = \ceil{np} + 1}^{n+1} Y_i$. Then $V_n$ and $W_n$ are independent, and we have seen that $U_{\ceil{np}} \sim \frac{V_n}{V_n + W_n}$. 
    
    Noting that $\EE V_n = \Var(V_n) = \ceil{np}$ we find
    \begin{align*}
    \sqrt{n} \qty(\frac{V_n}{n} - p) &= \frac{\sqrt{\ceil{np}}}{\sqrt n} \qty(\frac{V_n - \ceil{np}}{\sqrt{\ceil{np}}}) + \frac{\ceil{np} - np}{\sqrt n} \\
    &= \frac{\sqrt{\ceil{np}}}{\sqrt n}  \qty(\frac{V_n - \EE V_n}{\sqrt{\Var(V_n)}}) +  \frac{\ceil{np} - np}{\sqrt n}.
    \end{align*}
    Now, by the central limit theorem, the term between brackets converges to a standard $N(0, 1)$ distribution. The term $\sqrt{\ceil{np}}/{\sqrt n}$ converges to $\sqrt p$ and the term $(\ceil{np} - np)/\sqrt n$ converges to 0, so by Slutsky's lemma, we find
    \[
    \sqrt{n}\qty(\frac{V_n}{n} - p) \todist \sqrt p N(0, 1) = N(0, p). 
    \]
    
    Letting $q \ceq 1 - p$, 
    an analogous calculation shows that $\sqrt n \qty(\frac{W_n}{n} - q) \to N(0, q)$. 
    
    Now we define $g \colon (0, \infty)^2 \to (0, \infty)$ by $g(x, y) \ceq x / (x + y)$, which is differentiable at $(p, q)$ with derivative
    \[
    \nabla g(x, y) = \mqty[y / (x+y)^2 \\ -x / (x+y)^2] \implies \nabla g(p, q) = \mqty[ q \\ -p]. 
    \]Note that the distribution of $(V_n, W_n)$ is an $N(0, \smqty(p & 0 \\ 0 & q))$ distribution. By the delta method we find
    \begin{align*}
        \sqrt n \qty(U_{\ceil{np}} - p) &\deq \sqrt n\qty( g\qty(\frac{V_n}{n}, \frac{W_n}{n}) - g(p, q)) \\
        &\todist \nabla g(p, q)\T \cdot N\qty(0, \mqty(p & 0 \\ 0 & q)) \\
        &\deq  N\qty(0, \nabla g(p, q)\T  \mqty(p & 0 \\ 0 & q) \nabla g(p, q)) \\
        &\deq  N(0, pq),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
    \end{align*}
since $\nabla g(p, q)\T \mqty(p & 0 \\ 0 & q) \nabla g(p, q) = q^2 p + p^2 q = pq(p+q) = pq$.
\end{proof}

We now relate what we know about the uniform distribution to the  quantile function: 
\begin{theorem}
    Let $p \in (0, 1)$ and let $X_1, \dotsc, X_n \iid F$. Suppose that $F$ is differentiable at $\xi_p \ceq F^{-1}(p)$ with derivative $f(\xi_p)$. Then
    \[
    \sqrt n \qty(X_{(\ceil{np})} - \xi_p) \todist N\qty(0, \frac{p(1-p)}{f(\xi_p)^2}).
    \]
\end{theorem}

\begin{proof}
    Let $U_1, \dotsc, U_n \iid U(0, 1)$, then we know that $F^{-1}(U_i) \deq X_i$ and thus $F^{-1} (U_{(\ceil{np})}) \deq X_{(\ceil{np})}$. Applying the delta method with $g = F^{-1}$, together with the previous theorem yields
    \[
    \sqrt n \qty(X_{(\ceil{np})} - \xi_p) = \sqrt n \qty(F^{-1}(U_{(\ceil{np})}) - F^{-1}(p)) \todist (F^{-1})'(p) \cdot  N(0, p(1-p)).
    \]
    Noting that $(F^{-1})'(p) = \frac1{f(\xi_p)}$ yields the result. 
\end{proof}

\subsection{Concentration inequalities}
We turn our attention to concentration inequalities, with a focus on finite-sample results (instead of results that only hold for $n \to\infty$). 
\begin{definition}
    A random variable $X$ with mean 0 is called \emph{sub-Gaussian} with parameter $\sigma^2$ if
    \[
    M_X(t) = \EE(e^{tX}) \leq e^{t^2\sigma^2/2}
    \]
    for every $t \in \RR$. 
\end{definition}
\begin{remark}
Note that equality holds when $X \sim N(0, \sigma^2)$, since the MGF of an $N(\mu, \sigma^2)$ distribution is given by $t \mapsto \exp(\mu t + \sigma^2 t^2 /2)$. 
	content...
\end{remark}

\begin{recap}
	Recall the \emph{tail bound formula} for the expectation: if $X$ is a nonnegative random variable, then 
	\[
	\EE[X] = \int_0^\infty \PP(X > x) \dd{x}. 
	\]
	
	Furthermore, recall that the \emph{gamma function} is defined for $z \in (0, \infty)$ by
	\[
	\Gamma(z) = \int_0^\infty x^{z-1} e^{-x} \dd{x}
	\]
	and satisfies $\Gamma(n) = (n-1)!$ for all $n \in \NN$. 
	
	Finally, recall the following inequality: for all $a, b \in \RR$ and $p \geq 1$
	\[
	(a+b)^p \leq 2^{p-1} (a^p + b^p). 
	\]
	This follows from the convexity of the function $x \mapsto x^p$. 
\end{recap}

\begin{discussion}[Chernoff bounding]
	Let $X$ be any random variable, then by Markov's inequality, then we have for all $t > 0$ that 
	\[
	\PP(X \geq x) = \PP(e^{tX} \geq e^{tx}) \leq e^{-tx} \EE[e^{tX}] = e^{-tx} M_X(t).
	\]
	Since the left-hand side is independent of $t$, we can minimise over all $t$ and obtain \[
	\PP(X \geq x) \leq \inf_{t > 0} e^{-tx} M_X(t), 
	\]
	which often gives better results than the ``standard'' Markov bound. This technique is called \emph{Chernoff bounding}, and is very useful if bounds on $M_X(t)$ are known. 
\end{discussion}
\begin{proposition}We consider some characterisations of sub-Gaussianity: 
    \begin{enumerate}[(a)]
        \item     Let $X$ be sub-Gaussian with parameter $\sigma^2$. Then 
        \begin{equation} \label{eq:sg_tail_bounds}
        \max\qty{\PP(X \geq x), \PP(X \leq -x)} \leq e^{-x^2/(2\sigma^2)} \quad\text{for every $x \geq 0$}.
        \end{equation}
        
        \item Let $X$ be a random variable which satisfies $\EE(X) = 0$ and \cref{eq:sg_tail_bounds}. Then for every $q \in \NN$ it holds that
        \[
        \EE(X^{2q}) \leq 2 \cdot q! (2\sigma^2)^q \leq q! (2\sigma)^{2q}.
        \]
        
        \item If $X$ is a random variable with $\EE(X) = 0$ and $\EE(X^{2q}) \leq q! C^{2q}$ for all $q \in \NN$, then $X$ is sub-Gaussian with parameter $4C^2$. 
    \end{enumerate}

\end{proposition}


\begin{proof}
    \begin{enumerate}[(a)]
        \item We first consider $\PP(X \geq x)$. 
        By a Chernoff bound, we have
        \[
        \PP(X \geq x) \leq \inf_{t \in \RR} e^{-tx + t^2\sigma^2/2} = e^{-x^2/(2\sigma^2)}, 
        \]
        since the infimum of $t^2\sigma^2/2 - tx$ is attained at $t = x/\sigma^2$.
        
        For $\PP(X \leq -x) = \PP(-X \geq x)$ we can use the fact that $-X$ is also sub-Gaussian with parameter $\sigma^2$. 
        
        \item By the previous part, we have $\PP(\abs{X} \geq x) \leq 2 e^{-x^2/(2\sigma^2)}$. Some calculations give
        \begin{align*}
            \EE(X^{2q}) &= \int_0^\infty \PP(X^{2q} \geq x) \dd{x} = \int_0^\infty \PP(\abs{X} \geq x^{1/(2q)}) \\
            &= 2q \int_0^\infty x^{2q - 1} \PP(\abs{X}\geq x)\dd{x} \\
             &\leq 4q \int_0^\infty x^{2q - 1} e^{-x^2/(2\sigma^2)} \dd{x}.
        \end{align*}
    Now set $t = x^2/2\sigma^2$, so that $x = \sigma (2t)^{1/2}$ and thus $\dd{x} = \sigma (2t)^{-1/2} \dd{t}$. Plugging that in we get
    \begin{align*}
    \EE(X^{2q}) &\leq 4q \int_0^\infty (\sigma(2t)^{1/2})^{2q - 1} e^{-t} \sigma (2t)^{-1/2} \dd{t} = 2^{q+1} q \sigma^{2q} \int_0^\infty t^{q-1} e^{-t} \dd{t} \\
    &= 2^{q+1} q \sigma^{2q} \Gamma(q) = 2 \cdot q! (2\sigma)^q. 
    \end{align*}

\item Note that $x \mapsto e^{-tx}$ is convex for every $t \in \RR$, so $\EE(e^{-tX}) \geq e^{-t \EE(X)} = e^{0} = 1$ by Jensen's inequality. Let $X'$ denote an independent copy of $X$: then $X - X'$ has a symmetric distribution, so all its odd moments vanish. Therefore we find
\begin{align*}
	\EE[e^{tX}] &\leq \EE[e^{-tX'}] \EE[e^{tX}] = \EE[e^{t(X - X')}]= \EE \sum_{q=0}^\infty \qty[\frac{t^q (X - X')^{q}}{q!}] 
	\\
	&\overset\star= \sum_{q=0}^\infty \frac{t^{2q} \EE[(X - X')^{2q}]}{(2q)!} \leq \sum_{q=0}^\infty \frac{2^{2q-1} t^{2q} \qty(\EE[X^{2q}] + \EE[(X')^{2q}])}{(2q)!} \\
	&\leq \sum_{q=0}^\infty  \frac{2^{2q - 1}t^{2q} 2 q! C^{2q}}{(2q)!} =\sum_{q=0}^\infty  \frac{(2tC)^{2q}q!}{(2q)!} = \sum_{q=0}^\infty \frac{(2tC)^{2q}}{\prod_{j=1}^q (q+j)} \\
	&\leq \sum_{q=0}^\infty \frac{(2tC)^{2q}}{\prod_{j=1}^q (2j)} = \sum_{q=1}^\infty \frac{(2t^2C^2)^q}{q!} = e^{2t^2C^2}.
\end{align*}
Here, $\star$ follows from Fubini's theorem and the fact that the odd moments of $X - X'$ vanish. 
This shows that $X$ is sub-Gaussian with parameter $4C^2$. 
    \end{enumerate}
\end{proof}

Note that the proposition is not an ``if and only if''-type theorem: suppose we start with a sub-Gaussian variable $X$ with parameter $\sigma^2$. Then by (b), we have $\EE[X^{2q}] \leq q!(2\sigma)^{2q}$, and (c) then implies that $X$ is sub-Gaussian with parameter $16\sigma^2$. 

\begin{theorem}[Hoeffding's inequality] \label{thm:hoeffding}
	Let $X_1, \dotsc, X_n$ be independent sub-Gaussian random variables, with $X_i$ having parameter $\sigma_i^2$. Then $\bar X$ is sub-Gaussian with parameter $\overline{\sigma^2} = \frac1n \sum_{i=1}^n \sigma_i^2$. In particular, we have
	\[
	\max\qty{\PP(\bar X \geq x), \PP(\bar X \leq -x)} \leq e^{-nx^2/(2\overline{\sigma^2})}.
	\]
\end{theorem}

\begin{proof}
	For $t \in \RR$, we have
	\[
	\EE[e^{t\bar X}] = \EE[e^{(t/n)\sum_i X_i}] = \prod_{i=1}^n \EE[e^{(t/n)X_i}] \leq \prod_{i=1}^n e^{t^2\sigma_i^2/(2n^2)} = e^{t^2\overline{\sigma^2}/(2n)},
	\]
	which shows $\bar X$ is sub-Gaussian with parameter $\overline{\sigma^2}/n$. Applying part (a) of the previous proposition shows the second result. 
\end{proof}

\begin{remark}
	A direct consequence of Hoeffding's inequality is that
	\[
	\PP(\abs{\bar X} \geq x) \leq 2 e^{-nx^2/(2\overline{\sigma^2})}.
	\]
	The inequality is often stated in this weaker way. 
\end{remark}

\begin{lemma}[Hoeffding's lemma]
	Let $X$ be a random variable with $\EE X = 0$ that satisfies $a \leq X \leq b$. Then $X$ is sub-Gaussian with parameter $(b-a)^2/4$. 
\end{lemma}

\begin{proof}
	See Example Sheet 1, question 2. 
\end{proof}

\begin{corollary}
	Let $X_1, \dotsc, X_n$ be independent random variables where $\EE[X_i] = \mu_i$ and $a_i \leq X_i \leq b_i$. Then we have
	\[
	\PP(\bar X - \bar \mu \geq x) \leq \exp\qty(-\frac{2n^2x^2}{\sum_{i=1}^n (b_i - a_i)^2}). 
	\]
\end{corollary}

\begin{proof}
	By Hoeffding's lemma, $X_i - \mu_i$ is sub-Gaussian with parameter $(b_i - a_i)^2/4$ for each $i$. The result now follows from \cref{thm:hoeffding}.
\end{proof}

\begin{example}
	Note that when $X$ takes values in $[a, b]$, its variance is at most $(b-a)^2$. However, when $\Var(X_i) \ll (b_i - a_i)^2$, Hoeffding's inequality can be loose (for example, when $X_i \sim \Rm{Bern}(p_i)$ with $p_i$ small). In such circumstances, Bennett's or Bernstein's inequality may give better results (example sheet 1, question 4). 
\end{example}

\begin{theorem}[Bennett's inequality]
	Let $X_1, \dotsc, X_n$ be independent random variables with $\EE[X_i] = 0$, $\sigma_i^2 \ceq \Var(X_i) < \infty$, and $X_i \leq b$ for some $b > 0$. Define  $S \ceq \sum_{i=1}^n X_i$, $\nu \ceq \overline{\sigma^2}$ and $\phi \colon \RR \to \RR$ by $\phi(u) \ceq e^u - 1 - u = \sum_{k=2}^\infty \frac{u^k}{k!}$, then for every $t > 0$ we have
	\[
	\log\EE[e^{tS}] \leq \frac{n \nu }{b^2} \phi(bt). 
	\]
	
	Defining $h \colon (0, \infty) \to [0, \infty)$ by $h(u) \ceq (1+u)\log(1+u) - u$, we have for every $x > 0$ that
	\[
	\PP(\bar X \geq x) \leq \exp\qty(-\frac{n \nu}{b^2} h\qty(\frac{bx}{\nu})). 
	\]
\end{theorem}

\begin{proof}
	Define $g \colon \RR \to \RR$ by 
	\[
	g(u) \ceq \sum_{k=0}^\infty \frac{u^k}{(k+2)!} = \begin{cases}
		\frac{\phi(u)}{u^2}&\text{if $u \neq 0$}, \\
		\frac12 &\text{if $u = 0$}. 
	\end{cases} 
	\]
	Then one can check that $g$ is increasing on $\RR$, so 
	\[
	e^{tX_i} - 1 - t X_i = t^2 X_i^2 g(tX_i) \leq t^2 X_i^2 g(tb) = X_i^2 \frac{\phi(bt)}{b^2},
	\]
	and therefore
	\[
	e^{tX_i} \leq 1 + tX_i + X_i^2 \frac{\phi(bt)}{b^2} \implies \EE[e^{tX_i}] \leq 1 + \EE[X_i^2] \frac{\phi(bt)}{t^2} = 1 + \sigma_i^2 \frac{\phi(bt)}{b^2}. 
	\]
	
	Hence for $t > 0$ we have
	\begin{align*}
		\log \EE[e^{tS}] &= \sum_{i=1}^n \log\EE[e^{tX_i}] \leq n \cdot \frac1n\sum_{i=1}^n \log\qty(1 + \sigma_i^2 \frac{\phi(bt)}{b^2}) \\
		&\overset*\leq n \log(1 + \frac{\nu \phi(bt)}{b^2}) \overset{**}\leq  \frac{n\nu}{b^2}\phi(bt).
	\end{align*}
Here, $(*)$ follows from the fact that $\log$ is a concave function while $(**)$ follows from the fact that $\log(1+u) \leq u$ for all $u \geq 0$. This concludes the proof for the first part of the theorem.

Now, we apply the method of Chernoff bounding and find
\[
\PP(\bar X \geq x) = \PP(S \geq nx) \leq \inf_{t > 0} e^{-ntx} \EE[e^{t S}] \leq \inf_{t > 0} e^{-ntx + n\nu \phi(bt)/b^2} = \exp\qty(-\frac{n\nu}{b^2} h\qty(\frac{bx}{\nu})),
\]
since once can check that the infimum is attained at $t = b^{-1} \log(1 + bx/\nu)$. 
\end{proof}


\begin{definition}
	A random variable $X$ with $\EE X = 0$ is called \emph{sub-Gamma in the right tail} with variance factor $\sigma^2 > 0$ and scale $c > 0$ if
	\[
	\EE[e^{tX}] \leq \exp\qty(\frac{\sigma^2 t^2}{2(1 - ct)})
	\]
	for all $t \in [0, 1/c)$. 
\end{definition}

Note that this definition looks like that of sub-Gaussianity, except that $e^{tX}$ can explode as $t$ approaches $1/c$. We give some characteristics of sub-Gamma distributions:

%\begin{definition}
%	For any $x \in \RR$ we define $x_+ \ceq \max(x, 0)$. 
%\end{definition}

\begin{proposition}
\begin{enumerate}[(a)]
\item Let $X$ be sub-Gamma in the right tail with variance factor $\sigma^2$ and scale $c$. Then 
\[
\PP(X \geq x) \leq \exp\qty(-\frac{x^2}{2(\sigma^2 + cx)})
\]
for all $x \geq 0$.

\item Let $X$ be a random variable with $\EE X = 0$, $\EE[X^2] \leq \sigma^2$ and $\EE[(X_+)^q] \leq q! \sigma^2 c^{q-2}/2$ for all $q \geq 3$. Then $X$ is sub-Gamma in the right tail with variance factor $\sigma^2$ and scale parameter $c$. 
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(a)]
\item Again, we apply a Chernoff bound: we have
\begin{align*}
\PP(X \geq x) \leq \inf_{t \in [0, 1/c)} e^{-tx} \EE[e^{tX}] &\leq \inf_{t \in [0, 1/c)} \exp(-tx + \frac{\sigma^2t^2}{2(1 - ct)}) \\
&\leq \exp(- \frac{x^2}{2(\sigma^2 + cx)}),
\end{align*}
where we have set $t = x/(\sigma^2 + cx) \in [0, 1/c)$ in the final step. 

\item Recall from the proof of Bennett's inequality that $g$ is increasing and therefore for $u \leq 0$ we have $\phi(u) = u^2 g(u) \leq u^2 g(0) = \frac{u^2}{2}$. Therefore, for every $u \in \RR$ we have
\[
\phi(u) \leq \frac{u^2}{2} + \sum_{q=3}^\infty \frac{(u_+)^q}{q!}.
\]
We deduce that for $t \in [0, 1/c)$ we have (note $\log(x) \leq x - 1$ for all $x$):
\[
\log \EE[e^{tX}] \leq \EE(e^{tX}) - 1 = \EE[\phi(tX)] \leq \EE \qty[\frac{t^2X^2}{2} + \sum_{q=3}^\infty \frac{t^q X_+^q}{q!}].
\]
By Fubini's theorem, since the infinite sum has only positive terms we may interchange sum and expectation to obtain
\[
\EE \qty[\frac{t^2X^2}{2} + \sum_{q=3}^\infty \frac{t^q X_+^q}{q!}] =  \frac{t^2 \EE[X^2]}{2} + \sum_{q=3}^\infty \frac{t^q \EE[X_+^q]}{q!} \leq \frac{\sigma^2 t^2}{2} \sum_{q=2}^\infty t^{q-2} c^{q-2} = \frac{\sigma^2t^2}{2(1-ct)}. 
\]
\end{enumerate}
\end{proof}

Following this proposition, we can prove Bernstein's inequality:
\begin{theorem}[Bernstein's inequality]
	Let $X_1, \dotsc, X_n$ be independent random variables with $\EE[X] = 0$, $\frac1n\sum_{i=1}^n \Var(X_i) \leq \sigma^2$ and $\frac1n \sum_{i=1}^n \EE[(X_i)_+^q] \leq q! \sigma^2 c^{q-2}/2$ some $\sigma, c > 0$ and for all $q \geq 3$. Then $S \ceq \sum_{i=1}^n X_i$ is sub-Gamma in the right tail with variance factor $n\sigma^2$ and scale parameter $c$. In particular we have
	\[
	\PP(\bar X \geq x) \leq \exp\qty(- \frac{nx^2}{2(\sigma^2 + cx)}),
	\]
	for all $x \geq 0$. 
\end{theorem}

\begin{proof}
	We have by part (b) of the previous proposition
	\[
	\log\EE[e^{tS}] = \sum_{i=1}^n \log \EE[e^{tX_i}] \leq n \frac{\sigma^2 t^2}{2(1 - ct)},
	\]
	and the second claim follows from part (a) of the previous proposition:
	\[
	\PP(\bar X \geq x) = \PP(S \geq nx) \leq \exp\qty(- \frac{nx^2}{2(\sigma^2 + cx)}). 
	\]
\end{proof}