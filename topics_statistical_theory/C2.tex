\section{Kernel density estimation}
Let $X_1, \dotsc, X_n \iid f$, and suppose we wish to estimate the density function $f$. The oldest way to do this is with a histogram: we divide $\RR$ into equally sized intervals or \emph{bins}, and let $I_x$ denote the bin containing $x \in \RR$.
\begin{definition}
	The \emph{histogram density estimator} $\hat f_n^H$ with bin width $b > 0$ is given by
	\[
	\hat f_n^H(x) \ceq \frac{1}{nb} \sum_{i=1}^n \ind_{X_i \in I_x}.  
	\]
\end{definition}

There are a few major drawbacks to using histograms: it is difficult to choose $b$ and the positioning of bin edges, the theoretical performance is suboptimal (mostly due to their discontinuity) and graphical display in the multivariate case is difficult. 

\subsection{The univariate kernel density estimator}
\begin{definition}
	A Borel measurable function $K \colon \RR \to \RR$ is called a \emph{kernel} if it satisfies $\int_\RR K(x) \dd{x} = 1$. 
	
	A \emph{univariate kernel density estimator} of $f$ with kernel $K$ and \emph{bandwidth} $h > 0$ is defined as
	\[
	\hat f_n(x) \ceq \frac1{nh} \sum_{i=1}^n K\qty(\frac{x - X_i}{h}).
	\]
	Defining $K_h(x) \ceq \frac1h K\qty(\frac xh)$, we can rewrite this as
	\[
	\hat f_n(x) \ceq \frac1n \sum_{i=1}^n K_h(x - X_i). 
	\]
\end{definition}

Usually $K$ is chosen to be non-negative (which ensures that $K$ itself and $\hat f_n$ are themselves density functions), and $K$ is often chosen to be symmetric about 0. Generally, the choice of kernel $K$ is much less important than the choice of bandwidth $h$. 

If we consider $\hat f_n(x)$ as a point estimator of $f(x)$, we typically wish to minimise the \emph{mean squared error}
\[
\MSE(\hat f_n(x)) \ceq \EE\qty[(\hat f_n(x) - f(x))^2]. 
\]
Other possibilities include the mean absolute error which (unlike the MSE) is scale-invariant. However, the MSE has an appealing decomposition into variance and bias terms:
\[
\MSE(\hat f_n(x)) = \Var(\hat f_n(x)) + \Bias^2(\hat f_n(x)). 
\]

We can express the MSE in terms of convolutions: 
\begin{definition}
	Let $g_1, g_2 \colon \RR \to \RR$ be measurable. Then the \emph{convolution} of $g_1$ and $g_2$, denoted $g_1 * g_2$, is defined by
	\[
	(g_1 * g_2)(x) \ceq \int_\RR g_1(x - z) g(z) \dd{z}. 
	\]
\end{definition}
We can compute 
\begin{align}
\Bias \hat f_n(x) &= \EE[\hat f_n(x)] - f(x) = \EE[K_h(x - X_1)] - f(x) = \int_\RR  K_h(x - z) f(z) \dd{z} \nonumber \\
&= (K_h * f)(x) - f(x) \label{eq:bias_computation}. 
\end{align}

Analogously, letting $\xi_i \ceq K_h(x - X_i)$ (note that these are i.i.d.\ random variables), we have
\begin{equation} \label{eq:variance_computation}
	\Var \hat f_n(x) = \frac1n \Var(\xi_1) = \frac1n \qty(\EE[\xi_1^2] - \EE^2[\xi_1]) = \frac1n \qty[(K_h^2 * f)(x) - (K_h * f)^2(x)].
\end{equation}

To assess performance of $h$ and $K$, we want to assess the performance of $\hat f_n$ as an estimation of $f$ as a function. This gives the following definition:
\begin{definition}
	We define the \emph{mean integrated squared error} or MISE as\[
	\MISE(\hat f_n) \ceq \EE\qty(\int_\RR \qty(\hat f_n(x) - f(x))^2 \dd{x}) \overset\star= \int_\RR \MSE(\hat f_n(x)) \dd{x}, 
	\]
	where $\star$ follows from Fubini's theorem since the integrand is nonnegative. 
\end{definition}

We now aim to find bounds on the bias and the variance of $\hat f_n$ in order to choose $h$ and $K$ appropriately.

\subsection{Bounds on variance and bias}
\begin{definition}
	For a kernel $K$, define $R(K) \ceq \int_\RR K^2(u) \dd{u}$. 
\end{definition}

\begin{proposition} \label{prop:variance_bound}
	Let $\hat{f_n}$ be the kernel density estimator with kernel $K$ and bandwidth $h > 0$ constructed from $X_1, \dotsc, X_n \iid f$. Then for any $x \in \RR, h > 0, n \in \NN$ we have
	\[
	\Var \hat f_n(x) \leq \frac{1}{nh} \norm{f}_\infty R(K). 
	\]
\end{proposition}

\begin{proof}
	By \cref{eq:variance_computation} we have
	\begin{align}
	\Var \hat f_n(x) &\leq \frac1n (K_h^2 * f)(x) = \frac{1}{nh^2} \int_\RR K^2\qty(\frac{x - z}{h}) f(z) \dd{z} = \frac{1}{nh} \int_\RR K^2(u) f(x - uh) \dd{u}  \label{eq:var_bound}\\
	&\leq \frac1{nh} \norm{f}_\infty \int_\RR K^2(u) \dd{u} = \frac{1}{nh} \norm{f}_\infty R(K) .\nonumber 
	\end{align}
\end{proof}

Obtaining a bound on the bias is not at all straightforward: we wil need to introduce conditions on both the density $f$ and the kernel $K$. 
\begin{definition}
	Let $I \subseteq \RR$ be an interval, fix $\beta, L > 0$, and let $m \ceq \ceil{\beta} - 1$. A function $f \colon \RR \to \RR$ is said to belong to the \emph{HÃ¶lder class} $\HH(\beta, L)$ if $f$ is $m$ times differentiable on $I$ and
	\[
	\abs{f^{(m)}(x) - f^{(m)}(y)} \leq L \abs{x - y}^{\beta - m} \quad\text{for all $x, y \in I$}. 
	\]
	If $I$ is unspecified, we let $I = \RR$. 
	
	The densities in $\HH(\beta, L)$ are denoted by
	\[
	\FF(\beta, L) \ceq \qty{f \in \HH(\beta, L) \mid f \geq 0 \text{ and } \int_\RR f \dd{x} = 1}. 
	\]
\end{definition}

\begin{definition}
	Fix $\ell \in \NN$. We say a kernel $K$ is of \emph{order} $\ell$ if $\int_\RR x^j k(x) \dd{x} =0 $ for $j = 1, \dotsc, \ell - 1$. 
\end{definition}

\begin{remark}
	Most kernels used in practice are of order 2. Note that a kernel of order $\geq 3$ cannot be nonnegative, since we have $\int_\RR x^2K(x) \dd{x} = 0$. Therefore, the kernels are not themselves densities and the corresponding kernel density estimate is not guaranteed to be a density. 
\end{remark}

\begin{proposition} \label{prop:bias_bound}
	Assume that $f \in \FF(\beta, L)$ and that $K$ is a kernel of order $\ell \ceq \ceil{\beta}$, and furthermore assume that
	\[
	\mu_\beta(K) \ceq \int_\RR \abs{u}^\beta \abs{K(u)} \dd{u} < \infty. 
	\]
	Then the kernel density estimate with bandwidth $h$ and kernel $K$ based on $X_1, \dotsc, X_n \sim f$ satisfies
	\[
	\abs{\Bias \hat f_n(x)} \leq \frac{L}{(\ell - 1)!} \mu_\beta(K) h^\beta \quad\text{for all $x \in \RR, h > 0, n \in \NN$}. 
	\]
\end{proposition}

\begin{proof}
	By \cref{eq:bias_computation}, we have 
	\[
	\Bias \hat f_n(x) = \frac1h \int_\RR K\qty(\frac{x - z}{h}) f(z) \dd{z} - f(x) = \int_\RR K(u)\qty( f(x - uh) - f(x)) \dd{x}. 
	\]
	By applying Taylor's theorem with the Lagrange remainder we obtain, with $m = \ceil{\beta} - 1$, that
	\[
	f(x - uh) - f(x) = \sum_{j=1}^{m-1} \frac{(-uh)^j}{j!} f^{(j)}(x) + \frac{(-uh)^m}{m!} f^{(m)}(x - \tau u h) \quad\text{for some $\tau \in [0, 1]$}. 
	\]
	Since $\int_\RR u^j K(u) \dd{u} = 0$ for all $j \leq m$, plugging the sum into the integral will give 0. Therefore, we find
	\[
	\Bias \hat f_n(x) =  \frac{(-h)^m}{m!} \int_\RR u^m K(u) f^{(m)}(x - \tau uh) \dd{u} = \frac{(-h)^m}{m!} \int_\RR u^m K(u) \qty[f^{(m)}(x - \tau uh)
- f^{(m)}(x)] \dd{u},	\]
	where the last inequality follows again from the fact that $K$ is of order $m + 1$. 
	
	Now we use that $f \in \FF(\beta, L)$, and conclude
	\[
	\abs{\Bias \hat f_n(x)} \leq \frac{Lh^m}{m!} \int_\RR \abs{u}^m \abs{K(u)} \abs{\tau u h}^{\beta - m} \dd{u} \leq \frac{Lh^\beta}{m!} \int_\RR \abs{u}^\beta \abs{K(u)} \dd{u} = \frac{L}{(\ell - 1)!} \mu_\beta(K) h^\beta,
	\]	
	which concludes the proof. 
\end{proof}

Combining \cref{prop:variance_bound,prop:bias_bound}, we find that
\[
\MSE \hat f_n(x) \leq \frac{1}{nh} R(K) \norm{f}_\infty + \frac{L^2}{((\ell - 1)!)^2} \mu^2_\beta(K) h^{2\beta}. 
\]
By minimising this w.r.t.\ $h$, we find that the optimal $h$ is given by
\[
h_n^* = \qty(\frac{((\ell - 1)!)^2 \norm{f}_\infty R(K)}{2\beta L^2 \mu_\beta^2(K)})^{1/(2\beta + 1)} n^{-1/(2\beta + 1)}, 
\]
and the corresponding $\MSE$ is given by
\[
\MSE \hat f_n(x) \leq \qty(\frac{\norm{f}_\infty^{2\beta} R(K)^{2\beta} L^2 \mu_\beta^2(K) \qty[(2\beta)^{2\beta + 1} + 1]}{((\ell - 1)!)^2 (2\beta)^{2\beta}}) n^{-2\beta/(2\beta + 1)},
\]
This $O(n^{-2\beta/(2\beta + 1)})$ bound on the rate is slower than the $O(1/n)$ rate found in parametric problems, but such a rate is only obtained when the assumed model is correct. 

We can strengthen this as follows:
\begin{theorem}
	Assume that $K$ is a kernel of order $\ell \ceq \ceil{\beta}$ and that $\mu_\beta(K)$ and $R(K)$ are both finite. Fix $\alpha > 0$ and let $h = \alpha n^{-1/(2\beta + 1)}$. Then there exists $C > 0$, independent of $n$, such that
	\[
	\sup_{x \in \RR} \sup_{f \in \FF(\beta, L)} \MSE \hat f_n(x) \leq C n^{-2\beta/(2\beta + 1)}. 
	\]
\end{theorem}

\begin{proof}
	We will show that the class $\FF(\beta, L)$ is uniformly bounded in supremum norm. Let $K^*$ be a bounded kernel of order $\ell$ (see example sheet \TODO), then by the previous proposition with $h = 1$ we have by nonnegativity of $f$ that
	\begin{align*}
		f(x) &\leq \abs{f(x)- \int_{-\infty}^\infty K^*(x - z) f(z) \dd{z}} + \abs{\int_{-\infty}^\infty K^*(x-z) f(z) \dd{z}}.	\\
		&\leq \abs{\Bias \hat f_{n, K^*}(x)} + \norm{K^*}_\infty \int_{-\infty}^\infty  f(z) \dd{z} \\
		&\leq \frac{L}{(\ell - 1)!} \mu_\beta(K^*) + \norm{K^*}_\infty,
	\end{align*}
and this bound is independent of $f$ and $x$.

Now we have
\[
\MSE \hat f_n(x) \leq \frac{R(K) \norm{f}_\infty}{nh} + \frac{L^2}{((\ell - 1)!)^2} \mu^2_\beta(K) h^{2\beta} \leq C n^{-2\beta/(2\beta + 1)}. 
\]
\end{proof}


\subsection{Bounds on the integrated variance and bias}
To bound the MISE, we will give bounds on the integrated variance and bias. 
\begin{proposition}
	Let $\hat{f_n}$ denote the kernel density estimate with bandwidth $h$ and kernel $K$ based on $X_1, \dotsc, X_n \iid P$ (where $P$ is a distribution on $\RR$). Then
	\[
	\int_{-\infty}^\infty \Var \hat f_n(x) \dd{x} = \frac1{nh} R(K). 
	\]
\end{proposition} 

\begin{proof}
	We have by Fubini and \cref{eq:var_bound} that
	\begin{align*}
		\int_{-\infty}^\infty \Var\hat f_n(x) \dd{x} &\leq \frac1{nh^2} \int_{-\infty}^\infty \int_{-\infty}^\infty K^2\qty(\frac{x - z}{h}) f(z) \dd{z} \dd{x} = \frac1{nh^2} \int_{-\infty}^\infty f(z) \int_{-\infty}^\infty K^2\qty(\frac{x-z}{h}) \dd{x} \dd{z} \\
		&= \frac1{nh} R(K) \int_{-\infty}^\infty f(z) \dd{z} = \frac1{nh} R(K). 
	\end{align*}
\end{proof}

\begin{recap}
	Let $[a, b] = I \subseteq \RR$ be an interval, then $f \colon I \to \RR$ is called \emph{absolutely continuous} if, for every $\eps > 0$, there exists $\delta > 0$ such that, whenever $(x_1, y_1), \dotsc, (x_m, y_m)$ are disjoint subintervals of $I$ with $\sum_{i=1}^m (y_i - x_i) < \delta$, we have $\sum_{i=1}^m \abs{f(y_i) - f(x_i)} < \eps$. 
	
	It is known that absolute continuity is equivalent to being differentiable Lebesgue almost everywhere with a so-called \emph{weak derivative} $f'$ that satisfies $f(x) = f(a) + \int_a^x f'(t) \dd{t}$ for all $x \in [a, b]$. 
\end{recap}

\begin{recap}
	The \emph{generalised Minkowski inequality} states that any Borel measurable function $g \colon \RR^2 \to \RR$ we have that
	\[
	\int_\RR \qty(\int_\RR g(u, x) \dd{u})^2 \dd{x} \leq \qty(\int_\RR \qty(\int_\RR g^2(u, x) \dd{x})^{1/2} \dd{u})^2. 
	\] 
\end{recap}
To obtain bounds on the integrated squared bias, we will require smoothness conditions w.r.t.\ the $L^2(\RR)$ norm. 
\begin{definition}
	Fix $\beta, L > 0$ and let $m \ceq \ceil{\beta} - 1$. The \emph{Nikolski class} $\Nn(\beta, L)$ consists of functions $f \colon \RR \to \RR$ that are $(m-1)$ times differentiable and for  which $f^{(m-1)}$ is absolutely continuous with weak derivative $f^{(m)}$ satisfying
	\[
	\qty{\int_{-\infty}^\infty \qty[ f^{(m)}(x + t) - f^{(m)}(x)]^2 \dd{x}}^{1/2} \leq L \abs{t}^{\beta - m} \quad\text{for all $t \in \RR$}. 
	\]
	
	The densities in $\Nn(\beta, L)$ are denoted by $\FF_\Nn(\beta, L)$. 
\end{definition}

\begin{proposition}
	Fix $\beta, L> 0$ and let $K$ be a kernel of order $\ell \ceq \ceil\beta$. Let $\hat f_n$ denote the KDE with kernel $K$ and bandwidth $h$ based on $X_1, \dotsc, X_n \iid f \in \FF_\Nn(\beta, L)$. Then we have
	\[
	\int_{-\infty}^\infty \Bias^2 \hat f_n(x) \dd{x} \leq \frac{L^2}{((\ell - 1)!)^2} \mu_\beta^2(K) h^{2\beta}. 
	\]
\end{proposition}

\begin{proof}
	\TODO write this out (integration + taylor expansion + 2x minkowski). 
\end{proof}

Putting everything together, we obtain the following:
\begin{theorem} \label{thm:mise_bound}
	Fix $\beta, L > 0$, and let $K$ be a kernel of order $\ell = \ceil\beta$ with $R(k)$ and $\mu_\beta(K)$ finite. Let $\hat f_n$ be the KDE with kernel $K$ and bandwidth $h$ based on $X_1, \dotsc, X_n \iid f \in \FF_\Nn(\beta, L)$. Then we have
	\[
	\MISE\hat f_n \leq \frac{R(K)}{nh} + \frac{L^2}{((\ell - 1)!)^2} \mu_\beta^2(K) h^{2\beta}. 
	\]
	In particular, fixing $\alpha > 0$ and taking $h = \alpha n^{-1/(2\beta + 1)}$, there exists $C > 0$ independent of $n$ such that
	\[
	\sup_{f \in \FF_\Nn(\beta, L)} \MISE \hat f_{n, h, K} \leq C n^{-2\beta/(2\beta + 1)}. 
	\]
\end{theorem}

\subsection{Bandwidth selection}
The choice of bandwidth in the previous theorem is not practical since we have not specified $\alpha$ and $\beta$ is typically unknown. 

\subsubsection{Least squares cross validation}
One possible approach is \emph{least squares cross validation}. For this, note that minimising the $\MISE$ is equivalent to minimising
\[
\MISE(\hat f_n) - \int_\RR f^2(x) \dd{x} = \EE\qty[ \int_\RR \hat f_n^2(x) \dd{x}] - 2\EE\qty[ \int_\RR \hat f_n(x) f(x) \dd{x}],
\]
and it can be checked that an unbiased estimator for the above is given by 
\[
\Rm{LSCV}(h) \ceq \int_\RR \hat f_n^2(x) \dd{x} - \frac2n \sum_{i=1}^n \hat f_{n, -i}(X_i),
\]
where $\hat f_{n, -i}$ is the KDE based on all observations except $X_i$. We now choose $h$ such that LSCV is minimised. 

\subsubsection{Lepski}
\TODO write this subsection (\TODO understand this first)

\subsection{Choice of kernel}
To choose a kernel, we first fix the scale of the kernel by setting $\mu_2(K) = 1$. Now, by our bound on the MISE (\cref{thm:mise_bound}) it is reasonable to minimise $R(K)$, where for simplicity we assume that $K$ is a nonnegative second-order kernel. The solution is the Epanechnikov kernel
\[
K_E(x) \ceq \frac{3}{4\sqrt 5} \qty(1 - \frac{x^2}{5}) \ind_{\abs{x} \leq \sqrt 5}, 
\]
and the ratio $R(K_E)/R(K)$ is called the \emph{efficiency} of a kernel $K$. We find that for different kernels, the efficiency is greater than 0.9, which shows that kernel shape does not affect efficiency greatly. 


\subsection{Multivariate density estimation}
The general $d$-dimensional KDE is 
\[
\hat f_n(x) \ceq \frac{1}{n \sqrt{\det H}} \sum_{i=1}^n K(H^{-1/2}(x - X_i)),
\]
where $H$ is a symmetric positive-definite bandwidth bandwidth matrix (often chosen to be diagonal or a multiple of $I$). If $H = h^2 I$, it can be shown that, under an appropriate definition of a $\beta$ smoothness class, we have an optimal bandwidth of order $n^{-1/(d + 2\beta)}$, and with this choise, a MISE of order $n^{-2\beta/(d + 2\beta)}$. This is called the ``curse of dimensionality'': the higher the dimension becomes, the harder nonparametric estimation gets. 

