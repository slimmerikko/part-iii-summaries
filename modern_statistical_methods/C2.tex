\section{The Lasso}
\subsection{Model selection}
We go back to the linear model $Y = X\beta^0 + \eps$ with $\EE[\eps] = 0$ and $\Var(\eps) = \sigma^2 I$. Using the trace trick, one can easily compute that the MSPE of the OLS estimator is given by
\[
\frac1n \EE\norm{X\beta^0 - X\hat\beta^\Rm{OLS}}_2^2 = \frac{\sigma^2 p}{n}. 
\]
Defining $S = \qty{k \mid (\beta^0)_k \neq 0}$, there is often reason to assume that $S$ is small, i.e., $s \ceq \abs{S} \ll p$. If we could fit a model using only the variables in $S$, the MSPE would be much $\frac{\sigma^2 s}{n} \ll \frac{\sigma^2 p}{n}$. 

\paragraph{Best subset selection} A natural way to find $S$ is to consider all possible subsets of $\qty{1, \dotsc, p}$, and pick the best regression procedure using, for example, cross-validation. However, this can become computationally infeasible for moderately large $p$ (say $p \approx 10$). 

\paragraph{Forward selection} This is a greedy way of performing best subset regression. Given a target model size $m$, we first compute the intercept-only model $M_0$, and then one-by-one add the predictor variable that reduces the residual sum of squares the most, until we have a model with $m$ variables. 

\subsection{Lasso estimator}
The \emph{Least absolute shrinkage and selection operator} or \emph{Lasso} is given by
\[
(\hat\mu_\lambda^\Rm{L}, \hat\beta_\lambda^\Rm{L}) \ceq \argmin_{(\mu, \beta) \in \RR \times \RR^p} \frac1{2n}\norm{Y- \mu \vec 1 - X\beta}_2^2 + \lambda \norm{\beta}_1. 
\]
As with ridge regression, we usually centre and scale the matrix $X$, as well as centre the responses $Y$, in which case we find
\[
\hat\beta_\lambda^\Rm{L} = \argmin_{\beta \in \RR^p} \frac1{2n}\norm{Y - X\beta}_2^2 + \lambda \norm{\beta}_1. 
\]

The main difference between the lasso estimator and the ridge regression estimator is that it is likely that the lasso estimator has some zero components. This means that the lasso estimator also estimates which variables are relevant. 

We have the following:
\begin{theorem}[Slow rate]
	Assume $X$ has centred and scaled columns, and assume that $Y$ has been centred, so $Y = X\beta^0 + \eps - \bar\eps \vec 1$. Let $A > 0$ and suppose
	\[
	\lambda = A\sigma\sqrt{\frac{\log(p)}{n}}.
	\]
	Let $\hat\beta = \hat\beta_\lambda^\Rm{L}$, then with probability at least $1 - 2p^{-(A^2/2 - 1)}$ we have that the MSPE satisfies
	\[
	\frac1n \norm{X(\beta^0 - \hat\beta)}_2^2 \leq 4\lambda \norm{\beta^0}_1 = 4A\sigma\sqrt{\frac{\log(p)}{n}} \norm{\beta^0}_1. 
	\]
\end{theorem}

\begin{proof}
	By definition we have
	\[
	\frac1{2n}\norm{Y - X\hat\beta }_2^2 + \lambda \norm{\hat \beta}_1 \leq \frac1{2n}\norm{Y - X\beta^0}_2^2 + \lambda \norm{\beta^0}_1,
	\]
	and rearranging the terms gives
	\[
	\frac1{2n} \norm{X(\beta^0 - \hat\beta)}_2^2 \leq\frac1n \eps\T X (\hat\beta - \beta^0) + \lambda\norm{\beta^0}_1 - \lambda \norm{\hat\beta}_1. 
	\]
	By H\"older's inequality we have $\abs{\eps\T X(\beta^0 - \hat\beta)} \leq \norm{X\T\eps}_\infty \norm{\hat\beta - \beta^0}_1$. 
	Define the event \\ $\Omega = \qty{\norm{X\T\eps}_\infty/n \leq \lambda}$, then conditional on $\Omega$ we find
	\[
	\frac1{n} \norm{X(\beta^0 - \hat\beta)}_2^2 \leq 2\lambda \qty(\norm{\beta^0 - \hat\beta}_1 + \norm{\beta^0}_1 - \norm{\hat\beta}_1) \leq 4\lambda \norm{\beta^0}_1.
	\]
	In \cref{lem:probability_omega_bound}, we will show that $\PP(\Omega) \geq 1 - 2p^{-(A^2/2 - 1)}$, which completes the proof. 
\end{proof}

\subsection{Concentration inequalities}
Let $W$ be any random variable and $\phi \colon \RR\to[0,\infty)$ strictly increasing. Then by Markov's inequality we have
\[
\PP(W \geq t) = \PP(\phi(W) \geq \phi(t)) \leq \frac{\EE[\phi(W)]}{\phi(t)}. 
\]
Plugging in $\phi(x) = e^{\alpha x}$ (for some $\alpha > 0$), we get
\[
\PP(W \geq t) \leq e^{-\alpha t} \EE[e^{\alpha W}] = e^{-\alpha t} M_W(\alpha).
\]
Now we can take the infimum over all $\alpha$ on the right-hand side, and we get what is called the \emph{Chernoff bound}:
\[
\PP(W \geq t) \leq \inf_{\alpha > 0} e^{-\alpha t} M_W(\alpha). 
\]

\begin{definition}
	A random variable  $W$ with mean $\mu$ is called \emph{sub-Gaussian with parameter $\sigma > 0$} or $\sigma$-sub-Gaussian if 
	\[
	M_{W - \mu} \leq M_{N(0, \sigma^2)} \quad\text{or equivalently}\quad \EE[e^{\alpha(W - \mu)}] \leq e^{\alpha^2\sigma^2/2} \text{ for all $\alpha \in \RR$}. 
	\]
\end{definition}

We need the following lemma, which characterises an important class of sub-Gaussian random variables:
\begin{lemma}[Hoeffding]
	If $W$ is a mean-zero random variable which takes values in $[a, b]$, then $W$ is sub-Gaussian with parameter $(b-a)/2$. 
\end{lemma}

By Chernoff bounding, we obtain for a $\sigma$-sub-Gaussian random variable $W$ that \[\PP(W - \mu \geq t) \leq e^{-t^2/(2\sigma^2)}.\]

\begin{proposition}
	Let $W_1, \dotsc, W_n$ be independent, mean-zero random variables where $W_i$ is $\sigma_i$-sub-Gaussian. For any $\gamma \in \RR^n$, the random variable $\gamma\T W$ is sub-Gaussian with parameter $\qty(\sum_i \sigma_i^2\gamma_i^2)^{1/2}$.
\end{proposition}
\begin{proof}
	Since the $W_i$ are independent we have for all $\alpha \in \RR$ that
	\begin{align*}
		\EE[e^{\alpha \sum_i \gamma_i W_i}] = \prod_{i=1}^n \EE[e^{\alpha \gamma_i W_i}]  \leq \prod_{i=1}^n e^{\alpha^2\gamma_i^2\sigma_i^2/2} = e^{\alpha^2 \sum_i \gamma_i^2\sigma_i^2/2}.
	\end{align*}
\end{proof}

\begin{lemma} \label{lem:probability_omega_bound}
	Suppose $\eps_1, \dotsc, \eps_n$ are independent mean-zero $\sigma$-sub-Gaussian random variables and let $\lambda \ceq A\sigma \sqrt{\log(p)/n}$. Then
	\[
	\PP\qty(\frac{\norm{X\T\eps}_\infty}{n} \leq \lambda) \geq 1 - 2p^{-(A^2/2 - 1)}. 
	\]
\end{lemma}

\begin{proof}
	We have
	\[
	\PP\qty(\frac{\norm{X\T\eps}_\infty}{n} > \lambda) = \PP\qty(\bigcup_i \frac{\abs{X_i\T\eps}}{n} > \lambda) \leq \sum_i \PP\qty(\frac{\abs{X_i\T\eps}}{n} > \lambda). 
	\]
	By the previous proposition, both $X_i\T\eps$ and $-X_i\T\eps$ are sub-Gaussian with parameter $\sigma/\sqrt n$, so we have
	\[
	\sum_i \PP\qty(\frac{\abs{X_i\T\eps}}{n} > \lambda) \leq 2p \exp(\frac{-\lambda^2}{2(\sigma/\sqrt n)^2}) = 2p\exp(-A^2\log(p)/2) = 2 p^{-(A^2/2 - 1)}.
	\]
\end{proof}

We now move to finding tail bounds for products of sub-Gaussian random variables. To this end, we consider Bernstein's in equality. 
\begin{definition}
	A random variable $W$ with mean $\mu$ is said to satisfy \emph{Bernstein's condition} with parameters $\sigma > 0, b > 0$ if 
	\[
	\EE\qty(\abs{W - \mu}^k) \leq \frac12 k! \sigma^2 b^{k-2} \qquad k = 2, 3, \dotsc
	\]
\end{definition}

\begin{proposition}[Bernstein's inequality]
	Let $W_1, W_2, \dotsc$ be independent random variables with mean $\mu$, where each $W_i$ satisfies Bernstein's condition with parameter $(\sigma, b)$. Then 
	\begin{enumerate}
		\item $\EE[\exp(\alpha(W_i - \mu))] \leq \exp\qty(\frac{\alpha^2\sigma^2}{2(1 - b \abs{\alpha})})$ for all $\abs{\alpha} < b^{-1}$;
		\item $\PP\qty(\overline{W} - \mu \geq t) \leq \exp\qty(\frac{-nt^2}{2(\sigma^2 + bt)})$ for all $t \geq 0$.  
	\end{enumerate}
\end{proposition}

\begin{proof}
	See notes from topics in statistical theory.
\end{proof}

\begin{lemma}
	Let $W, Z$ be mean-zero, sub-Gaussian random variables with parameters $\sigma_W, \sigma_Z$. Then $WZ$ satisfies the Bernstein condition with parameters $(8\sigma_W\sigma_Z, 4\sigma_W\sigma_Z)$. 
\end{lemma}

\begin{proof}
	Using the tail probability formula for expectations one can prove that any mean-zero sub-Gaussian random variable $X$ satisfies $\EE[X^{2k}] \leq 2^{k+1} \sigma_X^{2k} k!$. 
	
	Furthermore for any random variable $Y$ we have
	\begin{align*}
		\EE\abs{Y - \EE Y}^k &\leq \EE\qty(\abs{Y} + \abs{\EE Y})^k \\
		&= \EE \sum_{t=0}^k \binom kt \abs{Y}^t \abs{\EE Y}^{k-t} \\
		&\overset*\leq \sum_{t=0}^k \binom kt \EE(\abs{Y}^t) \EE(\abs{Y}^{k-t}) \\
		&\overset*\leq \sum_{t=0}^k \binom kt \EE(\abs{Y}^k)^{t/k} \EE(\abs{Y}^k)^{k - t/k} \\
		&= \EE(\abs{Y}^k) \sum_{t=0}^k \binom kt = 2^k \EE\abs{Y}^k,
	\end{align*}
where both equations $*$ are applications of Jensen's inequality. 

From the above bound, we obtian 
\begin{align*}
\EE\abs{WZ - \EE WZ}^k &\leq 2^k \EE(\abs{W}^k \abs{Z}^k) \overset{\Rm{CS}}\leq 2^k (\EE W^{2k})^{1/2} (\EE Z^{2k})^{1/2} \leq 2^{2k+1} \sigma_W^k \sigma_Z^k k! \\
&= \frac{k!}{2} (8\sigma_W\sigma_Z)^2 (4\sigma_W\sigma_Z)^{k-2},
\end{align*}
which proves the claim. 
\end{proof}

\subsubsection{Optimisation theory and convex analysis}
\begin{proposition}
	Let $f \colon \RR^d \to \RR$ be twice continuously differentiable. Then $f$ is convex (resp.\ strictly convex) if and only if its Hessian is positive semi-definite (resp.\ positive definite) for all $x \in \RR^d$.
\end{proposition}