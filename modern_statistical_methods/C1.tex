Classical models rely on so-called ``large $n$ asymptotics'' (where $n$ is the sample size). This course focuses on the scenario where $p$, the number of variables, is larger or about as large as $n$. In this case, the classical theory breaks down, so we need new methods. 

\section{Kernel machines}
We represent data are pairs $(Y_i, x_i) \in \RR \times \RR^p$ ($i = 1, \dotsc, n$). The random variables $Y_i$ are called the \emph{responses}, and the (fixed) variables $x_i$ are called \emph{predictors}. 

\begin{recap}
    Let $X = (X_1, \dotsc, X_n)\T$ be a multivariate random variable. Its distribution function is given by
    \[
    F_{X} \colon \RR^n \to [0, 1] \colon \vec x \mapsto \PP(X_1 \leq x_1, \dotsc, X_n \leq x_n). 
    \]
    Its expected value is given by
    \[
    \EE[X] \ceq (\EE[X_1], \dotsc, \EE[X_n])\T \in \RR^n. 
    \]
    Its covariance matrix is given by 
    \[
    \Var[X] = \EE[(X - \EE[X])(X - \EE[X])\T] = \EE[XX\T] - \EE[X] \EE[X]\T. 
    \]
    The matrix $\Var[X]$ is symmetric positive semidefinite and satisfies $(\Var[X])_{ij} = \Cov(X_i, X_j)$. 
\end{recap}
\begin{definition}
    In a \emph{linear model}, we assume that 
    \[
    Y_i = x_i\T \beta^0 + \eps_i \quad (i = 1, \dotsc, n). 
    \]
    where $\beta \in \RR^p$ is unknown and the multivariate random variable $\eps = (\eps_1, \dotsc, \eps_n)$ satisfies $\EE (\eps) = 0$ and $\Var(\eps) = \sigma^2 I$. 
\end{definition}

\begin{definition}
    For an estimator $\tilde\beta$ of $\beta^0$, its \emph{mean squared error} (MSE) is given by 
    \[
    \EE_{\beta^0, \sigma^2} \qty[(\tilde\beta - \beta^0)(\tilde\beta - \beta^0)\T]  = \Var(\tilde\beta) + \qty[\EE(\tilde\beta - \beta^0)]\qty[\EE(\tilde\beta - \beta^0)]\T. 
    \]
\end{definition}
Note that if $\tilde\beta$ is unbiased, the second term will disappear and the MSE is simply the variance. 

\begin{recap}
    The maximum likelihood estimator (MLE) in this model is the ordinary least squares (OLS) estimator $\hat\beta^\Rm{OLS} = (X\T X)^{-1} X\T Y$, where the \emph{design matrix} $X \in \RR^{n \times p}$ is the matrix whose rows are the vectors $x_i$. This estimator only exists if $X$ has full column rank, so in particular, it is required that $p \leq n$. 
    
    The CramÃ©r-Rao lower bown states that, out of all unbiased estimators, the MLE has the optimal variance \emph{asymptotically} (i.e., for $n \to\infty$). 
\end{recap}

\subsection{Ridge regression}
\begin{definition}
    Let $\lambda \geq 0$, and let $\Bf 1 \in \RR^n$ be the all-ones vector. Then we define the \emph{ridge regression} estimators
    \[
    (\hat\mu_\lambda^\Rm{R}, \hat\beta_\lambda^\Rm{R}) \ceq \argmin_{(\mu, \beta) \in \RR \times \RR^p} \qty{ \norm{Y - \mu\Bf 1 - X\beta}^2 + \lambda \norm{\beta}^2}, 
    \]
    where the used norm is the 2-norm. The parameter $\lambda$ is called the \emph{regularisation parameter}. 
\end{definition}

The parameter $\lambda$ represents a penalty for large coefficients in the design matrix. The intercept is not penalised --- this is because a shift in units should not affect the fitted values. However, $X\hat\beta$ is not invariant under scale transformations, so it is common practice to centre the columns of $X$ to have mean 0, and then scale them to have $\ell_2$-norm $\sqrt n$. 

After that, we can compute $\hat\mu_\lambda^\Rm{R}$ by taking the derivative:  
\begin{align*}
    \norm{Y - \mu\Bf{1} - X\beta}^2 &=  \sum_i (Y_i - \mu - \sum_j X_{ij}\beta_j)^2. \\
    \pdv{\mu} \norm{Y- \mu\Bf{1} - X\beta}^2 &= -2 \sum_i \qty( Y_i - \mu - \sum_j X_{ij}\beta_j).
\end{align*}
Setting this derivative equal to 0 yields
\begin{align*}
    -2\sum_i \qty(Y_i - \mu - \sum_j X_{ij}\beta_j) &= 0 \\
    \sum_i Y_i - n\mu - \sum_j \beta_j (\sum_i X_{ij}) &= 0 \\
    \sum_i Y_i - n\mu &= 0 \\
    \mu &= \frac1n \sum_i Y_i = \bar Y. 
\end{align*}

Therefore we conclude $\hat\mu_\lambda^{\Rm R} = \bar Y$. 
After centering the responses (i.e.\, replacing $Y_i$ by $Y_i - \bar Y$), the problem can be reduced to 
\[
\hat\beta_\lambda^{\Rm R} = \argmin_{\beta \in \RR^p} Q(\beta) \ceq \argmin_{\beta \in \RR^p}  \qty[ \norm{Y- X\beta}^2 + \lambda \norm{\beta}^2]. 
\]

Since $Q(\beta)$ is convex quadratic, there is a unique root, and to find it we compute
\[
\nabla_\beta Q(\beta) = 2 X\T (Y - X \beta) + 2\lambda\beta = 0 \iff \beta = (X\T X + \lambda I)^{-1} X\T Y. 
\]

We conclude that $\hat\beta^\Rm{R}_\lambda = (X\T X + \lambda I)^{-1} X\T Y$. Note that, even if $X$ does not have full column rank, this estimator exists for all $\lambda > 0$.  In fact, for $\lambda$ sufficiently small, the ridge estimator outperforms the MLE in terms of mean squared error:
\begin{theorem}
    Fix $\beta^0, \sigma^2$, and assume that $\hat\beta^\Rm{OLS}$ exists (i.e., $X$ has full column rank). For some $\lambda > 0$ sufficiently small, it holds that the MSE of $\hat\beta^\Rm{OLS}$ minus the MSE of $\hat\beta^\Rm{R}_\lambda$ is positive definite. 
\end{theorem}

\begin{proof}
    This is simply writing out the MSE's. In the end, we find that the result holds for $0 < \lambda < 2\sigma^2 / \norm{\beta^0}^2$. 
\end{proof}

\subsubsection{The SVD and PCA}
\begin{recap}
    Recall that any $X \in \RR^{n \times p}$ can be factorised as $X = UDV\T$, where $U, V$ are $n \times n$ and $p \times p$ orthogonal matrices respectively, and $D \in \RR^{n \times p}$ satisfies $D_{11} \geq\dotsb\geq D_{mm} \geq 0$ where $m \ceq \min(n, p)$, and all other entries of $D$ are 0. This is called the \emph{singular value decomposition} or SVD of $X$. 
    
    If $n > p$, we can replace $U$ by its first $p$ columns and $D$ by its first $p$ rows to produce the so-called \emph{thin SVD} of $X$. Then $U \in \RR^{n \times p}$ has orthogo nal columns (so $U\T U = I$) and $D \in \RR^{p \times p}$ is square and diagonal. 
\end{recap}

Suppose $n \geq p$ and let $X = UDV\T$ be the thin SVD of our design matrix $X$. Then we can write the fitted values from the ridge regression as follows: 
\begin{align*}
    X\hat\beta_\lambda^\Rm{R} &= X(X\T X + \lambda I)^{-1} X\T Y \\
    &= UDV\T (VD^2V\T + \lambda I)^{-1} VDU\T Y \\
    &= UDV\T \qty(V(D^2 + \lambda I)V\T)^{-1} VDU\T Y \\
    &= UD (D^2 + \lambda I)^{-1} DU\T Y \\
    &= U D^2 (D^2 + \lambda I)^{-1} U\T Y \\
    &= \sum_{j=1}^p \frac{D_{jj}^2}{D_{jj}^2 + \lambda} U_j U_j\T Y. 
\end{align*}

Note that for OLS ($\lambda = 0$), this is simply the projection of $Y$ onto the column space of $X$ (if $X$ has full column rank). If $\lambda > 0$, $Y$ is still projected onto the column space of $X$, but the projection is shrunk in the directions of the left singular vectors, and the lower the corresponding singular value, the higher the shrinkage. 


\paragraph{Principal component analysis} Consider $v \in \RR^p$ with norm 1, then since the columns of $X$ have been centered, the sample mean of $Xv$ is 0, and the sample variance is therefore
\[
\frac1n \sum_i (Xv)_i^2 = \frac1n (Xv)\T Xv = \frac1n v\T X\T X v = \frac1n v\T VD^2V\T v. 
\]
Writing $a = V\T v$ (with $\norm{a} = 1$), we find
\[
\frac1n v\T VD^2 V\T v = \frac1n a\T D^2 a = \frac1n \sum_j a_j^2 D_{jj}^2
\]

Therefore, we see that the above is maximised if $a = \pm e_1$, or equivalently $v = \pm V_1$. Therefore, $V_1$ determines which combination of columns of $X$ has the largest variance (subject to having norm 1), and $XV_1 = D_{11} U_1$ is known as the \emph{first principal component} of $X$. Analogously, it can be shown that $D_{22} U_2, \dotsc, D_{pp} U_p$ have maximum variance $D_{jj}^2 / n$, subject to being orthonormal to all earlier principal components. 

We see that ridge regression shrinks $Y$ most in the smaller principal components of $X$. Therefore it will work well if most of the information is in the larger principal components of $X$. 

\paragraph{A comment on computation}
By analogous calculations as before, one cam compute $\hat\beta_\lambda^\Rm{R} = V(D^2 + \lambda I)^{-1} DU\T Y$. Since calculating the inverse of a diagonal matrix is trivial, we see that the complexity of computing $\hat\beta_\lambda^\Rm{R}$ for any $\lambda$ lies in $O(np)$. Of course, this is after computation of the SVD of $X$, which lies in $O(np \min(n, p))$. 

\subsection{$v$-fold cross validation}
Of course, we are still left with the problem of choosing $\lambda$ in ridge regression. We consider one possible way of doing so, namely $v$-fold cross validation, which is a general way of selection a good regression method from several competing methods. Here, we assume that our predictors are random, so that we have i.i.d.\ data pairs $(x_i, Y_i)$ ($i = 1, \dotsc, n$). Suppose $(x^*, Y^*)$ is a new data pair, independent of $(X, Y)$ and identically distributed. Ideally, we want to pick $\lambda$ which minimises the prediction error (averaged over $Y^*$ and $x^*$)
\[
\EE\qty[ \qty(Y^* - (x^*)\T \hat\beta_\lambda^\Rm{R}(X, Y))^2 \mid X, Y],
\]
where the dependence of $\hat\beta_\lambda^\Rm{R}$ on the training data $(X, Y)$ is made explicit by denoting it $\hat\beta_\lambda^\Rm{R}(X, Y)$. 

This is impossible to minimise, but it may be possible to minimise the expected prediction error (averaged over the training data)
\begin{equation} \label{eq:averaged_prediction}
\EE\qty{ \EE\qty[ \qty(Y^* - (x^*)\T \hat\beta_\lambda^\Rm{R}(X, Y))^2 \mid X, Y]}. 
\end{equation}

This is still not possible to compute directly, but we estimate it using $v$-fold cross validation. Split the data into $v$ groups or \emph{folds} of roughly equal size $(X^{(1)}, Y^{(1)}), \dotsc, (X^{(v)}, Y^{(v)})$ and let $(X^{(-k)}, Y^{(-k)})$ denote all data except that in the $k$-th fold. Then we define 
\[
\CV(\lambda) \ceq \frac1n \sum_{i=1}^n \qty[Y_i - x_i\T\hat\beta_\lambda^\Rm{R}(X^{(-\kappa(i))}, Y^{(-\kappa(i))})]^2, 
\]
and choose the value of $\lambda$ that minimises $\CV(\lambda)$. 

The function $\CV(\lambda)$ is called the \emph{out-of-sample error}, since the training data does not include $x_i$. 

\begin{recap}
    The \emph{tower rule} states that for random variables $X, Y$ we have $\EE[X] = \EE[\EE[X \mid Y]]$. 
\end{recap}

Note that for each $i$, we have
\[
\EE\qty[\qty{Y_i - x_i\T\hat\beta_\lambda^\Rm{R}(X^{(-\kappa(i))}, Y^{(-\kappa(i))})}^2] = \EE\qty[ \EE\qty[\qty{Y_i - x_i\T\hat\beta_\lambda^\Rm{R}(X^{(-\kappa(i))}, Y^{(-\kappa(i))})}^2 \mid X^{-\kappa(i)}, Y^{-\kappa(i)} ]]. 
\]
This equals the expected prediction error in \cref{eq:averaged_prediction}, except that the training data $X, Y$ are replaced with a smaller data set. 

We now have a bias-variance tradeoff in the size of the folds: if $v = n$ (known as ``leave-one-out'' cross-validation), the estimation will be almost unbiased, but the averaged quantities in $\CV(\lambda)$ will be highly correlated which leads to high variance. Typical choices of $v$ are 5 or 10. 

Instead of finding the single best $\lambda$, we can also aim to find the best weighted combination of $\lambda$'s. For example, suppose $\lambda$ is restricted to a grid $\lambda_1 > \dotsb > \lambda_L$. Then we can use any nonnegative least-squares optimization algorithm to minimise 
\[
\frac1n \sum_{i=1}^n \qty[ Y_i - \sum_{\ell = 1}^L w_\ell x_i\T \hat\beta_{\lambda_\ell}^\Rm{R}(X^{(-\kappa(i))}, Y^{(-\kappa(i))})]^2,
\]
over all $w \in \RR^L_{\geq 0}$. This procedure is known as \emph{stacking} and often outperforms cross-validation. 

\subsection{The kernel trick}
We note that 
\[
 X\T (XX\T + \lambda I) = (X\T X + \lambda I) X\T,
\]
and multiplying from the left with $(X\T X + \lambda I)^{-1}$ and from the right with $(XX\T + \lambda I)^{-1}$ gives
\[
(X\T X + \lambda I)^{-1} X\T = X\T (XX\T + \lambda I)^{-1}.
\]
Using this, we see that we can rewrite the fitted values from ridge regression as follows:
\[
X\hat\beta_\lambda^R = X(X\T X + \lambda I)^{-1} X\T Y = XX\T (XX\T + \lambda I)^{-1} Y.
\]
Two important remarks: 
\begin{enumerate}
    \item Computing the LHS of this equation takes roughly $O(np^2 + p^3)$ operations, while computing the RHS takes $O(n^2 p + n^3)$ operations (this is because in the LHS we invert an $p \times p$ matrix, while in the RHS we invert a $n \times n$ matrix). Therefore, if $p \gg n$, the RHS can be much cheaper to compute. 
    \item The LHS depends only on the matrix $K = XX\T$ (this matrix is called the \emph{kernel matrix}). Intuitively, since $K_{ij} = \ang{x_i, x_j}$, the entries of the kernel matrix show how `similar' the corresponding predictors are. 
\end{enumerate}

\begin{example}
    Suppose we have data $(Y_i, z_i)_{i = 1, \dotsc, n}$ with $z_i = (z_{i1}, \dotsc, z_{id})\T$, and we believe the following quadratic relation holds:
    \[
    Y_i = \sum_k \sqrt{2}\gamma_k z_{ik}  + \sum_{k, \ell} \theta_{k\ell} z_{ik} z_{i\ell} + \eps_i.
    \]
    To compute fitted values using ridge regression, we can rewrite this as a linear model $Y = X\beta + \eps$ where
    \[
    \beta = \mqty(\gamma_1 \\ \vdots \\ \gamma_d \\ \theta_{11} \\ \theta_{12} \\ \vdots \\ \theta_{dd}), \quad x_i = \mqty(\sqrt{2} z_{i1} \\ \vdots \\ \sqrt 2 z_{id} \\ z_{i1} z_{i1} \\ z_{i1}z_{i2} \\ \vdots \\ z_{id}z_{id}).
    \]
    
    In this case, we have $p = d^2 + d$ variables, which means computing $(X\T X + \lambda I)^{-1}$ takes $O(d^6)$ operations. In this case, computing $(XX\T + \lambda I)^{-1}$ is probably easier. 
    
    We are still left with the problem of computing $K \ceq XX\T$, which can take $O(n^2 p) = O(n^2 d^2)$ operations if done naively. However, observe that
    \[
    K_{ij} = x_i\T x_j = 2 \sum_k z_{ik} z_{jk} + \sum_{k, \ell} z_{ik} z_{i\ell} z_{jk} z_{j\ell} = \qty( 1 + \sum_k z_{ik} z_{jk})^2 - 1 = (1 + z_i\T z_j) - 1.
    \]
    This quantity can be computed in $O(d)$, and therefore $K$ can be computed in $O(n^2 d)$ operations: we have a factor $d$ improvement. 
\end{example}

The general point of the previous example is that we can bypass the features $x_i$ entirely and instead think directly of $K = XX\T$ where an entry $K_{ij}$ represents similarity between the inputs of the $i$-th and $j$-th samples. This leads to the notion of a kernel in general.

\subsection{Kernels}
We will assume our inputs $x_1, \dotsc, x_n$ live in an abstract space $\XX$. 
\begin{definition}
    A (\emph{positive-definite}) \emph{kernel} is a symmetric map $k \colon \XX^2 \to \RR$ such that for all $n \in \NN$ and all $x_1, \dotsc, x_n \in \XX$, the matrix $K \in \RR^{n \times n}$ with $K_{ij} = k(x_i, x_j)$ is positive semi-definite. 
\end{definition}

\begin{proposition}[Cauchy-Schwarz for kernels] \label{prop:kernel_cs}
    Let $k$ be a kernel and $x, x' \in \XX$, then 
    \[
    k(x, x')^2 \leq k(x, x) k(x', x'). 
    \]
\end{proposition}

\begin{proof}
    The matrix $\smqty(k(x, x) & k(x, x') \\ k(x', x) & k(x', x'))$ must be positive semi-definite so its determinant must be non\-negative. 
\end{proof}

In our old models, the data points $x_i$ were vectors in $\RR^p$. Now we try to think of them as points in an abstract space with an associated \emph{feature map} $\phi \colon \XX \to \HH$ (with $\HH$ an inner product space), and a kernel $k(x, x')$ gives a measure of similarity between $\phi(x)$ and $\phi(x')$. In this case, we have the following: 
\begin{proposition} \label{prop:ip_kernel}
    Let $\HH$ be an inner product space,  $\phi \colon \XX \to \HH$ and define $k(x, x') \ceq \ang{\phi(x), \phi(x')}$. Then $k$ is a kernel. 
\end{proposition}

\begin{proof}
    We have, for all $x_1, \dotsc, x_n \in \XX$ and $\alpha \in \RR^n$ that
    \[
    \alpha\T K \alpha = \sum_{i, j} K_{ij} \alpha_i \alpha_j = \sum_{i, j} \ang{\phi(x_i), \phi(x_j)} \alpha_i \alpha_j = \norm{\sum_i \alpha_i \phi(x_i)}^2 \geq 0. 
    \]
\end{proof}

\subsubsection{Examples of kernels}
The following proposition shows how to make new kernels from old:
\begin{proposition}
    Suppose $k_1, k_2, \dotsc$ are kernels. Then:
    \begin{enumerate}
        \item If $\alpha_1, \alpha_2 \geq 0$ then $\alpha_1 k_1 + \alpha_2 k_2$ is a kernel.
        \item The pointwise limit of a sequence of kernels is a kernel (if it exists). 
        \item The pointwise product $k_1 k_2$ is a kernel. 
    \end{enumerate}
\end{proposition}

\begin{proof}
    See Example Sheet 1. 
\end{proof}

\begin{example} Let us consider some examples of kernels:
    \begin{enumerate}
        \item For $\XX = \RR^p$ we have already seen the \emph{linear kernel} $k(x, x') = x\T x'$. 
        \item For $\XX = \RR^p$, the \emph{polynomial kernel} is defined as $k(x, x') = (1 + x\T x')^d$. This is a kernel since it is a power of a sum of two kernels. 
        \item For $\XX = \RR^p$, the \emph{Gaussian kernel} is defined by
        \[
        k(x, x') = \exp\qty( - \frac{\norm{x - x'}_2^2}{2\sigma^2}).
        \]
        To show this is a kernel, write $k$ as the pointwise product $k_1k_2$ where
        \[
        k_1(x, x') = \exp\qty(-\frac{\norm{x}^2}{2\sigma^2}) \exp\qty(-\frac{\norm{x'}^2}{2\sigma^2}), \quad k_2(x, x') = \exp\qty(\frac{x\T x'}{\sigma^2}). 
        \]
        Clearly $k_1$ is the kernel induced by the feature map $\phi(x) = \exp(-\norm{x}^2/(2\sigma^2))$, while $k_2$ can be seen to be a kernel by using the Taylor expansion, which shows that $k_2$ is a limit of nonnegative linear combinations of kernels. 
        
        \item For $\XX = [0, 1]$, define the \emph{Sobolev kernel} $k(x, x') = \min(x, x')$. The proof that this is a kernel is on example sheet 1. 
        \item For $\XX = \pow(\qty{1, \dotsc, p})$, define the \emph{Jaccard kernel}
        \[
        k(x, x') = \frac{\abs{x \cap x'}}{\abs{x \cup x'}} \quad\text{where $0/0 \ceq 1$}. 
        \]
        The proof that this is a kernel is on example sheet 1. 
    \end{enumerate}
\end{example}

\subsubsection{Reproducing kernel Hilbert spaces}
By \cref{prop:ip_kernel}, we see that every feature map $\phi \colon \XX \to \HH$ gives rise to a kernel. In the next (important!) theorem, we will see that every kernel is in fact induced by a feature map.

\begin{theorem} \label{thm:kernel_ip} 
    Let $k$ be a kernel, then there exists an inner product space $\HH$ and a feature map $\phi \colon \XX \to \HH$ such that 
    \[
    k(x, x') = \ang{\phi(x), \phi(x')} \quad\text{for all $x, x' \in \XX$}. 
    \]
\end{theorem}

\begin{proof}
    We will construct $\HH$ and $\phi$ explicitly. First we define the function space
    \[
    \HH = \qty{ \sum_{i=1}^n \alpha_i k(\cdot, x_i) \mid n \in \NN,  \alpha_i \in \RR, x_i \in \XX}. 
    \]
    Let $f = \sum_{i=1}^n \alpha_i k(\cdot, x_i)$ and $g = \sum_{j=1}^n \beta_j k(\cdot, x_j')$, then the inner product on $\HH$ is given by
    \[
    \ang{f, g} = \ang*{\sum_{i=1}^n \alpha_i k(\cdot, x_i), \sum_{j=1}^m \beta_j k(\cdot, x_j')} \ceq \sum_{i=1}^n \sum_{j=1}^m \alpha_i \beta_j k(x_i, x_j'). 
    \]
    We define $\phi \colon \XX \to \HH$ as $\phi(x) = k(\cdot, x)$. 
    
    We must check that the inner product does not depend on the choice of representation of $f$ and $g$. For this, note that 
    \[
    \sum_{i=1}^n \sum_{j=1}^m \alpha_i\beta_j k(x_i, x_j') = \sum_{i=1}^n \alpha_i g(x_i) = \sum_{j=1}^m \beta_j f(x_j'),
    \]
    which holds by symmetry of the kernel. Since $\sum_i \alpha_i g(x_i)$ is independent of the representation of $g$, while $\sum_j \beta_j f(x_j')$ is independent of the representation of $f$, we conclude that the entire expression is independent of both representations. 
    
    Secondly, we must verify that the formula $k(x, x') = \ang{\phi(x), \phi(x')}$ indeed holds. For any $f \in \HH, x \in \XX$ we have
    \begin{equation}
    \ang{k(\cdot, x), f} = \sum_{i=1}^n \alpha_i k(x_i, x) = f(x),
    \end{equation}
    i.e., evaluation of a function is a linear functional in $\HH$. 
    
    In particular, we have 
    \[
    \ang{\phi(x), \phi(x')} = \ang{k(\cdot, x), k(\cdot, x')} =  k(x, x'). 
    \]
    
    Finally, we must check that $\ang{\cdot, \cdot}$ is indeed an inner product. Symmetry and bilinearity are clear. Furthermore, we have
    \[
   \ang{f, f} = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(x_i, x_j) = \alpha\T K\alpha \geq 0
    \]
    by the fact that $k$ is a kernel. We must now only show that $f \neq 0 \implies \ang{f, f} > 0$. For this, note that $\ang{\cdot, \cdot}$ is a kernel on $\HH$, so by \cref{prop:kernel_cs} (Cauchy-Schwarz) we have
    \[
    f(x)^2 = \ang{k(\cdot, x), f}^2 \leq \ang{k(\cdot, x), k(\cdot, x)} \ang{f, f}, 
    \]
    and therefore if $f$ is nonzero anywhere, $\ang{f, f}$ must also be nonzero. 
\end{proof}

While $\HH$ constructed in the proof is an inner product space, it is not necessarily a Hilbert space. Let $(f_n) \subseteq \HH$ be Cauchy, then by Cauchy-Schwarz for kernels we find
\[
f_m(x) - f_n(x) = (f_m - f_n)(x) = \ang{k(\cdot, x), f_m - f_n} \leq \sqrt{k(x, x)} \norm{f_n - f_m}. 
\]
We can do an analogous computation for $f_n - f_m$ to conclude that $\abs{f_m(x) - f_n(x)} \leq \sqrt{k(x, x)} \norm{f_n - f_m}$, and therefore, if $(f_n)$ is Cauchy, then it converges pointwise to some $f^* \colon \XX \to \RR$. We will not prove the following theorem:
\begin{theorem} \label{thm:kernel_ip_hilbert}
    The inner product space $\HH$ constructed in the proof of \cref{thm:kernel_ip} can be extended to a Hilbert space by adding all pointwise limits $f^*$ of Cauchy sequences $(f_n) \subseteq \HH$. 
\end{theorem}

The completion of $\HH$ is a special type of Hilbert space:
\begin{definition}
    A Hilbert space $\BB$ of functions $f \colon \XX \to \RR$ is called a \emph{reproducing kernel Hilbert space} (RKHS) if for all $x \in \XX$, there exists $k_x \in \BB$ such that 
    \[
    f(x) = \ang{k_x, f}, 
    \]
    i.e., evaluation of functions is a linear functional. 
    
    The function $k(x, x') = \ang{k_x, k_{x'}}$ is known as the \emph{reproducing kernel} of $\BB$ (induced by the feature map $\phi(x) = k_x$). 
\end{definition}

If we start with a kernel $k$, construct the corresponding RKHS $\BB$, then it is easily checked that $k$ is indeed the reproducing kernel of $\BB$. 

\begin{example}[Linear kernel]
    Let $X = \RR^p$ and $k(x, x') = x\T x'$. Then we have 
    \[
    \HH = \qty{x \mapsto \sum_{i=1}^n \alpha_i x\T x_i = x\T \qty(\sum_i \alpha_i x_i) \mid \alpha_i \in \RR, x_i \in \RR^p} = \qty{x \mapsto x\T \beta \mid \beta \in \RR^p},
    \]
    and if $f(x) = x\T\beta$, $g(x) = x\T \beta'$, then 
    \[
    \ang{f, g} = k(\beta, \beta') = \beta\T \beta' \quad\text{so } \norm{f}_\HH = \norm{\beta}_2. 
    \]
\end{example}

\subsubsection{The representer theorem}
We started with data $(Y_i, x_i)$ in a model where our fitted values depend only on $XX\T = K$, where $K$ is the kernel matrix derived from the kernel $k(x_i, x_j) = \ang{x_i, x_j} = x_i\T x_j$. Suppose we change the kernel to some $\tilde k$, and replace $K$ by $\tilde K$ accordingly. Then by \cref{thm:kernel_ip} this is equivalent by replacing our predictors $x_i$ by $\phi(x_i)$ for some feature map $\phi$. 

Recall that the ridge regression objective function is given by
\[
\min_{\beta \in \RR^p} \sum_{i=1}^n (Y_i - x_i\T \beta)^2 + \lambda\norm{\beta}^2. 
\]
Let $\HH$ be the RKHS of the linear kernel, then the above is equivalent to
\begin{equation} \label{eq:objective_ridge_rkhs}
\min_{f \in \HH} \sum_{i=1}^n (Y_i - f(x_i))^2 + \lambda \norm{f}^2. 
\end{equation}

We will show that in general, if we change our predictors $x_i$ to $\phi(x_i)$, and update the corresponding RKHS in \cref{eq:objective_ridge_rkhs}, then \cref{eq:objective_ridge_rkhs} solves our new problem. Note that there are no references to the feature map $\phi$ in \cref{eq:objective_ridge_rkhs}. 

\begin{theorem}[Representer theorem]
Let:
\begin{itemize}
	\item $(Y_i, x_i)_{i=1}^n$ be our data with $x_i \in \XX$;
	\item $c \colon \RR^n \times \XX^n \times \RR^n \to \RR$ be an arbitrary loss function;
	\item $J \colon \RR_{\geq 0} \to \RR$ strictly increasing;
	\item $k$ a kernel with RKHS $\HH$ and kernel matrix $K_{ij} = k(x_i, x_j)$. 
\end{itemize}
Then $\hat f \in \HH$ minimises
\begin{equation*} % \label{eq:mini_q1}
Q_1(f) \ceq c(Y, x_1, \dotsc, x_n, f(x_1), \dotsc, f(x_n))
\end{equation*}
if and only if $\hat f(\cdot) = \sum_{i=1}^n \hat\alpha_i k(\cdot, x_i)$, where $\hat\alpha \in \RR^n$ minimises
\begin{equation*} % \label{eq:mini_q2}
Q_2(\alpha) \ceq c(Y, x_1, \dotsc, x_n, K\alpha) + J(\alpha\T K\alpha). 
\end{equation*}
\end{theorem}

\begin{proof}
	Let $f \in \HH$ and $U = \Span\qty{k(\cdot, x_1), \dotsc, k(\cdot, x_n)} \subseteq \HH$, then $U$ is closed since it is finite-dimensional so we can write $f = u + v$ with $u \in U, v \in U\P$. Note that
	\[
	f(x_i) = \ang{k(\cdot, x_i), u+v} = \ang{k(\cdot, x_i), u} = u(x_i) \quad(i = 1, \dotsc, n),
	\]
	so minimisation of $Q_1$ is equivalent to minimisation of
	\begin{equation} \label{eq:mini_proof_representer}
	c(Y, x_1, \dotsc, x_n, u(x_1), \dotsc, u(x_n)) + J(\norm{u}^2 + \norm{v}^2)
	\end{equation}
	w.r.t.\ $u \in U$, $v \in U\P$. Now, since $J$ is strictly increasing, any minimiser $(u, v)$ of  \cref{eq:mini_proof_representer} will satisfy $v = 0$.
	
	Write $u(\cdot) = \sum_{i=1}^n \alpha_i k(\cdot, x_i) \in U$, then clearly
	\[
	\mqty[ u(x_1) \\ \vdots \\ u(x_n)] = \mqty[ \sum_i \alpha_i k(x_1, x_i) \\ \vdots \\ \sum_i \alpha_i k(x_n, x_i)] = K\alpha \quad\text{and}\quad\norm{u}^2 = \sum_{i, j} \alpha_i \alpha_j k(x_i, x_j) = \alpha\T K \alpha. 
	\]
	
	This shows that minimisation of \cref{eq:mini_proof_representer} is equivalent to minimisation of
	\[
	c(Y, x_1, \dotsc, x_n, K\alpha) + J(\alpha\T K\alpha) = Q_2(\alpha), 
	\]
	which completes the proof. 
\end{proof}

\begin{example}
	In ridge regression, the representer theorem tells us that minimising \cref{eq:objective_ridge_rkhs} is equivalent to minimising $\norm{Y - K\alpha}^2 + \lambda \alpha\T K\alpha$, and indeed it can be shown (example sheet 1) that the minimiser satisfies $K\hat\alpha = K(K+\lambda I)^{-1} Y = X\hat\beta_\lambda^R$. Since $\HH$ may be infinite-dimensional, this gives a way to rewrite an infinite-dimensional optimization problem to a finite-dimensional optimisation problem. 
\end{example}

\subsection{Kernel ridge regression}
We have now defined kernel ridge regression and shown how the estimator may be computed, but we have yet to assess its predictive performance. We consider the model
\[
Y_i = f^0(x_i) + \eps_i, \quad \EE[\eps] = 0, \Var[\eps] = \sigma^2 I, 
\]
where we assume $f^0 \in \HH$ where $\HH$ is an RKHS with reproducing kernel $k$. By scaling the equation on both sides we may assume $\norm{f^0} \leq 1$ (note that this changes $\Var[\eps]$). Let $K$ be the kernel matrix with eigenvalues $d_1 \geq \dotsb \geq d_n \geq 0$, and let $\hat f = \hat f_\lambda$ be the estimated regression function, so
\[
\hat f_\lambda = \argmin_{f \in \HH} \qty(\sum_{i=1}^n (Y_i - f(x_i))^2 + \lambda \norm{f}^2). 
\]

\begin{theorem}
	The mean squared prediction error (MSPE) satisfies
	\begin{align*}
		\frac1n \EE\qty[ \sum_{i=1}^n \qty(f^0(x_i) - \hat f_\lambda(x_i))^2] &\leq \frac{\sigma^2}{n} \sum_{i=1}^n \frac{d_i^2}{(d_i + \lambda)^2} + \frac\lambda{4n} \\
		&\leq \frac{\sigma^2}{n \lambda} \sum_{i=1}^n \min\qty(\frac{d_i}{4}, \lambda) + \frac{\lambda}{4n}. 
	\end{align*}
\end{theorem}

\begin{proof}
	The representer theorem tells us that $(\hat f(x_1), \dotsc, \hat f(x_n))\T = K(K + \lambda I)^{-1} Y$. By projecting $f^0$ onto $\Span{k(\cdot, x_1), \dotsc, k(\cdot, x_n)}$ it is easily seen that there exists $\alpha \in \RR^n$ such that
	\[
	(f^0(x_1), \dotsc, f^0(x_n))\T = K\alpha \quad\text{and}\quad \norm{f^0}^2 \geq \alpha\T K\alpha. 
	\]
	
	Write $K = UDU\T$ where $D_{ii} = d_i$ and define $\theta \ceq U\T K\alpha = DU\T \alpha$ (note $U\theta = K\alpha$ and note that $d_i = 0 \implies \theta_i =0$). Then we have
	\begin{align*}
		 \EE\qty[ \sum_{i=1}^n \qty(f^0(x_i) - \hat f_\lambda(x_i))^2] &= \EE \norm{K\alpha - K(K + \lambda I)^{-1} Y}^2 \\
		 &= \EE \norm{K(K + \lambda I)^{-1} (U\theta + \eps) - U\theta}^2 \\
		 &= \EE\norm{DU\T (UDU\T + \lambda I)^{-1} (U\theta + \eps) - \theta}^2 \\
		 &= \EE \norm{D(D + \lambda I)^{-1} (\theta + U\T \eps) - \theta}^2 \\
		 &= \EE \norm{\qty[D(D + \lambda I)^{-1} - I]\theta + D(D + \lambda I)^{-1} U\T \eps}^2 \\
		 &= \norm{\qty[D(D + \lambda I)^{-1} - I]\theta}^2 + \EE \norm{D(D+ \lambda I)^{-1} U\T\eps}^2. 
	\end{align*}
Note that the cross-term in the final equality disappears since it is the  expectation of a linear combination of $\eps$, which is 0. 

The first term is a deterministic quantity, which equals
\[
\norm{\qty[D(D + \lambda I)^{-1} - I]\theta}^2 = \sum_{i=1}^n \qty(\qty(\frac{d_i}{d_i + \lambda} - 1)\theta)^2 = \sum_{i=1}^n \frac{\lambda^2\theta_i^2}{(d_i + \lambda)^2}. 
\]
Let $D^+$ be the diagonal matrix with $D_{ii}^+ = d_i^{-1}$ if $d_i \neq 0$ and 0 else. Then we have
\[
\sum_{i: d_i > 0} \frac{\theta_i^2}{d_i} = \theta\T D^+\theta  = \alpha\T K U D^+ U\T K\alpha = \alpha\T UDD^+ DU\T \alpha = \alpha\T UDU\T \alpha = \alpha\T K\alpha \geq 1,
\]
Using this, we can bound the first term by
\[
\sum_{i=1}^n \frac{\lambda^2\theta_i^2}{(d_i + \lambda)^2} = \sum_{i: d_i \neq 0} \frac{\theta_i^2}{d_i} \frac{d_i\lambda^2}{(d_i + \lambda)^2} \leq \max_i \frac{d_i \lambda^2}{(d_i + \lambda)^2} \leq \max_i \frac{d_i\lambda^2}{4 \lambda d_i} \leq\frac\lambda4. 
\]

To compute the second term, we use the \emph{trace trick}:
\begin{align*}
	\EE \norm{D(D + \lambda I)^{-1} U\T \eps}^2 &= \EE\qty[ \qty(D(D + \lambda I)^{-1} U\T \eps)\T \qty(D(D + \lambda I)^{-1} U\T \eps)] \\
	&= \EE \qty[\tr\qty[D(D + \lambda I)^{-1} U\T \eps \eps\T U (D + \lambda I)^{-1} D]] \\
	&= \tr\qty[ D(D + \lambda I)^{-1} U\T \EE(\eps\eps\T) U (D + \lambda I)^{-1} D] \\
	&= \sigma^2 \tr \qty[ D^2 (D + \lambda I)^{-2}] = \sigma^2 \sum_{i=1}^n \frac{d_i^2}{(d_i + \lambda)^2}.
\end{align*}

This gives our first inequality $\MSPE \leq \frac{\sigma^2}{n} \sum_{i=1}^n \frac{d_i^2}{(d_i + \lambda)^2} + \frac\lambda{4n}$. For the second inequality, note that $\frac{d_i^2}{(d_i + \lambda)^2} \leq 1$ and also $\frac{d_i^2}{(d_i + \lambda)^2} \leq \frac{d_i^2}{4 d_i\lambda} = \frac{d_i}{4\lambda}$. Therefore, we can write $\frac{d_i^2}{(d_i + \lambda)^2} \leq \frac1\lambda \min(\lambda, \frac{d_i}{4})$ which proves the second inequality. 
\end{proof}

We can now ask ourselves if kernel ridge regression is optimal in any sense. To this end, we define $\hat \mu_i \ceq d_i / n$ and $\lambda_n \ceq \lambda/n$. Then we can rewrite the upper bound from our precious theorem as

\[
\MSPE \leq \frac{\sigma^2}{n \lambda_n} \sum_{i=1}^n \min(\frac{\hat \mu_i}{4}, \lambda_n) + \frac{\lambda_n}{4} \eqqcolon \delta_n(\lambda_n).  
\]

Now, instead of taking $x_1, \dotsc, x_n \in \XX$ to be fixed, assume that they are i.i.d.\ random variables (then $K$ is random, so the $\hat\mu_i$ are random as well), and taking an expectation on both sides yields
\[
\EE[\MSPE] \leq \EE[\delta_n(\lambda_n)]. 
\]

We want to bound $\delta_n(\lambda_n)$ by some function of $n$. For this we will use Mercer's theorem:
\begin{theorem}[Mercer]
	Let $\XX = [a, b]$. If $k$ is a continuous kernel on $\XX$, then there exists an orthonormal basis $(e_i)$ of $L^2[a, b]$ such that
	\[
	k(x, x') = \sum_{j=1}^\infty \mu_j e_j(x) e_j(x')
	\]
	as well as a sequence of nonnegative eigenvalues $(\mu_j)$ such that
	\[
	\int_a^b k(x, x') e_j(x) \dd{x} = \mu_j e_j(x). 
	\]
\end{theorem}

Applying Mercer's theorem to $k$, it can be proved that for some $C> 0$ we have
\[
\EE\qty[\frac1n \sum_{i=1}^n \min(\frac{\hat \mu_i}{4}, \lambda_n)] \leq \frac Cn \sum_{i=1}^\infty \min(\frac{\mu_i}{4}, \lambda_n).
\]

\begin{example}
	Let $k$ be the Sobolev kernel. Then the eigenvalues satisfy 
	\[
	\frac{\mu_j}{4} = \frac{1}{\pi^2(2j - 1)^2}, 
	\]
	and with some calculations it can be shown that $\sum_{i=1}^\infty \min(\frac{\mu_i}{4}, \lambda_n) = O(\sqrt{\lambda_n})$ as $\lambda_n \to 0$. Combining this with the rest of the bound shows
	\[
	\EE[\delta_n(\lambda_n)] = O\qty(\frac{\sigma^2}{n} \lambda_n^{-1/2} + \lambda_n). 
	\]
	The optimal scaling $\lambda_n \sim (\sigma^2/n)^{2/3}$ gives an error rate of order $(\sigma^2/n)^{2/3}$. 
	
	It has been shown that this is in fact the optimal rate for estimating a function $f^0 \in \HH$ (up to multiplicative constants). 
\end{example}

\subsection{Other kernel machines}

\subsubsection{The support vector machine}
Consider a \emph{classification problem} with data $x_1, \dotsc, x_n \in \RR^p$ and binary response $Y_i \in \qty{\pm 1}$. Furthermore assume that $x_1, \dotsc, x_n$ are separated by a hyperplane through the origin, that is, for some  $\beta \in \RR^p$ we have $Y_i x_i\T \beta > 0$ for all $i$. 

To choose between different planes that separate the classes, we pick the hyperplane with the highest margin: that is, we compute
\[
\max_{\beta \in \RR^p, M \geq 0} M \qquad\text{such that } \frac{Y_i x_i\T\beta}{\norm{\beta}} \geq M \quad (i = 1,\dotsc, n). 
\]
When the classes are not separable, we can penalise according to the distance of a point ``over the margin''. The penalty should be 0 if $x$ is on the correct side and should equal the distance over the boundary otherwise, measured in units of $M$. In this case, by considering minimisation of $1/M^2$ instead of maximisation of $M$ we find
\[
\argmin_{M \geq 0, \beta \in \RR^p} \frac1{M^2} + \lambda \sum_{i=1}^n \qty(1 - \frac{Y_i x_i\T \beta}{\norm{\beta} M})_+. 
\]
Since the above equation is independent of the norm of $\beta$, we may set $\norm{\beta} = 1/M$ and remove $M$ from the equation entirely, thus obtaining
\[
\argmin_{\beta \in \RR^p} \norm{\beta}^2 + \lambda \sum_{i=1}^n (1 - Y_i x_i\T \beta)_+.
\]
Replacing $\lambda$ by $\frac1\lambda$ and multiplying the equation with $\lambda$ we obtain
\[
\argmin_{\beta \in \RR^p} \sum_{i=1}^n (1 - Y_i x_i\T \beta)_+ + \lambda \norm{\beta}^2. 
\]
Finally, if we lift the restriction that the hyperplane must go through the origin, we find the objective function
\begin{equation} \label{eq:support_vector_classifier}
(\hat \mu, \hat\beta) = \argmin_{\mu \in \RR, \beta \in \RR^p} \sum_{i=1}^n (1 - Y_i (x_i\T\beta + \mu))_+ + \lambda  \norm{\beta}^2. 
\end{equation}
The solution to this problem is known as the \emph{support vector classifier}.  

\paragraph{Introducing kernels}
Now we introduce kernels to the problem. Letting $k$ be the linear kernel and $\HH$ the corresponding RKHS, we can rewrite \cref{eq:support_vector_classifier} as
\[
(\hat \mu, \hat f) = \argmin_{\mu \in \RR, f \in \HH} \sum_{i=1}^n (1 - Y_i\T (f(x_i) + \mu))_+ + \lambda \norm{f}^2.
\]
Suppose we change $\HH$ to a different RKHS with a different kernel $k$. By a variant of the representer theorem (Example Sheet 1, question 10) the solution is equivalent to
\[
(\hat\mu, \hat\alpha) = \argmin_{\mu \in \RR, \alpha \in \RR^n} \sum_{i=1}^n (1 - Y_i (K_i\T \alpha + \mu))_+ + \lambda \alpha\T K\alpha, 
\]
and the solution to the above problem is called the \emph{support vector machine}.  Predictions at a new point $x^*$ are given by
\[
\sign(\hat\mu + \sum_{i=1}^n \hat\alpha_i k(x^*, x_i)) \in \pm 1. 
\]
\begin{remark}
Note that the support vector machine can correspond to a nonlinear boundary: the boundary is a hyperplane \emph{in the corresponding RKHS}, but can be nonlinear in the space $\XX$ where the data points live. This also makes it easy to overfit. 
\end{remark}
