\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=3cm]{geometry}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{mathtools, amsmath, amssymb, amsthm, enumerate, mdframed, bbm, graphicx, float, physics, xcolor, cleveref}

\hypersetup{
    colorlinks   = true, %Colours links instead of ugly boxes
    urlcolor     = blue, %Colour for external hyperlinks
    linkcolor    = blue, %Colour of internal links
    citecolor   = red %Colour of citations
}

% Definition of numbered environments.
% Usage: \begin{theorem} ... \end{theorem}
% Remark has no numbering.
\theoremstyle{plain}
\newtheorem{question}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% Question with box around it
\newenvironment{qbox}{\begin{mdframed}\begin{question}}{\end{question}\end{mdframed}}

% A proof with "solution" instead of "proof" and no QED symbol
\newenvironment{solution}{\begin{proof}[Solution]\renewcommand\qedsymbol{}}{\end{proof}}

% Some renewed commands
\renewcommand{\vec}{\mathbf}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\phi}{\varphi}

% Frequently used math alphabets
\newcommand{\Bb}{\mathbb}
\newcommand{\Cal}{\mathcal}
\newcommand{\Bf}{\mathbf}
\newcommand{\Rm}{\mathrm}

% Frequently used letters in the blackboard alphabet
\newcommand{\CC}{\Bb C}
\newcommand{\NN}{\Bb N}
\newcommand{\PP}{\Bb P}
\newcommand{\QQ}{\Bb Q}
\newcommand{\RR}{\Bb R}
\newcommand{\EE}{\Bb E}

\newcommand\XX{\Cal X}

% Usage: \ang{...} is equivalent to \langle ... \rangle, while \ang*{...} is equivalent to \left\langle ... \right\rangle
% For other delimiters: use \qty from the physics package (i.e., \qty(...))
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Frequently used commands
\newcommand{\T}{^\top} % Matrix transpose A\T
\newcommand{\C}{^\complement} % Set complement A\C
\newcommand\ceq\coloneqq % Definitions :=
\newcommand\pow{\Cal P} % Power sets
\newcommand\eps\epsilon
\newcommand\ind{\mathbbm 1} % Blackboard 1 for indicator functions
\newcommand\restr{\mathord\restriction}
\newcommand\TODO{{\color{red} TODO: }}

% Functions that appear frequently
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Modern Statistical Methods --- Example Sheet 1} % subject
\author{Lucas Riedstra}
\date{...} % date

\begin{document}
\maketitle

\begin{qbox}
    Consider minimising the following objective involving response $Y \in \RR^n$ and design matrix $X \in \RR^{n \times p}$ over $(\mu, \beta) \in \RR \times \RR^p$: 
    \[
    \norm{Y - \mu \Bf{1} - X\beta}_2^2 + J(\beta).
    \]
    Here $J \colon \RR^p \to \RR$ is an arbitrary penalty function. Suppose $\bar{X_k} = 0$ for $k = 1, \dotsc, p$. Assuming that a minimiser $(\hat\mu, \hat\beta)$ exists, show that $\hat\mu = \bar Y$. Now take $J(\beta) = \lambda \norm{\beta}_2^2$ so we have the ridge regression objective. Show that
    \[
    \hat\beta = (X\T X + \lambda I)^{-1} X\T Y.
    \] 
    From here onwards, whenever we refer to ridge regression, we will assume $X$ has had its columns mean-centred. 
\end{qbox}

\begin{solution}
    If a minimiser $\hat\mu$ exists, then the derivative of the objective must be 0. We expand the objective function as
    \begin{align*}
    \norm{Y - \mu\Bf{1} - X\beta}_2^2 + J(\beta) &= \sum_{i=1}^n \qty(Y_i - \mu - (X\beta)_i)^2 + J(\beta) \\
    &= \sum_{i=1}^n \qty(Y_i - \mu - \sum_{j=1}^p X_{ij}\beta_j)^2. 
    \end{align*}
Now, differentiating w.r.t.\ $\mu$ yields
\begin{align*}
\pdv{\mu} \norm{Y - \mu\Bf{1} - X\beta}_2^2 + J(\beta) &= -2 \sum_{i=1}^n \qty( Y_i - \mu - \sum_{j=1}^p X_{ij}\beta_j) \\
&= -2 \qty(\sum_{i=1}^n Y_i  - n\mu - \sum_{i=1}^n \sum_{j=1}^p X_{ij}\beta_j) \\
&= -2 \qty(\sum_{i=1}^n Y_i  - n\mu - \sum_{j=1}^p \beta_j (\sum_{i=1}^p X_{ij})) \\
&= -2 \qty( \sum_{i=1}^n Y_i - n\mu), 
\end{align*}
since $\sum_{i=1}^p X_{ij} = 0$ for all $i$ by assumption. Now, setting the derivative to 0 shows that $\hat\mu = \frac1n \sum_{i=1}^n Y_i = \bar Y$ is the minimiser. Since the second derivative of the objective function is constantly $+2$, this $\hat\mu$ is indeed a minimum. 

The computation of $\hat\beta$ is given in the lecture notes. 
\end{solution}

\begin{question}
    Consider performing ridge regression when $Y = X \beta^0 + \eps$, where $X \in \RR^{n \times p}$ has full column rank, and $\Var(\eps) = \sigma^2 I$. Let the SVD of $X$ be $UDV\T$ and write $U\T X \beta^0 = \gamma$. Show that
    \[
    \frac1n \EE\norm{X \beta^0 - X \hat\beta_\lambda^R}_2^2 = \frac1n \sum_{j=1}^p \qty(\frac\lambda{\lambda + D_{jj}^2})^2 \gamma_j^2 + \frac{\sigma^2}{n} \sum_{j=1}^p \frac{D_{jj}^4}{(\lambda + D_{jj}^2)^2}. 
    \]
\end{question}

\begin{solution}
    We write $\hat\beta \ceq \hat\beta_\lambda^R$. We write out
    \[
    X\hat\beta = UDV\T (V\T D^2 V + \lambda I)^{-1} V\T D U\T Y =  UD^2 (D^2 + \lambda I)^{-1} U\T Y = \sum_j \frac{D_{jj}^2}{\lambda + D_{jj}^2} U_j U_j\T Y. 
    \]
    Since $X\beta^0 = UDV\T \beta^0$ lies in the range of $U$, we have $X\beta^0 = \sum_j U_j U_j\T X\beta^0$. 
    \begin{align*}
        X\beta^0 - X\hat\beta &= \sum_j U_j U_j\T X \beta^0 -  \sum_j \frac{D_{jj}^2}{\lambda + D_{jj}^2} U_j U_j\T Y \\
        &= \sum_j \qty(U_j\T X \beta^0 - \frac{D_{jj}^2 U_j\T Y}{\lambda + D_{jj}^2}) U_j \\
        &= \sum_j \qty(\frac{D_{jj}^2 U_j\T (X\beta^0 - Y) + \lambda U_j\T X \beta^0}{\lambda + D_{jj}^2})
    \end{align*}
and therefore
\begin{align*}
    \frac1n \EE\norm{X\beta^0 - X\hat\beta}_2^2 &= \frac1n \sum_j \EE \qty(\frac{D_{jj}^2 U_j\T (X\beta^0 - Y) + \lambda U_j\T X \beta^0}{\lambda + D_{jj}^2})^2 \\
    &\overset{*}= \frac1n \sum_j \frac{D_{jj}^4}{(\lambda + D_{jj}^2)^2} \EE[(U_j\T (X\beta^0 - Y))^2] + \frac1n \sum_j \qty(\frac{\lambda}{\lambda + D_{jj}^2})^2  (U_j\T X \beta^0)^2 \\
&\overset{**}= \frac{\sigma^2}{n} \sum_j \frac{D_{jj}^4}{(\lambda + D_{jj}^2)^2}  + \frac1n \sum_j \qty(\frac{\lambda}{\lambda + D_{jj}^2})^2  \gamma_j^2.
\end{align*}
Note that the cross-terms in $(*)$ disappear since they are linear combinations of $\EE(X\beta^0 - Y) = 0$, while the equality $**$ holds since 
\[
\EE[(U_j\T(X \beta^0 - Y))^2] = \Var(U_j\T (X\beta^0 - Y)) = U_j\T \Var(X\beta^0 - Y) U_j = \sigma^2 U_j\T U_j = \sigma^2. 
\]
\end{solution}

\begin{question}
    Show that the ridge regression estimates can be otained by ordinary least squares regression on an augmented data set with $\sqrt\lambda I$ added to the botten of $X$, and $p$ zeroes added to the end of the response $Y$. 
\end{question}

\begin{solution}
    Let $\tilde X, \tilde Y$ be the new data set, then our ordinary least squares objective function is
    \begin{align*}
    L(\beta) & \ceq \norm{\tilde Y - \tilde X \beta}^2 \\
    &= \norm{Y - X\beta}^2 + \norm{\sqrt\lambda\beta}^2 \\
    &= \norm{Y - X\beta}^2 + \lambda \norm{\beta}^2,
    \end{align*}
which is exactly our ridge regression objective function. 
\end{solution}

\begin{question}
    In the following, assume that forming $AB$ where $A \in \RR^{a \times b}, B \in \RR^{b \times c}$ requires $O(abc)$ computational operations, and that if $M \in \RR^{d \times d}$ is invertible, then forming $M^{-1}$ requires $O(d^3)$ operations. 
    \begin{enumerate}[(a)]
        \item Suppose we wish to apply ridge regression to data $(Y, X) \in \RR^n \times \RR^{n \times p}$ with $n \gg p$. A complication is that the data is split into $m$ separate datasets of size $n/m \in \NN$, 
        \[
        Y = \mqty( Y^{(1)} \\ \vdots \\ Y^{(m)}), \quad X = \mqty( X^{(1)} \\\vdots\\X^{(m)}),
        \]
        with each dataset located on a different server. Moving large amounts of data between servers is expensive. Explain how one can produce ridge estimates $\hat\beta_\lambda$ by communicating only $O(p^2)$ numbers from each server to some central server. What is the total order of the computation time required at each server, and at the central server for your approach? 
        
        \item Now suppose instead that $p \gg n$ and it is instead the variable sthat are split across $m$ servers, so each server has only a subset of $p/m \in \NN$ variables for each observation, and some central server stores $Y$. Explain how one can obtain the fitted values $X\hat\beta_\lambda$ communicating only $O(n^2)$ numbers from each server to the central server. What is the total order of the computation time required at each server, and at the central server for your approach? 
    \end{enumerate}
\end{question}

\begin{solution}
    \begin{enumerate}[(a)]
        \item Note that
        \[
        X\T Y = \mqty( X^{(1)\top} \dotsc X^{(m)\top}) \mqty( Y^{(1)}\\\vdots\\Y^{(m)}) = \sum_{i=1}^m X^{(i)\top} Y^{(i)}. 
        \]
        Each of the $X^{(i)\top} Y^{(i)}$ can be computed at the data server (requiring $O(np)$ computation) and produces $p$ numbers per data server. 
        
        Analogously, we have
        \[
        X\T X = \sum_{i=1}^m X^{(i)\top} X^{(i)},
        \]
        which can also be computed at the data server (requiring $O(np^2)$ computation) and produces $p^2$ numbers per data server. In the end, each server will therefore have $O(p^2)$ numbers to send, having done $O(np^2)$ computations. 
        
        When each server sends their numbers to the central data server, the central data server can then compute $(X\T X + \lambda I)^{-1}$ using $O(p^3)$ computations and then multiply that with $X\T Y$ which will require $O(np)$ computations. 
        
        \item In this case, we have 
        \[
        X = \mqty( X^{(1)} \dotsb X^{(m)} ). 
        \]
        We use the alternative form of  $\hat\beta_\lambda$, namely
        \[
        X \hat\beta_\lambda^R = XX\T (XX\T + \lambda I)^{-1} Y.
        \]
        
        Now, we have $XX\T = \sum_{i=1}^m X^{(i)} X^{(i)\top}$, and each $X^{(i)} X^{(i)\top}$ can be computed at the data server, requiring $O(n^2 p)$ computations and producing $O(n^2)$ numbers to send.  
        
        After sending these numbers over to the central server, the inverse $(XX\T + \lambda I)^{-1}$ can be computed in $O(n^3)$ time, then multiplied from the left with $XX\T$ in $O(n^3)$ time, and multiplied with $Y$ in $O(n^2)$ time. The total amount of computation at the central server will therefore be $O(n^3)$ as well. 
    \end{enumerate}
\end{solution}

\begin{question}
	Prove Proposition 4 in the notes. \emph{Hint: for part (ii) it may help to consider the eigendecompositions of positive semi-definite matrices $K^{(1)}$ and $K^{(2)}$ derived from kernels $k_1$ and $k_2$ in the form $K^{(1)} = PDP\T = \sum_{i=1}^n P_iP_i\T D_{ii}$ for example. }
\end{question}

\begin{proof}
	Proposition 4 is the following:
	\begin{mdframed}
		Suppose $k_1, k_2, \dotsc$ are kernels. 
		\begin{enumerate}[(i)]
			\item If $\alpha_1, \alpha_2 \geq 0$ then $\alpha_1 k_1 + \alpha_2 k_2$ is a kernel. If $\lim_{k\to\infty} k(x, x') \eqqcolon k(x, x')$ exists for all $x, x' \in \Cal X$, then $k$ is a kernel. 
			\item The pointwise product $k = k_1k_2$ is a kernel.
		\end{enumerate}
	\end{mdframed}

Let $x_1, \dotsc, x_n \in \Cal X$ and define the positive definite matrix $K^{(m)}$ by $K^{(m)}_{i, j} = k_m(x_i, x_j)$. 
\begin{enumerate}[(i)]
	\item Since $(\alpha_1 k_1 + \alpha_2 k_2)(x_i, x_j) = \alpha_1 k_1(x_i, x_j) + \alpha_2 k_2(x_i, x_j)$, the kernel matrix $K_{ij} \ceq (\alpha_1 k_1 + \alpha_2 k_2)(x_i, x_j)$ equals $\alpha_1 K_1 + \alpha_2 K_2$, and is therefore clearly symmetric positive semi-definite. This shows that $\alpha_1 k_1 + \alpha_2 k_2$ is a kernel. 
	
	For the second part (the pointwise limit of kernels is a kernel), it suffices to show that the entrywise limit of positive semi-definite matrices is again positive semidefinite. But this is clear: if $K^{(m)} \to K$ entrywise, then for any $\vec x \in \RR^n$ we have
	\[
	\vec x\T K \vec x = \vec x\T (\lim_{m\to\infty} K^{(m)}) \vec x = \lim_{m\to\infty} \vec x\T K^{(m)}\vec x \geq 0,
	\]
	so $K$ is positive semi-definite.
	
	\item Let $K^{(1)}$ and $K^{(2)}$ be positive semi-definite, then we must show that the pointwise product $K^{(1)} \circ K^{(2)}$ is positive semi-definite as well. Write $K^{(1)} = \sum_{\ell=1}^n \lambda_\ell P_\ell P_\ell\T$ and $K^{(2)} = \sum_{\ell=1}^n \mu_\ell Q_\ell Q_\ell\T$, where $\lambda_i, \mu_i \geq 0$ and $\qty{P_\ell}$, $\qty{Q_\ell}$ are orthonormal bases of $\RR^n$. 
	
	Then we find
	\begin{align*}
	(K^{(1)} \circ K^{(2)})_{ij} &= \qty(\sum_m \lambda_m P_{im} P_{jm}) \qty(\sum_\ell \mu_\ell Q_{i\ell} Q_{j\ell}) = \sum_{m, \ell} \lambda_m \mu_\ell (P_{im} P_{jm} Q_{i\ell} Q_{j\ell}) \\
	&= \sum_{m, \ell} \lambda_m \mu_\ell \qty[(P_m \circ Q_\ell)(P_m \circ Q_\ell)\T]_{ij},
	\end{align*}
	
	and therefore $K^{(1)} \circ K^{(2)} = \sum_{m, \ell} \lambda_m \mu_\ell \qty[(P_m \circ Q_\ell)(P_m \circ Q_\ell)\T]$, which is a nonnegative combination of positive semi-definite matrices and therefore positive semi-definite. 
\end{enumerate}
\end{proof}

\begin{question}
	Let $\XX \ceq \qty{x \in \RR^d : \norm{x} < 1}$. Show that $k(x, x') \ceq (1 - x\T x')^{-\alpha}$ defined on $\XX \times \XX$, where $\alpha > 0$, is a kernel.
\end{question}

\begin{proof}
	Note by Cauchy-Schwarz we have for $x, x' \in \XX$ that $\abs{x\T x'} \leq \norm{x} \norm{x'} < 1$, so $k$ is well-defined. 
	
	Symmetry of $k$ is clear. Define the function 
	\[
	\phi \colon (-\infty, 1) \to \RR \colon x \mapsto (1-x)^{-\alpha}. 
	\]
	Then $\phi$ is continuous, and we can compute its Taylor series around 0. It is computed that 
	\[
	\phi^{(k)}(x) = (1-x)^{-\alpha - k} \prod_{j=0}^{k-1}(\alpha + j) \implies \phi^{(k)}(0) = \prod_{j=0}^{k-1} (\alpha + j). 	\]
	Therefore, the Taylor series of $\phi$ is given by
	\[
	\sum_{k=0}^\infty \qty(\prod_{j=0}^{k-1} (\alpha + j)) \frac{x^k}{k!}, 
	\]
	and its radius of convergence is given by
	\[
	\lim_{k\to\infty} \frac{\frac{1}{k!}\prod_{j=0}^{k-1} (\alpha + j)}{\frac{1}{(k+1)!} \prod_{j=0}^k (\alpha + j)} = \lim_{k\to\infty} \frac{k}{\alpha + k} = 1,
	\]
	so $\phi$ agrees with its Taylor series on $(-1, 1)$. By Cauchy-Schwarz, for $x, x' \in \XX$ we have $x_i\T x_j \leq \norm{x_i} \norm{x_j} < 1$, so we can write for $x, x' \in \XX$
	\[
	k(x, x') = \phi(x\T x') = \sum_{k=0}^\infty \qty(\prod_{j=0}^{k-1} (\alpha + j)) \frac{(x\T x')^k}{k!}.
	\]
	By proposition 4, every term in the sum is a kernel, and therefore the limit is a kernel as well. 
\end{proof}


\begin{question}
	Suppose we have a matrix of predictors $X \in \RR^{n \times p}$ where $p \gg n$. Explain how to obtain the fitted values of the following ridge regression using the kernel trick:
	\begin{gather*}
		\text{Minimise over $\beta \in \RR^p$, $\theta \in \RR^{p(p-1)/2}$, $\gamma \in \RR^p$,} \\
		\sum_{i=1}^n \qty(Y_i - \sum_{k=1}^p X_{ik}\beta_k - \sum_{k=1}^p \sum_{j=1}^{k-1} X_{ik} X_{ij} \theta_{jk} - \sum_{k=1}^p X_{ik}^2\gamma_k)^2 + \lambda_1 \norm{\beta}^2 + \lambda_2 \norm{\theta}^2 + \lambda_3\norm{\gamma}^2. 
	\end{gather*}
Note that we have indexed $\theta$ with two numbers for convenience. 
\end{question}

\begin{proof}
	Our linear model has predictors
	\[
	\qty{X_{ik} \mid 1 \leq k \leq p} \cup \qty{X_{ij} X_{ik} \mid 1 \leq j < k \leq p} \cup \qty{X_{ik}^2 \mid 1 \leq k \leq p}
	\]
	for $i = 1, \dotsc, k$. We want to assume that $\lambda_1 = \lambda_2 = \lambda_3$: to this end, define $\xi = \frac{\lambda_1}{\lambda_2}$ and $\eta = \frac{\lambda_3}{\lambda_1}$, and replace $\theta$ by $\xi\theta$ and $\gamma$ by $\eta\gamma$. This means we will have to replace our predictors by
	\[
	Z_i \ceq 
	(X_{ik} \mid 1 \leq k \leq p)\T \cup (\xi X_{ij} X_{ik} \mid 1 \leq j < k \leq p)\T \cup (\gamma X_{ik}^2 \mid 1 \leq k \leq p)\T
	\]
	We use the kernel trick: note that
	\begin{align*}
	\ang{Z_i, Z_j} &= \sum_k X_{ik}X_{jk} + \xi^2 \sum_{k < \ell} X_{ik} X_{i\ell} X_{jk} X_{j\ell} + \gamma^2 \sum_k X_{ik}^2 X_{jk}^2 \\
	&= \qty(\frac{1}{\sqrt 2\xi} + \frac{\xi}{\sqrt 2}\sum_k X_{ik}X_{jk})^2 + (\gamma^2 - \frac{\xi^2}{2})\sum_k X_{ik}^2 X_{jk}^2 - \frac{1}{2\xi^2}.
	\end{align*}
	We have therefore rewritten $\ang{Z_i, Z_j}$ into something that can be computed in $O(p)$, which is exactly the aim of the kernel trick.
\end{proof}

\begin{question}
	Let $\hat\alpha$ be a minimiser of $\norm{Y - K\alpha}^2 + \lambda \alpha\T K\alpha$, with $K$ being a kernel matrix as usual. Show that $K\hat\alpha = K(K + \lambda I)^{-1} Y$. 
\end{question}

\begin{solution}
	Suppose $\hat\alpha$ minimises $Q(\alpha) \ceq \norm{Y - K\alpha}^2 + \lambda \alpha\T K\alpha$. Then we have
	\begin{align*}
		0 = \nabla_\alpha Q(\hat\alpha) &= -2 K (Y - K\hat\alpha) + 2\lambda K\hat\alpha \\
		\lambda K\hat\alpha &= KY - K^2\hat\alpha \\
		(K + \lambda) K\hat\alpha &= KY \\
		K\hat\alpha &= (K + \lambda)^{-1} KY.
	\end{align*}
Since $K$ and $(K + \lambda)^{-1}$ are simultaneously diagonalisable, they commute, and therefore $K\hat\alpha = K(K + \lambda)^{-1} Y$. 
%\begin{align*}
%(K + \lambda)^{-1} KY &= (U(D + \lambda) U\T)^{-1} UDU\T Y \\
%&= U(D + \lambda)^{-1} D U\T Y \\
%&= UD (D+ \lambda)^{-1} U\T Y\\
%&= UDU\T (U (D+ \lambda) U\T) Y\\
%&= K(K + \lambda)^{-1} Y,
%\end{align*}
%which proves the claim.
\end{solution}

\begin{question}
	...
\end{question}

\begin{proof}
	content...
\end{proof}

\begin{question}
	...
\end{question}

\begin{proof}
	...
\end{proof}

\begin{question}
	Show from first principles that the Sobolev kernel is indeed a (positive definite) kernel.
\end{question}

\begin{solution}
	Let $\XX = [0, 1]$ and $k(x, x') = \min(x, x')$ the Sobolev kernel. We must then show that for any $x_1, \dotsc, x_n \in \XX$, the matrix $K_{ij} = \min(x_i, x_j)$ is positive semi-definite. 
\end{solution}
\end{document}