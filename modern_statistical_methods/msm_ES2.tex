\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[margin=3cm]{geometry}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools, amsmath, amssymb, amsthm, mdframed, bbm, graphicx, float, physics, xcolor, cleveref}

\hypersetup{
    colorlinks   = true, %Colours links instead of ugly boxes
    urlcolor     = blue, %Colour for external hyperlinks
    linkcolor    = blue, %Colour of internal links
    citecolor   = red %Colour of citations
}

% Definition of numbered environments.
% Usage: \begin{theorem} ... \end{theorem}
% Remark has no numbering.
\theoremstyle{plain}
\newtheorem{question}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

% Question with box around it
\newenvironment{qbox}{\begin{mdframed}\begin{question}}{\end{question}\end{mdframed}}

% A proof with "solution" instead of "proof" and no QED symbol
\newenvironment{solution}{\begin{proof}[Solution]\renewcommand\qedsymbol{}}{\end{proof}}

% Some renewed commands
\renewcommand{\vec}{\mathbf}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\phi}{\varphi}

% Frequently used math alphabets
\newcommand{\Bb}{\mathbb}
\newcommand{\Cal}{\mathcal}
\newcommand{\Bf}{\mathbf}
\newcommand{\Rm}{\mathrm}

% Frequently used letters in the blackboard alphabet
\newcommand{\CC}{\Bb C}
\newcommand{\NN}{\Bb N}
\newcommand{\PP}{\Bb P}
\newcommand{\QQ}{\Bb Q}
\newcommand{\RR}{\Bb R}
\newcommand{\EE}{\Bb E}

\newcommand\XX{\Cal X}
\newcommand\HH{\Cal H}

% Usage: \ang{...} is equivalent to \langle ... \rangle, while \ang*{...} is equivalent to \left\langle ... \right\rangle
% For other delimiters: use \qty from the physics package (i.e., \qty(...))
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Frequently used commands
\newcommand{\T}{^\top} % Matrix transpose A\T
\newcommand{\C}{^\complement} % Set complement A\C
\newcommand\ceq\coloneqq % Definitions :=
\newcommand\pow{\Cal P} % Power sets
\newcommand\eps\epsilon
\newcommand\ind{\mathbbm 1} % Blackboard 1 for indicator functions
\newcommand\restr{\mathord\restriction}
\newcommand\TODO{{\color{red} TODO: }}
\renewcommand\P{^\perp}

% Functions that appear frequently
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\Nul{\Cal N}
\newcommand\Ran{\Cal R}

\title{Modern Statistical Methods --- Example Sheet 2} % subject
\author{Lucas Riedstra}
\date{...} % date

\begin{document}
\maketitle
\begin{question}
	Let $Y \in \RR^n$ be a vector of responses, $\Phi \in \RR^{n \times p}$ a design matrix, $J \colon [0, \infty) \to [0, \infty)$ a strictly increasing function and $c \colon \RR^n \to \RR^n$ some cost function. Set $K = \Phi\Phi\T$. Show, without using the representer theorem, that $\hat\theta$ minimises
	\[
	Q_1(\theta) \ceq c(Y, \Phi\theta) + J(\norm{\theta}_2^2)
	\]
	over $\theta \in \RR^p$ if and only if $\Phi\hat\theta = K\hat\alpha$ and $\hat\alpha$ minimises
	\[
	Q_2(\alpha) \ceq c(Y, K\alpha) + J(\alpha\T K\alpha)
	\]
	over $\alpha \in \RR^n$. 
\end{question}

\begin{proof}
	Let $\hat\theta$ be a minimiser of $Q_1$, and write $\hat\theta = \Phi\T\hat\alpha + \hat\beta$ with $\Phi\T \hat\alpha \in \Nul(\Phi)\P = \Ran(\Phi\T)$, $\hat\beta \in \Nul(\Phi)$. 
	
	Noting that $K\hat\alpha = \Phi\Phi\T \hat\alpha = \Phi\hat\theta$ and  $\norm{\Phi\T\hat\alpha} = \alpha\T K\alpha$ we see
	\[
	Q_1(\theta) = c(Y, K\hat\alpha) + J(\alpha\T K\alpha + \|\hat\beta\|^2),
	\]
	and therefore it is necessary that $\hat\beta = 0$. The claim follows. 
\end{proof}

\begin{question}
	Let $x, x' \in \RR^p$ and let $\psi \in \qty{-1, 1}^p$ be a random vector with independent components taking values $-1, 1$ each with probability $1/2$. Show that $\EE(\psi\T x \psi\T x') = x\T x'$. Construct a random feature map $\hat\phi \colon \RR^p \to \RR$ such that $\EE\qty{\hat\phi(x)\hat\phi(x')} = (x\T x)^2$. 
\end{question}

\begin{solution}
	We have 
	\begin{align*}
		\psi\T x \psi\T x' = \qty(\sum_i \psi_i x_i) \qty(\sum_j \psi_j x'_j) = \sum_i x_i x_i' + 2 \sum_{i < j} \psi_i \psi_j x_i x_j'. 
	\end{align*}
	Noting that for $i \neq j$ we have $\EE[\psi_i \psi_j] = \EE[\psi_i] \EE[\psi_j] = 0$ it follows that $\EE[\psi\T x \psi\T x'] = \sum_i x_i x_i' = x\T x'$. 
	
	Let $\psi_*$  be an identical independent copy of $\psi$ and define $\hat\phi(x) = \psi\T x \psi_*\T x$. Then we find
	\[
	\EE[\hat\phi(x)\hat\phi(x')] = \EE[\psi\T x \psi\T x']\EE[\psi_*\T x \psi_*\T x'] = (x\T x')^2. 
	\]
\end{solution}

\begin{question}
	Let $\XX = \pow(\qty{1, \dotsc, p})$ and $z, z' \in \XX$. Let $k$ be the Jaccard similarity kernel. Let $\pi$ be a random permutation of $\qty{1, \dotsc, p}$. Let $M = \min\qty{\pi(j) \mid j \in z}$, $M' = \min\qty{\pi(j) \mid j \in z'}$. Show that
	\[
	\PP(M = M') = k(z, z'),
	\]
	when $z, z' \neq \emptyset$. Now let $\psi \in \qty{-1, 1}^p$ be a random vector with i.i.d.\ components taking the values -1 or 1, each with probability $1/2$. By considering $\EE[\psi_M\psi_{M'}]$ show that the Jaccard similarity kernel is indeed a kernel. Explain how we can use the ideas above to approximate kernel ridge regression with Jaccard similarity, when $n$ is very large (you may assume none of the data points are the empty set). 
\end{question}

\begin{proof}
	We have \[
	\PP(M = M') = \PP\qty(\argmin_{j \in z \cup z'} \pi(j) \in z \cap z') = \frac{\abs{z \cap z'}}{\abs{z \cup z'}} = k(z, z') \quad\text{since $\pi$ is random.}
	\]
	
	Furthermore, we have
	\[
	\EE[\psi_M \psi_{M'}] = \PP(M = M') \EE[\psi_M^2] + \PP(M \neq M') \EE[\psi_M \psi_{M'}] = k(z, z'), 
	\]
	since for $M \neq M'$ we have $\EE[\psi_M\psi_M'] = \EE[\psi_M]\EE[\psi_{M'}] = 0$. 
	Let $z_1, \dotsc, z_n \in \XX$ with corresponding $M_1, \dotsc, M_n$, and write $\hat \psi = (\psi_{M_1}, \dotsc, \psi_{M_n})\T$, then the kernel matrix $K$ is given by $\EE[\hat\psi \hat\psi\T]$ which is positive semidefinite. 
	
	Using the random feature map $\hat\phi(z) = \psi_{M_z}$ we can approximate kernel ridge regression using the random feature map method. 
\end{proof}

\begin{question}
	Consider the logistic regression model where we assume $Y_1, \dotsc, Y_n \in \qty{-1, 1}$ are independent and
	\[
	\log(\frac{\PP(Y_i = 1)}{\PP(Y_i = -1)}) = x_i\T\beta^0. 
	\]
	Show that the maximum likelihood estimate $\beta$ minimises
	\[
	\sum_{i=1}^n \log(1 + \exp(-Y_i x_i\T \beta))
	\]
	over $\beta \in \RR^p$. 
\end{question}

\begin{proof}
	Let $(y_1, \dotsc, y_n)$ be the responses, then the likelihood function becomes
	\[
	L(\beta) = \PP(Y_1, \dotsc, Y_n \mid \beta)
	\]
	
	Note that
	\begin{align*}
		\frac{\PP(Y_i = 1)}{1 - \PP(Y_i = 1)} = \exp(x_i\T\beta) \implies \PP(Y_i = 1) = \frac{1}{1 + \exp(-x_i\T \beta)},
	\end{align*}
and analogously
\begin{align*}
	\PP(Y_i = -1) =  \frac{1}{1 + \exp(x_i\T\beta)}. 
\end{align*}
We can combine the above formulas as
\[
\PP(Y_i = y_i) = \frac{1}{1 + \exp(-y_i x_i\T\beta)}. 
\]

Therefore our the MLE $\hat\beta$ maximises
\[
L(\beta) = \prod_{i=1}^n \frac{1}{1 + \exp(-Y_i x_i\T\beta)}, 
\]
and it also maximises the log-likelihood function
\[
\log(L(\beta)) = - \sum_{i=1}^n \log(1 + \exp(-Y_i x_i\T\beta))
\]
which is of course equivalent to minimising
\[
-\log(L(\beta)) = \sum_{i=1}^n \log(1 + \exp(-Y_i x_i\T\beta)). 
\]
\end{proof}

\begin{question}
	Consider the following algorithm for model selection when we have a response $Y \in \RR^n$ and a matrix of predictors $X \in \RR^{n \times p}$.
	\begin{enumerate}[(a)]
		\item First centre $Y$ and all the columns of $X$. Initiale the current model $M \subseteq \qty{1, \dotsc, p}$ to be $\emptyset$ and set the current residual $R$ to be $Y$. 
		\item Find the variable $k^*$ in $M^c$ most correlated with the current residual $R$. Set $M$ to be $M \cup \qty{k^*}$. Replace $R$ with the residual from regressing $R$ onto $X_{k^*}$. Further replac eeach variable in $M^c$ with the residual from regressing itself onto $X_{k^*}$.
		\item Continue the previous step unto $R = 0$. 
	\end{enumerate}
Show that this algorithm is equivalent to forward selection.

\emph{Hint:} Use induction on the iteration $m$ of the algorithm. Consider strengthening the natural induction hypothesis that the model at iteration $m$ is the same as that selected after $m$ steps of forward selection. 
\end{question}

\begin{solution}
	content...
\end{solution}

\begin{question}
	Show that if $W$ is mean-zero and sub-Gaussian with parameter $\sigma$, then $\Var(W) \leq \sigma^2$. 
\end{question}

\begin{proof}
	If $W$ is mean-zero, then $\Var(W) = \EE(W^2)$. By the proof of lemma 15 we have $\EE(W^2) \leq 2 \sigma^2$, but this bound is too loose.   
	
	Since $W$ is sub-Gaussian, we have for all $\alpha \in \RR$ that
	\begin{align*}
		\EE[e{\alpha W}] &\leq e^{\alpha^2\sigma^2/2} \\
		\sum_{k=0}^\infty \frac{\EE[W^k] }{k!} \alpha^k &\leq \sum_{k=0}^\infty \frac{\sigma^{2k}}{2^k k!} \alpha^{2k} \\
		\frac{\EE[W^2]}{2} \alpha^2 + \sum_{k=3}^\infty \frac{\EE[W^k]}{k!} \alpha^k &\leq \frac{\sigma^2}{2} \alpha^2 + \sum_{k=2}^\infty \frac{\sigma^{2k}}{2^k k!} \alpha^{2k} \\
		\frac12\alpha^2 \qty(\sigma^2 - \EE[W^2]) &\geq \alpha^3 P(\alpha),
		\end{align*}
	where $P$ is a power series in $\alpha$. Rescaling we find \[
	\frac12(\sigma^2 - \EE[W^2]) \geq \alpha P(\alpha),
	\]
	and letting $\alpha \to 0$ also lets $\alpha P(\alpha) \to 0$, and therefore $\sigma^2 - \EE[W^2] \geq 0$ so $\Var(W) \leq \sigma^2$. 
\end{proof}

\begin{question}
	Verify Hoeffding's lemma for the special case where $W$ is a Rademacher random variable, so $W$ takes the values $-1, 1$ each with probability $1/2$. 
\end{question}

\begin{proof}
		If $W$ is a Rademacher random variable then we have, using $(2k)! \geq 2^k k!$ for each $k \in \NN$, that
	\begin{align*}
		\EE[e^{\alpha W}] = \frac12 e^{-\alpha} + \frac12 e^{\alpha} = \sum_{k=0}^\infty \frac{\alpha^{2k}}{(2k)!} \leq \sum_{k=0}^\infty \frac{\alpha^{2k}}{2^k k!} = e^{\alpha^2/2}
	\end{align*}
which shows that $W$ is sub-Gaussian with parameter $1 = (1 -(-1))/2$, so Hoeffding's lemma holds indeed. 
\end{proof}

\begin{question}
	\begin{enumerate}[(a)]
		\item Let $W \sim \chi_d^2$. Show that
		\[
		\PP(\abs{W/d - 1} \geq t) \leq 2e^{-dt^2/8}
		\]
		for $t \in (0, 1)$. You may use the facts that the mgf of a $\chi_1^2$ random variable is $(1 - 2\alpha)^{-1/2}$ for $\alpha < 1/2$, and $e^{-\alpha} (1 - 2\alpha)^{-1/2} \leq e^{2\alpha^2}$ when $\abs{\alpha} < 1/4$. 
		
		\item Let $A \in \RR^{d\times p}$ have i.i.d.\ standard normal entries. Fix $u \in \RR^p$. Use the result above to conclude that
		\[
		\PP\qty(\abs{\frac{\norm{Au}_2^2}{d \norm{u}_2^2} - 1} \geq t) \leq 2e^{-dt^2/8}. 
		\]
		
		\item Suppose we have data $u_1, \dotsc, u_n \in \RR^p$, with $p$ large and $n \geq 2$. Show that for a given $\eps \in (0, 1)$ and $d > 16\log(n/\sqrt\eps)/t^2$, each data point may be compressed down to $u_i \mapsto Au_i/\sqrt d = w_i$, whilst approximately preserving the distance between the points:
		\[
		\PP\qty(1 - t \leq \frac{\norm{w_i - w_j}_2^2}{\norm{u_i - u_j}_2^2} \leq 1 + t \text{ for all } i \neq j \in \qty{1, \dotsc, n}) \geq 1 - \eps. 
		\]
		This is the famous Johnson-Lindenstrauss Lemma. 
	\end{enumerate}
\end{question}

\begin{proof}
	\begin{enumerate}[(a)]
		\item 
	\end{enumerate}
\end{proof}
\end{document}