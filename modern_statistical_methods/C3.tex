\section{High-dimensional covariance estimation and PCA}
Until now, we've focused on regression. In this chapter, we will assume the samples $x_1, \dotsc, x_n$ are i.i.d.\ from some distribution in $\RR^d$. We are interested in estimating the covariance matrix of $x_i$. There are two reasons for this:
\begin{enumerate}
	\item Covariance can reveal underlying structure relating the variables.
	\item When $d$ is large, interpreting the covariance matrix directly is hard. However, its largest eigenvalues and corresponding eigenvectors can reveal the principal modes of variation in the data. 
\end{enumerate}

\subsection{Covariance estimation}
\subsubsection{Maximum likelihood in multivariate normal model}
Let $x_1, \dotsc, x_n$ be i.i.d.\ samples from a $N_d(\mu, \Sigma)$ distribution. We will assume that $\Sigma$ is invertible with inverse $\Omega$ (this inverse is sometimes called the \emph{precision matrix}). Furthermore, we define the \emph{sample covariance} \[
\hat\Sigma \ceq \frac1n \sum_{i=1}^n (x_i - \bar X)(x_i - \bar X)\T. 
\]

It can be shown that the maximum likelihood estimate for $\mu$ is $\bar X \ceq \frac1n \sum_{i=1}^n x_i$, and that the maximum likelihood estimate $\Omega$ is
\[
\Omega = \min_{\Omega \succ 0} \qty(-\log(\det\Omega) + \tr(\hat\Sigma\Omega)). 
\]
Since this is the minimisation of a convex function over a convex set, we know that $\Omega$ is a minimiser if and only if the derivative w.r.t.\ each element of $\Omega$ is 0. From this, we can compute that $(\hat\Omega^\Rm{ML})^{-1} = \hat\Sigma$. Therefore, if $X$ has full column rank so that $\hat\Sigma$ is invertible, we find $\hat\Omega^\Rm{ML} = \hat\Sigma^{-1}$, and since matrix inversion is a bijective map on the set of positive definite matrices, we conclude $\hat\Sigma^\Rm{ML} = \hat\Sigma$. 

\subsubsection{Non-asymptotic error bounds}
In the classical setting, we know that the maximum likelihood estimator is asymptotically optimal if we let $n \to \infty$ and keep all other parameters fixed. However, we are interested in \emph{non-asymptotic error bounds}, which do not rely on $n$ going to $\infty$. From these bounds, we can infer much more about the $n\to\infty$ case, namely, how much the other parameters are allowed to grow with $n$. 

We will assume for simplicity that $\mu = 0$ and that $\hat\Sigma = \frac1n \sum_{i=1}^n x_i x_i\T$.  Recall that the largest (resp.\ smallest) eigenvalue of a symmetric matrix $A \in \RR^{n\times n}$ is given by the maximum (resp.\ minimum) of $v\T A v$ over all $v \in S^{n-1}$. It follows that the operator norm of $A$ is given by $\norm{A}_\Rm{op} = \max_{v \in S^{n-1}} \abs{v\T A v}$. 

\begin{theorem}
	Let $X \in \RR^{n\times d}$ have i.i.d.\ rows $x_1, \dotsc, x_n$ with mean 0 and covariance $\Sigma$, so that 
	\[
	\EE\qty[ e^{\lambda v\T x_1}] \leq e^{\lambda^2\sigma^2/2} \quad\text{for all $v \in S^{d-1}$}. 
	\]
	Then there are universal constants $c, C$ such that $\hat\Sigma$ satisfies 
	\[
	\EE\qty[e^{\lambda\norm{\hat\Sigma - \Sigma}_\Rm{op}}] \leq \exp\qty(c \qty[ \frac{\lambda^2\sigma^4}{n} + d]) \quad\text{for all $\abs{\lambda} \leq \frac n{16\sigma^2}$},
	\]
	and for all $\delta > 0$, we have
	\[
	\PP\qty(\frac{\norm{\hat\Sigma - \Sigma}_\Rm{op}}{\sigma^2} \leq C \qty(\frac{d + \delta}{n} \lor \sqrt{\frac{d + \delta}{n}})) \geq 1 - e^{-\delta}. 
	\]
\end{theorem}

This is a non-asymptotic error bound. Note that, as long as $d/n \to 0$, the estimator $\hat\Sigma$ will be consistent. 

\begin{proof}
	The tail probability bound follows from Chernoff bounding, see lecture 17 for the details. 
	
	For the proof of (i), cover $S^{d-1}$ with balls of radius $1/8$ centred at points $v_1, \dotsc, v_N \in S^{d-1}$. It is a known geometric fact that this is possible with $N \leq e^{4d}/2$.  We will prove that for any symmetric $Q$ we have $ \norm{Q}_\Rm{op} \leq 2 \max_i \abs{v_i\T Q v_i}$. For this, let $v \in S^{d-1}$ and choose $i$ such that $\norm{v - v_i} \leq \frac18$, then
	\begin{align*}
	\abs{v\T Qv} &= \abs{(v - v_i)\T Q (v - v_i) + 2 v_i\T Q(v - v_i) - v_i\T Q v_i} \\
	&\leq \abs{(v - v_i)\T Q(v - v_i)} + 2 \abs{v_i\T Q(v - v_i)} + \abs{v_i\T Q v_i} \\
	&\leq \norm{v - v_i}^2 \norm{Q}_\Rm{op} + 2 \norm{v_i} \norm{Q}_\Rm{op} \norm{v - v_i} + \abs{v_i\T Q v_i} \\
	&\leq \qty(\frac28 + \frac1{64}) \norm{Q}_\Rm{op} + \abs{v_i\T Q v_i} \leq \frac12 \norm{Q}_\Rm{op} + \abs{v_i\T Q v_i} \\
	&\leq \frac12\norm{Q}_\Rm{op} + \max_i \abs{v_i\T Q v_i}. 
	\end{align*}
	Since the right-hand side is independent of $v$ we can maximise the left-hand side over $v$ and obtain 
	\[
	\norm{Q}_\Rm{op} \leq \frac12\norm{Q}_\Rm{op} + \max_i \abs{v_i\T Q v_i} \implies \norm{Q}_\Rm{op} \leq 2 \max_i \abs{v_i\T Q v_i}. 
	\]
	
	Now, we apply this to $Q \ceq \hat\Sigma - \Sigma$. Then we have
	\begin{equation} \label{eq:crude_moment_bound}
		\EE[e^{\lambda \norm{Q}_\Rm{op}}] \leq \EE[e^{2\lambda \max_i \abs{v_i\T Qv_i}}] \leq \sum_{i=1}^N \EE[e^{2\lambda v_i\T Q v_i}] + \EE[ e^{-2\lambda v_i\T Q v_i}], 
		\end{equation}
	where for the final step we use that $\max_i \abs{v_i\T Q v_i} \in \qty{\pm v_i\T Q v_i \mid i = 1, \dotsc, n}$. 
	
	Next, we claim that for fixed $u \in S^{d-1}$, we have $\EE[e^{tu\T Q u}] \leq \exp(64\frac{t^2\sigma^4}{n})$ for $\abs{t} \leq \frac n{8\sigma^2}$.  For this, observe that 
	\[
	u\T Q u = \frac1n \sum_{i=1}^n (u\T x_i)^2 - u\T \Sigma u, 
	\]
	and note that $u\T \Sigma u = \Var(u\T x_i) = \EE[(u\T x_i)^2]$ since the $x_i$ have mean zero. Using the fact that the $x_i$ are independent we have
	\[
	\EE[\exp(t u\T Q u)] = \EE\qty[\exp(\frac tn \qty[(u\T x_1)^2 - u\T \Sigma u])]^n = \EE[\exp(Z^2 - \EE[Z^2])]^n, 
	\]
	where $Z \ceq \sqrt{\frac tn} u\T x_1$ is sub-Gaussian with parameter $\sqrt{\abs{t}\sigma^2/n}$. Therefore, $Z^2$ satisfies the Bernstein condition with parmeters $(8\abs{t} \sigma^2/n, 4 \abs{t}\sigma^2/n)$, and recalling that $\abs{t}\leq \frac n{8\sigma^2}$ we obtain 
	\begin{align*}
		\EE[\exp(Z^2 - \EE[Z^2])]  &\leq 1 + \sum_{k=2}^\infty \frac{\EE[(Z^2 - \EE Z^2)^k]}{k!} \\
		&\overset\star\leq 1 + \sum_{k=2}^\infty 2^{2k + 1} \sigma^{2k} \qty(\frac{\abs{t}}{n})^k \\
		&\overset{\star\star}\leq 1 + \frac{2^5 \sigma^4 t^2}{n} \frac{1}{1 - 4\sigma^2 \abs{t}/n} \\
		&\leq 1 + \frac{64 \sigma^2 t^2}{n} \leq e^{64 \sigma^4 t^2/n},
	\end{align*}
where $\star$ follows from Bernstein's inequality and $\star\star$ from a geometric series, which proves the claim. 

Substituting this back into \cref{eq:crude_moment_bound} using $t =  \pm 2\lambda$ shows that for $\abs{\lambda} < \frac{n}{16\sigma^2}$ we have
\[
	\EE[e^{\lambda \norm{Q}_\Rm{op}}] \leq 2N e^{256 \lambda^2 \sigma^4/n} \leq  e^{256\lambda^2\sigma^4/n + 4d} \leq  e^{256(\lambda^2 \sigma^4/n + d)}. 
\]
\end{proof}

The upper bound in this theorem is tight when $\Sigma = I$. However, note that our bound in \cref{eq:crude_moment_bound} is quite crude, and we can obtain much better rates if we assume that the $x_i$ are normally distributed (in this case, more is known about the supremum). 

Furthermore, when the spectrum of $\Sigma$ is concentrated on a few directions, the rate can be improved significantly. For this, we need a definition.
\begin{definition}
	The \emph{effective rank} $r(\Sigma)$ of a nonzero symmetric positive semi-definite matrix $\Sigma$ with eigenvalues $0 \leq \gamma_d(\Sigma) \leq \dotsb \leq \gamma_1(\Sigma)$ is given by
	\[
	r(\Sigma) = \frac{\Tr(\Sigma)}{\norm{\Sigma}_\Rm{op}} = \frac{\sum_{i=1}^d \gamma_i(\Sigma)}{\gamma_1(\Sigma)}. 
	\]
\end{definition}
It is easily seen that $r(\Sigma) \leq \Rm{rank}(\Sigma)$, with equality if and only if all nonzero eigenvalues of $\Sigma$ are equal. The following theorem (which we will not prove) gives a tighter bound if we can assume that the rows of $X$ are normally distributed and that $r(\Sigma)$ is constrained:
\begin{theorem}
	Suppose $X$ has i.i.d.\ $N(0, \Sigma)$ rows with $r(\Sigma) \leq cn$. Then there exists $C$ (depending on $c$) such that for all $\delta > 0$ we have
	\[
	\PP\qty(\frac{\norm{\hat\Sigma - \Sigma}_\Rm{op}}{\norm{\Sigma}_\Rm{op}} \leq C \qty(\frac{r(\Sigma) + \delta}{n} \lor \sqrt{\frac{r(\Sigma) + \delta}{n}})) \geq 1 - e^{-\delta}. 
	\]
\end{theorem}

In comparison with the previous bound, the dimension of the samples $d$ is replaced by the effective rank $r(\Sigma)$ (in fact, $d$ does not occur at all in this theorem!). This means that $d$ can grow rapidly with $n$, as long as $r(\Sigma)/n \to 0$. This can even be generalised to an infinite-dimensional setting with appropriate definitions. 

\subsubsection{The graphical Lasso}
We saw that if we make the structural assumption that the effective rank of $\Sigma$ is small, the sample covariance matrix $\hat\Sigma$ is a good estimator. However, there are other possible structural assumptions. 
One is based on the following:
\begin{proposition}
	Let $Z \sim N(\mu, \Omega^{-1})$. Then $z_k \perp z_j \mid z_{-jk}$ ($z_k$ is independent of $z_j$ conditional on the other $z_i$) if and only if $\Omega_{jk} =0 $.  
\end{proposition}

\begin{proof}
	We have
	\begin{align*}
		f(z_j, z_k \mid z_{-jk}) &= \frac{f(z_1, \dotsc, z_n)}{f(z_{-jk})} \propto f(z_1, \dotsc, z_n) \propto \exp(-\frac12 (z-\mu)\T \Omega(z-\mu)) \\
		&= g(z_j) h(z_k) \exp(- z_j z_k \Omega_{jk}), 
	\end{align*}
and this factorises if and only if $\Omega_{jk} = 0$. 
\end{proof}

Assuming that many variables are independent of each other conditional on the other variables (or in other words, that $z_k$ and $z_j$ depend on each other only ``through the other $z_i$''), is a fairly natural assumption, which means it is sensible to assume that $\Omega$ is sparse. If this is the case, $\hat\Sigma$ may not be the best estimator. 

The \emph{graphical lasso} explicitly penalises estimators of $\Omega$ with many non-zero entries, and is defined as
\[
\hat\Omega^\Rm{GL} \ceq \argmin_{\Omega \succ 0} \qty(-\log(\det\Omega) + \Tr(\hat\Sigma\Omega) + \lambda \norm{\Omega}_1), 
\]
where the 1-norm of a matrix is defined as the sum of the absolute values of its entries. This is exactly minus the log-likelihood plus a penalty term, and finding $\hat\Omega^\Rm{GL}$ is a convex optimisation problem which generally leads to sparse estimators. 

Note that when we penalise $\norm{\Omega}_1$, we are also penalising the diagonal entries, which have nothing to do with conditional independence. Therefore, sometimes we penalise only $\sum_{j \neq k} \abs{\Omega_{jk}}$. 

\subsection{Principal Component Analysis (PCA)}
Let $X \in \RR^{n\times d}$ have i.i.d.\ rows with mean 0 and covariance $\Sigma$. Let $\lambda_1 \geq \dotsb \geq \lambda_d \geq 0$ be the eigenvalues of $\Sigma$ with eigenvectors $u_1, \dotsc, u_ d\in S^{d-1}$. For any $s \leq d$ we have
\[
\Span(u_1, \dotsc, u_s) = \argmax_{\dim(S) = s} \EE[ \norm{P_S x_1}^2] = \argmin_{\dim(S) = s} \EE[\norm{x_1 - P_S x_1}^2], 
\]
i.e., $\Span(u_1, \dotsc, u_s)$ is the $s$-dimensional subspace of $\RR^d$ with maximum variance and minimal residual variance. 

The quantity $q_s = \frac{\sum_{i=1}^s \lambda_i}{\Tr(\Sigma)}$ is the proportion of variance captured by the largest $s$ eigenvalues, and this may be close to 1 for $s \ll d$. The idea of PCA is to approximate the eigenvalues and eigenvectors of $\Sigma$ by those of $\hat\Sigma = \frac1n (X - \bar X \mathbf{1})\T (X - \bar X \mathbf{1}) \ceq \frac1n \tilde X\T \tilde X$. 

The main question is: how robust are eigenvectors and eigenvalues under perturbation of the data? 

\subsubsection{Perturbation of eigenvalues and eigenvectors}
Let $E = \hat\Sigma - \Sigma$, and suppose $E$ is a deterministic perturbation. We have proven bounds on $\norm{E}_\Rm{op}$. It is easily seen that
\[
\hat{\lambda_1} = \max_{v \in S^{d-1}} v\T (\Sigma + E)v \leq\max_{v \in S^{d-1}} v\T \Sigma v + \max_{v \in S^{d-1}} v\T E v  \leq \lambda_1 + \norm{E}_\Rm{op}, 
\]
and analogously it can be seen that $\lambda_1 \leq \hat\lambda_1 + \norm{E}_\Rm{op}$, so we obtain $\abs{\lambda_1 - \hat\lambda_1} \leq \norm{E}_\Rm{op}$. This can be generalised:
\begin{theorem}[Weyl]
	We have $\max_i \abs*{\hat\lambda_i - \lambda_i} \leq \norm{E}_\Rm{op}$. 
\end{theorem}

We conclude that eigenvalues are generally robust to small perturbations. However, eigenvectors are genearlly not robust to small perturbations. This is because when $\lambda_i$ and $\lambda_{i+1}$ are close to each other, then $\Span(\hat u_i, \hat u_{i+1})$ may be a good approximation of $\Span(u_i, u_{i+1})$, but it is not possible to identify the individual eigenvectors.  Therefore we must place further assumptions on the eigenvectors. 