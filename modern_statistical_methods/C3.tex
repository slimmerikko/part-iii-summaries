\section{High-dimensional covariance estimation and PCA}
Until now, we've focused on regression. In this chapter, we will assume the samples $x_1, \dotsc, x_n$ are i.i.d.\ from some distribution in $\RR^d$. We are interested in estimating the covariance matrix of $x_i$. There are two reasons for this:
\begin{enumerate}
	\item Covariance can reveal underlying structure relating the variables.
	\item When $d$ is large, interpreting the covariance matrix directly is hard. However, its largest eigenvalues and corresponding eigenvectors can reveal the principal modes of variation in the data. 
\end{enumerate}

\subsection{Covariance estimation}
\subsubsection{Maximum likelihood in multivariate normal model}
Let $x_1, \dotsc, x_n$ be i.i.d.\ samples from a $N_d(\mu, \Sigma)$ distribution. We will assume that $\Sigma$ is invertible with inverse $\Omega$ (this inverse is sometimes called the \emph{precision matrix}). Furthermore, we define the \emph{sample covariance} \[
\hat\Sigma \ceq \frac1n \sum_{i=1}^n (x_i - \bar X)(x_i - \bar X)\T. 
\]

It can be shown that the maximum likelihood estimate for $\mu$ is $\bar X \ceq \frac1n \sum_{i=1}^n x_i$, and that the maximum likelihood estimate $\Omega$ is
\[
\Omega = \min_{\Omega \succ 0} \qty(-\log(\det\Omega) + \tr(\hat\Sigma\Omega)). 
\]
Since this is the minimisation of a convex function over a convex set, we know that $\Omega$ is a minimiser if and only if the derivative w.r.t.\ each element of $\Omega$ is 0. From this, we can compute that $(\hat\Omega^\Rm{ML})^{-1} = \hat\Sigma$. Therefore, if $X$ has full column rank so that $\hat\Sigma$ is invertible, we find $\hat\Omega^\Rm{ML} = \hat\Sigma^{-1}$, and since matrix inversion is a bijective map on the set of positive definite matrices, we conclude $\hat\Sigma^\Rm{ML} = \hat\Sigma$. 

\subsubsection{Non-asymptotic error bounds}
In the classical setting, we know that the maximum likelihood estimator is asymptotically optimal if we let $n \to \infty$ and keep all other parameters fixed. However, we are interested in \emph{non-asymptotic error bounds}, which do not rely on $n$ going to $\infty$. From these bounds, we can infer much more about the $n\to\infty$ case, namely, how much the other parameters are allowed to grow with $n$. 

We will assume for simplicity that $\mu = 0$ and that $\hat\Sigma = \frac1n \sum_{i=1}^n x_i x_i\T$.  Recall that the largest (resp.\ smallest) eigenvalue of a symmetric matrix $A \in \RR^{n\times n}$ is given by the maximum (resp.\ minimum) of $v\T A v$ over all $v \in S^{n-1}$. It follows that the operator norm of $A$ is given by $\norm{A}_\Rm{op} = \max_{v \in S^{n-1}} \abs{v\T A v}$. 

\begin{theorem}
	Let $X \in \RR^{n\times d}$ have i.i.d.\ rows $x_1, \dotsc, x_n$ with mean 0 and covariance $\Sigma$, so that 
	\[
	\EE\qty[ e^{\lambda v\T x_1}] \leq e^{\lambda^2\sigma^2/2} \quad\text{for all $v \in S^{d-1}$}. 
	\]
	Then there are universal constants $c, C$ such that $\hat\Sigma$ satisfies 
	\[
	\EE\qty[e^{\lambda\norm{\hat\Sigma - \Sigma}_\Rm{op}}] \leq \exp\qty(c \qty[ \frac{\lambda^2\sigma^4}{n} + d]) \quad\text{for all $\abs{\lambda} \leq \frac n{16\sigma^2}$},
	\]
	and for all $\delta > 0$, we have
	\[
	\PP\qty(\frac{\norm{\hat\Sigma - \Sigma}_\Rm{op}}{\sigma^2} \leq C \qty(\frac{d + \delta}{n} \lor \sqrt{\frac{d + \delta}{n}})) \geq 1 - e^{-\delta}. 
	\]
\end{theorem}

This is a non-asymptotic error bound. Note that, as long as $d/n \to 0$, the estimator $\hat\Sigma$ will be consistent. 

\begin{proof}
	The tail probability bound follows from Chernoff bounding, see lecture 17 for the details. 
	
	For the proof of (i), cover $S^{d-1}$ with balls of radius $1/8$ centred at points $v_1, \dotsc, v_N \in S^{d-1}$. It is a known geometric fact that this is possible with $N \leq e^{4d}/2$.  We will prove that for any symmetric $Q$ we have $ \norm{Q}_\Rm{op} \leq 2 \max_i \abs{v_i\T Q v_i}$. For this, let $v \in S^{d-1}$ and choose $i$ such that $\norm{v - v_i} \leq \frac18$, then
	\begin{align*}
	\abs{v\T Qv} &= \abs{(v - v_i)\T Q (v - v_i) + 2 v_i\T Q(v - v_i) - v_i\T Q v_i} \\
	&\leq \abs{(v - v_i)\T Q(v - v_i)} + 2 \abs{v_i\T Q(v - v_i)} + \abs{v_i\T Q v_i} \\
	&\leq \norm{v - v_i}^2 \norm{Q}_\Rm{op} + 2 \norm{v_i} \norm{Q}_\Rm{op} \norm{v - v_i} + \abs{v_i\T Q v_i} \\
	&\leq \qty(\frac28 + \frac1{64}) \norm{Q}_\Rm{op} + \abs{v_i\T Q v_i} \leq \frac12 \norm{Q}_\Rm{op} + \abs{v_i\T Q v_i} \\
	&\leq \frac12\norm{Q}_\Rm{op} + \max_i \abs{v_i\T Q v_i}. 
	\end{align*}
	Since the right-hand side is independent of $v$ we can maximise the left-hand side over $v$ and obtain 
	\[
	\norm{Q}_\Rm{op} \leq \frac12\norm{Q}_\Rm{op} + \max_i \abs{v_i\T Q v_i} \implies \norm{Q}_\Rm{op} \leq 2 \max_i \abs{v_i\T Q v_i}. 
	\]
	
	Now, we apply this to $Q \ceq \hat\Sigma - \Sigma$. Then we have
	\begin{equation} \label{eq:crude_moment_bound}
		\EE[e^{\lambda \norm{Q}_\Rm{op}}] \leq \EE[e^{2\lambda \max_i \abs{v_i\T Qv_i}}] \leq \sum_{i=1}^N \EE[e^{2\lambda v_i\T Q v_i}] + \EE[ e^{-2\lambda v_i\T Q v_i}], 
		\end{equation}
	where for the final step we use that $\max_i \abs{v_i\T Q v_i} \in \qty{\pm v_i\T Q v_i \mid i = 1, \dotsc, n}$. 
	
	Next, we claim that for fixed $u \in S^{d-1}$, we have $\EE[e^{tu\T Q u}] \leq \exp(64\frac{t^2\sigma^4}{n})$ for $\abs{t} \leq \frac n{8\sigma^2}$.  For this, observe that 
	\[
	u\T Q u = \frac1n \sum_{i=1}^n (u\T x_i)^2 - u\T \Sigma u, 
	\]
	and note that $u\T \Sigma u = \Var(u\T x_i) = \EE[(u\T x_i)^2]$ since the $x_i$ have mean zero. Using the fact that the $x_i$ are independent we have
	\[
	\EE[\exp(t u\T Q u)] = \EE\qty[\exp(\frac tn \qty[(u\T x_1)^2 - u\T \Sigma u])]^n = \EE[\exp(Z^2 - \EE[Z^2])]^n, 
	\]
	where $Z \ceq \sqrt{\frac tn} u\T x_1$ is sub-Gaussian with parameter $\sqrt{\abs{t}\sigma^2/n}$. Therefore, $Z^2$ satisfies the Bernstein condition with parmeters $(8\abs{t} \sigma^2/n, 4 \abs{t}\sigma^2/n)$, and recalling that $\abs{t}\leq \frac n{8\sigma^2}$ we obtain 
	\begin{align*}
		\EE[\exp(Z^2 - \EE[Z^2])]  &\leq 1 + \sum_{k=2}^\infty \frac{\EE[(Z^2 - \EE Z^2)^k]}{k!} \\
		&\overset\star\leq 1 + \sum_{k=2}^\infty 2^{2k + 1} \sigma^{2k} \qty(\frac{\abs{t}}{n})^k \\
		&\overset{\star\star}\leq 1 + \frac{2^5 \sigma^4 t^2}{n} \frac{1}{1 - 4\sigma^2 \abs{t}/n} \\
		&\leq 1 + \frac{64 \sigma^2 t^2}{n} \leq e^{64 \sigma^4 t^2/n},
	\end{align*}
where $\star$ follows from Bernstein's inequality and $\star\star$ from a geometric series, which proves the claim. 

Substituting this back into \cref{eq:crude_moment_bound} using $t =  \pm 2\lambda$ shows that for $\abs{\lambda} < \frac{n}{16\sigma^2}$ we have
\[
	\EE[e^{\lambda \norm{Q}_\Rm{op}}] \leq 2N e^{256 \lambda^2 \sigma^4/n} \leq  e^{256\lambda^2\sigma^4/n + 4d} \leq  e^{256(\lambda^2 \sigma^4/n + d)}. 
\]
\end{proof}

The upper bound in this theorem is tight when $\Sigma = I$. However, note that our bound in \cref{eq:crude_moment_bound} is quite crude, and we can obtain much better rates if we assume that the $x_i$ are normally distributed (in this case, more is known about the supremum). 

Furthermore, when the spectrum of $\Sigma$ is concentrated on a few directions, the rate can be improved significantly. For this, we need a definition.
\begin{definition}
	The \emph{effective rank} $r(\Sigma)$ of a nonzero symmetric positive semi-definite matrix $\Sigma$ with eigenvalues $0 \leq \gamma_d(\Sigma) \leq \dotsb \leq \gamma_1(\Sigma)$ is given by
	\[
	r(\Sigma) = \frac{\Tr(\Sigma)}{\norm{\Sigma}_\Rm{op}} = \frac{\sum_{i=1}^d \gamma_i(\Sigma)}{\gamma_1(\Sigma)}. 
	\]
\end{definition}
It is easily seen that $r(\Sigma) \leq \Rm{rank}(\Sigma)$, with equality if and only if all nonzero eigenvalues of $\Sigma$ are equal. The following theorem (which we will not prove) gives a tighter bound if we can assume that the rows of $X$ are normally distributed and that $r(\Sigma)$ is constrained:
\begin{theorem}
	Suppose $X$ has i.i.d.\ $N(0, \Sigma)$ rows with $r(\Sigma) \leq cn$. Then there exists $C$ (depending on $c$) such that for all $\delta > 0$ we have
	\[
	\PP\qty(\frac{\norm{\hat\Sigma - \Sigma}_\Rm{op}}{\norm{\Sigma}_\Rm{op}} \leq C \qty(\frac{r(\Sigma) + \delta}{n} \lor \sqrt{\frac{r(\Sigma) + \delta}{n}})) \geq 1 - e^{-\delta}. 
	\]
\end{theorem}

In comparison with the previous bound, the dimension of the samples $d$ is replaced by the effective rank $r(\Sigma)$ (in fact, $d$ does not occur at all in this theorem!). This means that $d$ can grow rapidly with $n$, as long as $r(\Sigma)/n \to 0$. This can even be generalised to an infinite-dimensional setting with appropriate definitions. 

\subsubsection{The graphical Lasso}
We saw that if we make the structural assumption that the effective rank of $\Sigma$ is small, the sample covariance matrix $\hat\Sigma$ is a good estimator. However, there are other possible structural assumptions. 
One is based on the following:
\begin{proposition}
	Let $Z \sim N(\mu, \Omega^{-1})$. Then $z_k \perp z_j \mid z_{-jk}$ ($z_k$ is independent of $z_j$ conditional on the other $z_i$) if and only if $\Omega_{jk} =0 $.  
\end{proposition}

\begin{proof}
	We have
	\begin{align*}
		f(z_j, z_k \mid z_{-jk}) &= \frac{f(z_1, \dotsc, z_n)}{f(z_{-jk})} \propto f(z_1, \dotsc, z_n) \propto \exp(-\frac12 (z-\mu)\T \Omega(z-\mu)) \\
		&= g(z_j) h(z_k) \exp(- z_j z_k \Omega_{jk}), 
	\end{align*}
and this factorises if and only if $\Omega_{jk} = 0$. 
\end{proof}

Assuming that many variables are independent of each other conditional on the other variables (or in other words, that $z_k$ and $z_j$ depend on each other only ``through the other $z_i$''), is a fairly natural assumption, which means it is sensible to assume that $\Omega$ is sparse. If this is the case, $\hat\Sigma$ may not be the best estimator. 

The \emph{graphical lasso} explicitly penalises estimators of $\Omega$ with many non-zero entries, and is defined as
\[
\hat\Omega^\Rm{GL} \ceq \argmin_{\Omega \succ 0} \qty(-\log(\det\Omega) + \Tr(\hat\Sigma\Omega) + \lambda \norm{\Omega}_1), 
\]
where the 1-norm of a matrix is defined as the sum of the absolute values of its entries. This is exactly minus the log-likelihood plus a penalty term, and finding $\hat\Omega^\Rm{GL}$ is a convex optimisation problem which generally leads to sparse estimators. 

Note that when we penalise $\norm{\Omega}_1$, we are also penalising the diagonal entries, which have nothing to do with conditional independence. Therefore, sometimes we penalise only $\sum_{j \neq k} \abs{\Omega_{jk}}$. 

\subsection{Principal Component Analysis (PCA)}
Let $X \in \RR^{n\times d}$ have i.i.d.\ rows with mean 0 and covariance $\Sigma$. Let $\lambda_1 \geq \dotsb \geq \lambda_d \geq 0$ be the eigenvalues of $\Sigma$ with eigenvectors $u_1, \dotsc, u_ d\in S^{d-1}$. For any $s \leq d$ we have
\[
\Span(u_1, \dotsc, u_s) = \argmax_{\dim(S) = s} \EE[ \norm{P_S x_1}^2] = \argmin_{\dim(S) = s} \EE[\norm{x_1 - P_S x_1}^2], 
\]
i.e., $\Span(u_1, \dotsc, u_s)$ is the $s$-dimensional subspace of $\RR^d$ with maximum variance and minimal residual variance. 

The quantity $q_s = \frac{\sum_{i=1}^s \lambda_i}{\Tr(\Sigma)}$ is the proportion of variance captured by the largest $s$ eigenvalues, and this may be close to 1 for $s \ll d$. The idea of PCA is to approximate the eigenvalues and eigenvectors of $\Sigma$ by those of $\hat\Sigma = \frac1n (X - \bar X \mathbf{1})\T (X - \bar X \mathbf{1}) \ceq \frac1n \tilde X\T \tilde X$. 

The main question is: how robust are eigenvectors and eigenvalues under perturbation of the data? 

\subsubsection{Perturbation of eigenvalues and eigenvectors}
Let $E = \hat\Sigma - \Sigma$, and suppose $E$ is a deterministic perturbation. We have proven bounds on $\norm{E}_\Rm{op}$. It is easily seen that
\[
\hat{\lambda_1} = \max_{v \in S^{d-1}} v\T (\Sigma + E)v \leq\max_{v \in S^{d-1}} v\T \Sigma v + \max_{v \in S^{d-1}} v\T E v  \leq \lambda_1 + \norm{E}_\Rm{op}, 
\]
and analogously it can be seen that $\lambda_1 \leq \hat\lambda_1 + \norm{E}_\Rm{op}$, so we obtain $\abs{\lambda_1 - \hat\lambda_1} \leq \norm{E}_\Rm{op}$. This can be generalised:
\begin{theorem}[Weyl]
	We have $\max_i \abs*{\hat\lambda_i - \lambda_i} \leq \norm{E}_\Rm{op}$. 
\end{theorem}

We conclude that eigenvalues are generally robust to small perturbations. However, eigenvectors are genearlly not robust to small perturbations. This is because when $\lambda_i$ and $\lambda_{i+1}$ are close to each other, then $\Span(\hat u_i, \hat u_{i+1})$ may be a good approximation of $\Span(u_i, u_{i+1})$, but it is not possible to identify the individual eigenvectors.  Therefore we must place further assumptions on the eigenvectors. 

\begin{definition}
	Let $S, U \in \RR^{n \times d}$ be matrices with orthogonal columns. We define the \emph{$\sint$ distance} between the column space of $S$ and $U$ as
	\[
	\sint(S,U) \ceq \norm{\Pi_S(I - \Pi_U)}_F = \norm{\Pi_U(I - \Pi_S)}_F
	\]
\end{definition}
To see that the last equality holds, note that
\begin{align*}
	\norm{\Pi_S(I - \Pi_U)}_F^2 &= \Tr\qty((I - \Pi_U) \Pi_S^2 (I - \Pi_U)) = \Tr\qty(\Pi_S(I - \Pi_U)) = \Tr(\Pi_S) - \Tr(\Pi_S\Pi_U) \\
	&= \rank(S) - \Tr(\Pi_S\Pi_U) = \rank(U) - \Tr(\Pi_U\Pi_S) = \norm{\Pi_U(I - \Pi_S)}_F^2. 
\end{align*}

In the fourth example sheet we prove the following:
\begin{proposition} \label{prop:sint_distance}
	Let $S, U$ be matrices with orthogonal columns and equal rank. Then
	\[
	\min_{O \in \RR^{d \times d} \text{ }\Rm{orthogonal}} \norm{S - UO}_F \leq \sqrt2 \sint(S, U). 
	\]
\end{proposition}

\begin{theorem}[Davis-Kahan]
	Let $\Sigma$ be a covariance matrix with eigenvector decomposition $\Sigma = U\Lambda U\T$ (where $\lambda_1 \geq \dotsb \geq \lambda_d$). Write $\Sigma = U_0 \Lambda_0 U_0\T + U_1 \Lambda_1 U_1\T$ where $U_0 = [u_1, \dotsc, u_r]$. 
	
	For a perturbed matrix $\hat\Sigma = \Sigma + E$ with $E$ symmetric, define an analogous decomposition $\hat\Sigma = \hat U_0 \hat\Lambda_0 \hat U_0\T + \hat U_1 \hat\Lambda_1 \hat U_1\T$. If $\lambda_r - \hat\lambda_{r+1} > 0$, then
	\[
	\sint (U_0, \hat U_0)  \leq \frac{\norm{E}_F}{\lambda_r - \hat\lambda_{r+1}}. 
	\]
\end{theorem}

\begin{proof}
	We write
	\[
	\norm{\Pi_{U_0}(I - \Pi_{\hat U_0})}_F = \norm{U_0 U_0\T \hat U_1 \hat U_1\T}_F = \norm{U_0\T \hat U_1}_F, 
	\]
	and note that 
	\[
	\Sigma U_0 = U_0\Lambda U_0\T U_0 + U_1 \Lambda_1 U_1\T U_0 =  U_0\Lambda_0, \quad \hat\Sigma \hat U_1 = \hat U_1 \hat\Lambda_1. 
	\]
	We therefore find 
	\begin{align*}
	E U_0 &= (E + \Sigma) U_0 - \Sigma U_0 = \hat\Sigma U_0 - U_0 \Lambda_0 \\
	\hat U_1\T EU_0 &= \hat U_1\T \hat\Sigma U_0 - \hat U_1\T U_0 \Lambda_0 = \hat\Lambda_1 \hat U_1\T U_0 - \hat U_1\T U_0 \Lambda_0. 
	\end{align*}
By the triangle inequality we have
\begin{align*}
\norm{\hat U_1\T E U_0}_F &\geq \norm{\hat U_1\T U_0 \Lambda_0}_F - \norm{\hat\Lambda_1 \hat U_1\T U_0}_F \\
&\overset\star\geq \lambda_r \norm{\hat U_1\T U_0} - \hat\lambda_{r+1} \norm{\hat U_1\T U_0}_F = (\lambda_r - \hat\lambda_{r+1}) \norm{U_1\T U_0}_F = (\lambda_r - \hat\lambda_{r+1}) \sint(U_0, \hat U_0). 
\end{align*}
Noting that $\norm{\hat U_1\T E U_0}_F \leq \norm{E}_F$,
since
\[
\norm{E}_F = \norm{\hat U\T E U} = \norm{\mqty[\hat U_0\T \\ \hat U_1\T ] E \mqty[U_0 & U_1]}_F \geq \norm{\hat U_1\T E U_0}_F. 
\] yields the result. 
\end{proof}

The Davis-Kahan theorem is not directly useful since the result depends on the gap $\lambda_r - \hat\lambda_{r+1}$. However, the following variant was proved in 2014, whose result depends only on the eigenvalues of $\Sigma$:
\begin{theorem}
	If $\lambda_r - \lambda_{r+1} > 0$, then
	\[
	\sint (U_0, \hat U_0) \leq \frac{2\min \qty{\sqrt r \norm{E}_\Rm{op}, \norm{E}_F}}{\abs{\lambda_r - \lambda_{r+1}}}. 
	\]
\end{theorem}

\subsubsection{The spiked covariance model}
As an application of the Davis-Kahan theorem, consider the model where
\[
\Sigma = I + \theta v v\T, \qquad  v\in S^{d-1}. 
\]
Any random vector with covariance $\Sigma$ can be represented as $\eta \sqrt\theta v + \eps$, where $\eta, \eps_1, \dotsc, \eps_d$ are independent, mean 0, variance 1 random variables. The samples are a random ``spike'' with scale $\sqrt\theta$ in a fixed direction $v$, with independent noise added to each entry. 

The eigenvalues of $\Sigma$ are $1+\theta, 1, \dotsc, 1$, and $v$ is the top eigenvector. Using the variant of the Davis-Kahan theorem and \cref{prop:sint_distance} we find 
\[
\min_{z = \pm 1} \norm{v - z\hat v} \leq \sqrt2 \sint(v, \hat v) \leq \frac{\norm{\hat \Sigma - \Sigma}_\Rm{op}}{(1 + \theta) - 1}
\]
If we assume that the rows of $X$ are vector sub-Gaussian, we can bound $\norm{\hat\Sigma - \Sigma}_\Rm{op}$ in probability, and we find that 
\[
\PP\qty(\min_{z = \pm 1} \norm{v - z\hat v} \geq C \frac{1 + \theta}{\theta} \qty(\frac{d + \delta}{n} \lor \sqrt{\frac{d + \delta}{n}})) \leq e^{-\delta}. 
\]
We conclude that if $\frac{1+\theta}{\theta} \sqrt{\frac dn} \to 0$, then $\hat v$ estimates $v$ consistently. 

\subsubsection{High-dimensional asymptotics}
{\color{red} Non-examinable}. 

\subsection{Sparse PCA}
We have seen that in the spiked covariance model, PCA is consistent when $d/n \to 0$, but PCA can fail if $d/n$ is bounded away from zero. Considering the case $d/n \to \gamma > 0$, we can ask if there exists a better estimator. Minimax lower bounds show that in general, the answer is negative, and therefore we must place some assumptions on $\Sigma$.

Again, we assume $\Sigma = \theta vv\T + I$. Define
\[
\norm{v}_0 \ceq \abs{\qty{k \mid v_k \neq 0}} \quad\text{(the amount of nonzero components of $v$)}
\]
We will assume that $v$ is spare, i.e., $\norm{v}_0 = k \ll d$. This means that the extra variance in the data along the ``spike'' only involved a few variables. 
We can now define an estimator similar to best subset selection
\[
\hat v^{(0, k')} \ceq \argmax_{w \in S^{d-1}, \norm{w}_0 \leq k'} w\T \hat\Sigma w. 
\]

\begin{proposition}
	Let $\hat\Sigma$ be the sample covariance of $n$ i.i.d.\ $N(0, \Sigma)$ random variables. Suppose $k \leq d/2$ and $k \leq k'$, then 
	\[
	\PP\qty(\min_{z = \pm 1} \norm{v - z\hat v^{(0, k')}}_2 \geq  C\frac{1+\theta}{\theta} \qty[ \sqrt{\eta_n \lor \eta_n}]) \leq e^{-\delta} \quad\forall \delta > 0
	\]
	where 
	\[
	\eta_n = \frac{(k + k') \log(de^2/(k+ k')) + \delta}{n}.
	\]
\end{proposition}
Note that the dependence on $d$ is now logarithmic, so morally, we only need $\frac{\max(k, k')}{n} \to 0$ for consistency. 

The problem with this estimator is that it is hard to compute. The SCoTLASS estimator relaxes the estimator with 
\[
\hat v^{(1, \lambda)} \ceq \argmax_{w \in S^{d-1}, \norm{w}_1 \leq \lambda} w\T \hat\Sigma w.
\]
The optimisation problem is not convex (since we are maximising a convex function instead of minimising), but is easier to compute and has similar convergence rates as the estimator $\hat v^{(0, k)}$. We can relax the problem even further (see example sheet) which gives an easily computable solution. 

It has been shown that sparce PCA has no estimators computable in polynomial time which achieve minimax rates for certain distribution classes, which means that sparce PCA has some inherent computational difficulty. 